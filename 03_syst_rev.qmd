# Systematic Review

Trustworthy clinical practice guidelines [@Graham2011] are supported by systematic reviews meeting explicit standards [@Eden2011; @PCORI2019]. The systematic reviews supporting ASA practice parameters conform to those standards.

## Protocol

The protocol, developed collaboratively between the task force and methodologists, guides systematic review conduct, and provides documentation for updates. It includes background material, key questions, PICOTS,[^03_syst_rev-1] analytic framework, study inclusion and exclusion criteria, search strategy, and the anticipated approach to evidence synthesis. Depending on the anticipated scope, protocols may be registered on [PROSPERO](https://www.crd.york.ac.uk/PROSPERO/) [@Booth2012]. However, when the systematic review includes numerous questions and anticipated to require substantial refinement and modifications, registration is omitted. The protocol is included as a supplement to the published practice parameter. (An example draft protocol can be viewed [here](assets/Protocol_Geriatrics_2022-09-17.pdf)).

## Outcome Importance

Outcomes vary in importance according to patient values and preferences [@Guyatt2011]. From that perspective, following protocol completion the task force members rate outcome importance for decision-making. The ratings are reviewed by the task force and revised as necessary to achieve consensus. Outcomes are assigned a level --- critical, important but not critical, low importance --- and then ranked to prioritize those to include in the evidence synthesis. @fig-priority and @fig-priority_geri illustrate the results obtained from ratings and rankings.

![Prioritization of outcomes for neuromuscular monitoring.](assets/priority_nmb.png){#fig-priority fig-align="left" width="90%"}

```{r}
#| label: fig-priority_geri 
#| fig-cap: "Example of assessing outcome importance rankings in a geriatrics guideline. Rankings for the 5 most important outcomes across 7 key questions (11 respondents with maximum 77 for each outcome rank or any top 5 ranking)."
#| echo: false
#| warning: false
library(conflicted)
library(janitor)
library(htmltools)
library(reactable)
library(reactablefmtr)
library(tidyverse)
conflicts_prefer(dplyr::filter)
## key questions 1-6 & 8 --------------------------------------- (2023-01-24 11:48) @----
# function for outcome priority table by kq
outcome_tab_all <- function(outcome_dat, responses) {
  reactable(
    outcome_dat,
    pagination = FALSE,
    # highlight = TRUE,
    defaultSorted = "any_top_5",
    defaultSortOrder = "desc",
    defaultColDef = colDef(
      cell = data_bars(outcome_dat,
        box_shadow = TRUE,
        force_outside = c(1, 77),
        bar_height = 12,
        max_value = responses
        # fill_color = "#B22215"
      ),
      style = cell_style(outcome_dat,
        font_size = "11px",
      )
    ),
    columns = list(
      outcome = colDef(name = "Outcome", width = 200),
      rank1 = colDef(name = "Rank 1", width = 80, headerClass = "header", align = "center"),
      rank2 = colDef(name = "Rank 2", width = 80, headerClass = "header", align = "center"),
      rank3 = colDef(name = "Rank 3", width = 80, headerClass = "header", align = "center"),
      rank4 = colDef(name = "Rank 4", width = 80, headerClass = "header", align = "center"),
      rank5 = colDef(name = "Rank 5", width = 80, headerClass = "header", align = "center"),
      any_top_5 = colDef(
        name = "Any", width = 80, headerClass = "header", align = "center",
        cell = data_bars(
          outcome_dat,
          force_outside = c(1, 77),
          box_shadow = TRUE,
          bar_height = 12,
          max_value = responses,
          fill_color = "#B22215"
        )
      )
    ),
    theme = reactableTheme(
      borderColor = "#dfe2e5",
      stripedColor = "#f6f8fa",
      highlightColor = "#f0f5f9",
      cellPadding = "0px 0px"
    ),
    compact = TRUE,
    class = "priority-tbl"
  )
}

# read data for all kq
priority_dat <- readxl::read_xlsx(path = "~/Documents/_projects01/asa/_methods_manual/data/OutcomeRankingRawData_2023-01-23.xlsx", range = "A1:S89", sheet = "data") |>
  remove_empty(which = "rows") |> 
  fill(kq) |>
  # exclude kq7 not being addressed
  filter(kq != 7) |>
  mutate(id = paste0(kq, "-", id))

# function to tally the number by rank 1, 2, 3, 4, 5
rankings_all <- function(priority) {
  priority_dat |>
    # group_by(kq) |>
    summarise(across(3:19, ~ sum(.x == priority, na.rm = TRUE)), .names = "{.col}") |>
    mutate(rank = priority)
}

rankings_dat <- bind_rows(rankings_all(1), rankings_all(2), rankings_all(3), rankings_all(4), rankings_all(5)) |>
  select(-.names)

rankings_dat <- tblhelpr::transpose_tibble(rankings_dat, col_names = rank, id_col = "outcomes") |>
  rename_with(.fn = ~ paste0("rank", .)) |>
  rename(outcome = rankoutcomes) |>
  mutate(across(rank1:rank5, as.numeric),
    any_top_5 = rank5 + rank4 + rank3 + rank2 + rank1
  )

# rankings_dat |>
#   summarise(across(rank1:rank5, sum))

outcome_tab_all(rankings_dat, 77)
```

<font size = 2>*Rows can be reordered according to ranking by clicking on column headers.*</font>

## Identifying Literature

### Database Searches

A librarian/information specialist develops search strategies after reviewing the protocol and participating in task force meetings. The primary bibliographic databases queried include PubMed, Embase®, Scopus®, and Cochrane Central Register of Controlled Trials. The task force also submits relevant references for consideration, including systematic reviews and guidelines for reference checking. To ensure that relevant publications have been captured, search result identification of references submitted by the task force is examined. Grey literature searches are topic-dependent relying on registries, conference abstracts, preprint servers, and FDA documents including advisory meeting transcripts. The search dates are determinied by the task force and consider sensitivity [@Xu2022], applicability and generalizability to current practice, and resources required to conduct the review. Depending on the key question, searches may not be limited to English language publications [@Jia2020; @Juni2002; @Egger1997; @Mao2020].

### Citation Searching

Backwards searching for studies included in relevant systematic reviews, meta-analyses, and guidelines are considered eligible for inclusion. The selection process outlined below (@fig-checking) is used to identify typically 2 to 3 reviews. Studies included in the those reviews are compiled in a bibliographic database. Those studies not identified in the primary search are subsequently assessed for eligibility. On a selective basis, forward citation searching is conducted using seminal studies to identify citing studies. [Citationchaser](https://estech.shinyapps.io/citationchaser/) [@Haddaway2022] and/or [Paperfecter](https://paperfetcher-paperfetcher-web-app-paperfetcher-app-0w0vu2.streamlit.app/) [@Pallath2023] are used to facility citation searching.

![Approach to backward citation searching.](assets/referenceChecking.png){#fig-checking fig-align="left" width="500"}

### Task Force

The task force is given the opportunity to submit potentially relevant primary studies, guidelines, systematic reviews, and meta-analyses. The non-primary research are included in the reference checking process and the remainder considered in the standard selection process.

### Retracted Publications

Identifying retracted publications is critical to assuring the integrity of the systematic review. Accordingly, searches for retractions of included studies are conducted using relevant search terms (eg, [see this guide](https://ambulance.libguides.com/retracted)) and the [Retraction Watch Database](http://retractiondatabase.org/RetractionSearch.aspx?) (can be facilitated using Zotero's [Retracted Items](https://www.zotero.org/blog/retracted-item-notifications/) feature).

### Deduplication

Deduplication is performed using EndNote™ (used as the primary bibliographic database) and a dedicated systematic review software platform (DistillerSR).

## Study Selection

Based on the inclusion-exclusion criteria (study design and PICOTS), study selection is performed by reviewing titles and abstracts. The semi-automated predictive tool for title and abstract screening implemented in DistillerSR is utilized. [@Polanin2019] If the number of references is exceedingly large (eg, \> 10,000 or 15,000), screening may be truncated when inclusion predictions for the remaining unscreened references are low (eg, less than 2% to 3%). Full-text review of potentially relevant publications is then conducted with reasons for exclusion at the full-text stage are recorded using a standard set of justifications.

Study designs considered eligible for specific key questions are determined by the questions, PICOTS, and evidence availability. For example, although randomized designs generally offer the most convincing evidence, if few address a particular question/PICOTS, nonrandomized designs may be included. Similarly, nonrandomized designs may be included for evaluation of harms. Case reports and case series, conference abstracts, letters not considered brief research reports, non-English publications, and animal studies are generally not considered eligible.

Two reviewers independently apply inclusion-exclusion criteria at each stage with discrepancies resolved by consensus including a third reviewer. Training sets are used to develop agreement concerning the application of inclusion-exclusion criteria.

## Data Extraction/Management

Accurate data extraction, quality control, and data management enhance reproducibility and support valid evidence synthesis. The workflow standardizes data extraction into a dedicated database with an audit log, and once entered, minimizes manual data manipulation (eg, cutting and pasting).

A standard review-specific set of data entry forms, modified for each systematic review, are used:

-   Study characteristics<br/>
-   Study arm data<br/>
-   Dichotomous outcomes including ordinal data/>
-   Continuous outcomes<br/>
-   Likert or other rating scale outcomes<br/>
-   Risk of bias

Data are abstracted by a single reviewer with verification [@PCORI2019 SR-1] of data relevant for quantitative synthesis and rating (GRADEing) the strength of evidence. Figures are digitized as necessary to obtain results for synthesis. Data are maintained and edited in DistillerSR, a data dictionary compiled, and then transferred to a local repository for evidence synthesis or reports created using DistillerSR. A sample study characteristics form can be seen [here](assets/studyCharacteristicsGeri.pdf).

## Study Risk of Bias Assessment

Risk of bias for individual studies are evaluated using tools relevant for the study design. The most commonly used tools include:

-   Randomized clinical trials --- [Cochrane risk of bias tool 2.0](https://sites.google.com/site/riskofbiastool/welcome/rob-2-0-tool/current-version-of-rob-2)
-   Nonrandomized studies of interventions --- [ROBINS-I](https://sites.google.com/site/riskofbiastool/welcome/home) (Risk Of Bias In Non-randomized Studies of Interventions)
-   Diagnostic studies -- [QUADAS-2](https://www.bristol.ac.uk/population-health-sciences/projects/quadas/quadas-2/) (Quality Assessment of Diagnostic Accuracy Studies)

Risk of bias assessments are performed independently by 2 reviewers. Discordances across domain and signalling question results reconciled by consensus including a third reviewer as necessary. Separate risk of bias assessments are conducted for important clinical and patient reported outcomes, but may not be conducted for each of the typically multiple outcomes examined.

::: {.content-hidden when-format="pdf"}
::: {.callout-note collapse="true"}
# Modification History

| Date       | Modifications | Version | Note            |
|:-----------|:--------------|:--------|:----------------|
| 2023-09-11 | None          | 0.1     | Initial version |
:::
:::

[^03_syst_rev-1]: Populations, interventions, comparators, outcomes, timing, and setting.
