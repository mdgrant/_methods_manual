[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ASA Methods Manual (version 0.1)",
    "section": "",
    "text": "Preface\nThis manual represents the methods used to develop the American Society of Anesthesiologists (ASA) practice parameters. It describes processes, procedures, and relevant policies overseen by the Committee on Practice Parameters (CPP).\nAs the methods and approaches evolve, modification are incorporated. Those representing ASA policy or falling under CPP’s authority are included only after administrative approval (eg, matters related to conflict of interest or the choice of strength of evidence framework). Other changes, for example evidence synthesis methods, are the purview of methodologists. They are updated as appropriate or when clarifications are necessary. A history of substantive modifications are listed at the end of each chapter (in the online version only).\nComments, suggestions for additions, or corrections can be sent to Mark Grant.\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nModifications\nVersion\nNote\n\n\n\n\n2023-07-24\nNone\n0.1\nInitial version\n\n\n2024-03-25\nAdded large effect definition to GRADE section. Revised approach to assessing inconsistency. (Borenstein, 2023)\n0.1\n\n\n\n\n\n\n\n\n\n\n\nBorenstein, M. (2023). Avoiding common mistakes in meta-analysis: Understanding the distinct roles of q, i-squared, tau-squared, and the prediction interval in reporting heterogeneity. Res Synth Methods. doi: 10.1002/jrsm.1678"
  },
  {
    "objectID": "01_intro.html#background",
    "href": "01_intro.html#background",
    "title": "1  Introduction",
    "section": "1.1 Background",
    "text": "1.1 Background\nPractice parameters are “strategies for patient management developed by the profession to assist physicians in clinical decision making” (Health Subcommittee Hearing, 1990). The methods described here apply to the development of ASA Practice Guidelines and Practice Advisories. They are similar in approach and methodologies but differ in that the evidence included in Advisories is limited in overall quantity, quality, and consistency. Classifying a guidance document as a Practice Advisory is accordingly based on the supporting systematic review. Differences notwithstanding, both types of guidance adhere to standards for trustworthy clinical practice guidelines (Graham, 2011).\nThe first ASA Practice Guidelines, published in 1993, included management of the difficult airway (Caplan et al., 1993) and pulmonary artery catheterization (Roizen et al., 1993). Initial guideline development followed an approach outlined in the Manual for Clinical Practice Guideline Development (Woolf, 1991) commissioned by the Agency for Health Care Policy and Research1. The approach was state of the art for guideline development at the time detailing 59 steps accompanied by worksheets, table formats, meeting schedules, and goals. While many of those steps became standard practice in ASA guideline development, others were omitted or modified. Over time, some changes to the guideline development process occurred slowly, while others were more frequent, including the strength of evidence ratings (1999, 6 categories;2 2009, 5 categories;3⁠ 2010, 4 categories;4 2013, 3 categories5). ⁠\nFollowing the release of Clinical Practice Guidelines we can Trust (Graham, 2011) from the National Academy of Medicine and Finding What Works in Health Care: Standards for Systematic Review (Eden, 2011), scrutiny of guideline development increased. In that context, the approach and methods outlined here reflect the evolution of the ASA practice parameter enterprise and their adherence to current standards."
  },
  {
    "objectID": "01_intro.html#overview",
    "href": "01_intro.html#overview",
    "title": "1  Introduction",
    "section": "1.2 Overview",
    "text": "1.2 Overview\nFigure 1.1 broadly outlines the structure and main steps followed in developing recommendation for each question and detailed in chapters 3 through # of this manual.\n\n\nFigure 1.1: The ASA process of developing recommendations.\n\n\n\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\nDate\nModification\nVersion\nNote\n\n\n\n\n2023-07-24\nNone\n0.1\nInitial version\n\n\n\n\n\n\n\n\n\n\nCaplan, R. A., Benumof, J. L., Berry, F. A., Blitt, C. D., Bode, R. H., Cheney, F. W., Connis, R. T., Guidry, O. F., & Ovassapian, A. (1993). Practice guidelines for management of the difficult airway. A report by the american society of anesthesiologists task force on management of the difficult airway. Anesthesiology, 78(3), 597–602.\n\n\nEden, J. (2011). Finding what works in health care: Standards for systematic reviews. Washington, D.C.: National Academies Press.\n\n\nGraham, R. (2011). Clinical practice guidelines we can trust. Washington, DC: National Academies Press.\n\n\nHealth Subcommittee Hearing. (1990). Hearing before the subcommittee on health of the committee on ways and means house of representatives one hundred first congress second session april 23, 1990 serial 101-95.\n\n\nRoizen, M. F., Berger, D. I., Gabel, R. A., Gerson, J., Mark, J. B., Parks, R. I., Paulus, D. A., Smith, J. S., & Woolf, S. H. (1993). Practice guidelines for pulmonary artery catheterization. A report by the american society of anesthesiologists task force on pulmonary artery catheterization. Anesthesiology, 78(2), 380–394.\n\n\nWoolf, S. H. (1991). Manual for clinical practice guideline development. Rockville, MD: U.S. Dept. of Health; Human Services, Public Health Service, Agency for Health Care Policy; Research ; Springfield, VA: Available from the National Technical Information Service."
  },
  {
    "objectID": "01_intro.html#footnotes",
    "href": "01_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Predecessor to the Agency for Healthcare Research and Quality (AHRQ).↩︎\nSupportive, suggestive, equivocal, insufficient, inconclusive, silent.↩︎\nA: supportive literature, B: suggestive literature, C: equivocal literature, D: insufficient evidence from literature, Inadequate.↩︎\nA: supportive literature, B: suggestive literature, C: equivocal literature, D: insufficient evidence from literature.↩︎\nCategory A, Category B, Insufficient Evidence.↩︎"
  },
  {
    "objectID": "02_organization.html#committee-on-practice-parameters",
    "href": "02_organization.html#committee-on-practice-parameters",
    "title": "2  Organization",
    "section": "2.1 Committee on Practice Parameters",
    "text": "2.1 Committee on Practice Parameters\nThe Committee on Practice Parameters (CPP) oversees the development of practice parameters, including topic prioritization, reviewing and approving drafts, developing relevant policies (eg, conflict of interest), and evaluating guidelines from other organizations for endorsement1 or affirmation of value2. CPP members are self-appointed and include six active ASA members representing geographically diverse areas, adjunct member(s), and ex officio members from four quality-focused ASA committees. The chair, self-appointed with the ASA president’s approval, is responsible for directing and coordinating all committee activities."
  },
  {
    "objectID": "02_organization.html#task-forces",
    "href": "02_organization.html#task-forces",
    "title": "2  Organization",
    "section": "2.2 Task Forces",
    "text": "2.2 Task Forces\nFollowing a decision to develop a new practice parameter or revise an existing one, the CPP chair forms a task force. A chair (and optional co-chair) leads the task force that includes clinicians, patient representative(s), a librarian/information specialist, methodologists, and the CPP chair. The clinician members are selected based on subject-matter expertise, guideline development and review methodology experience, potential conflicts of interest, and practice diversity. To minimize potential bias across the task force, membership selection seeks diversity in sex, gender, race, ethnicity, practice environment, area of expertise, and geographic region. The task force chair, co-chairs, and CPP chair oversee practice parameter scope, adherence to timelines and ASA methodology."
  },
  {
    "objectID": "02_organization.html#conflict-of-interest",
    "href": "02_organization.html#conflict-of-interest",
    "title": "2  Organization",
    "section": "2.3 Conflict of Interest",
    "text": "2.3 Conflict of Interest\nTask force members are required to disclose all personal and immediate household member3 relationships with industry and other entities that might pose a potential conflict of interest. Disclosures cover the 3 years preceding the first task force meeting and are updated annually through the year following practice parameter publication. Task force members are asked to avoid as much as possible changes in potential conflicts of interest from the time of appointment to the publication. They must verbally disclose any relevant relationships at the beginning of all conference calls and meetings. Employees of industry, part- or full-time, are prohibited from task force membership.\nA task force member has a relevant relationship which is considered a conflict of interest when:\n\nThe relationship or interest relates to the same or similar subject matter, intellectual property, asset, topic, or issue addressed in the practice parameter.\nThe company/entity with whom the relationship exists makes a drug, drug class, or device addressed by the task force makes a drug or device that competes for use with a product addressed in the practice parameter.\nThe person or household member has a reasonable possibility of financial, professional, or other personal gains as a result of the issues or content addressed by the task force — and is judged to create a risk that a relationship will unduly influence a person’s judgment.\n\nChairs and co-chairs, and at least half of the entire task force (chair, co-chair, other members) must be free of potential conflicts of interest. Task force members without conflicts of interest participate in discussions, drafting, and voting on recommendations. Members with potential conflicts participate in discussions and drafting of documents, but are recused from voting on recommendations related to those conflicts.\nThe disclosure policy can be viewed here."
  },
  {
    "objectID": "02_organization.html#practice-parameter-nomination-and-prioritization",
    "href": "02_organization.html#practice-parameter-nomination-and-prioritization",
    "title": "2  Organization",
    "section": "2.4 Practice Parameter Nomination and Prioritization",
    "text": "2.4 Practice Parameter Nomination and Prioritization\nThe process of deciding practice parameters to update or develop is outlined in Figure 2.1. Existing practice parameters are prioritized annually for updating by ASA leadership, the Anesthesia Patient Safety Foundation (APSF), committee chairs, and CPP members. Topic nominations are solicited from ASA leadership, committee chairs, and APSF in a standardized format. Nominations for new practice parameters are also accepted from others at any time (sent to the CPP chair or submitted to ASA Standards and Guidelines; see template for suggested content).\nApplying evaluation criteria (separate criteria for updating practice parameters and new topics) developed by CPP members, the committee next reviews potential practice parameter updates given the prioritization survey results and new topic nominations. In a final survey conducted following the meeting, each CPP member ranks four top choices.\n\n\nFigure 2.1: Depiction of the prioritization process."
  },
  {
    "objectID": "02_organization.html#developing-practice-parameters",
    "href": "02_organization.html#developing-practice-parameters",
    "title": "2  Organization",
    "section": "2.5 Developing Practice Parameters",
    "text": "2.5 Developing Practice Parameters\nFigure 2.2 outlines the practice parameter development process. An introductory meeting serves to orient the task force chairs and co-chairs to the process, timeline, and the roles of methodologists. Subsequent task force meetings are then devoted to defining the PICOs (populations, interventions, comparators, and outcomes) and key questions questions. A protocol is then drafted by the methodologists and reviewed by the task force 2 to 4 weeks later. The systematic review and evidence synthesis is then conducted, during which time the task force is convened as needed for input and decisions concerning any issues that arise including modifications to the protocol. The methodologists complete the evidence synthesis to inform recommendations. Finally, the practice parameter is drafted, submitted to Anesthesiology for review, public comment is solicited, followed by submission to the ASA Board of Directors for approval and finally the House of Delegates.\n\n\nFigure 2.2: Depiction of the practice parameter development process.\n\n\n\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\nDate\nModifications\nVersion\nNote\n\n\n\n\n2023-07-24\nNone\n0.1\nInitial version"
  },
  {
    "objectID": "02_organization.html#footnotes",
    "href": "02_organization.html#footnotes",
    "title": "2  Organization",
    "section": "",
    "text": "The document generally satisfies ASA’s guideline development requirements, and there is general agreement with all recommendations in the document.↩︎\nGuideline or practice parameter has merit and value but does not generally satisfy ASA’s guideline development requirements, or there is no general agreement with all recommendations in the document.↩︎\nPartner with whom participant has lived for ≥ 1 year in the same home. Dependent or any other related person (by blood or marriage) with whom participant has lived for ≥1 year in the same home.↩︎"
  },
  {
    "objectID": "03_syst_rev.html#protocol",
    "href": "03_syst_rev.html#protocol",
    "title": "3  Systematic Review",
    "section": "3.1 Protocol",
    "text": "3.1 Protocol\nThe protocol, developed collaboratively between the task force and methodologists, guides systematic review conduct, and provides documentation for updates. It includes background material, key questions, PICOTS,1 analytic framework, study inclusion and exclusion criteria, search strategy, and the anticipated approach to evidence synthesis. Depending on the anticipated scope, protocols may be registered on PROSPERO (Booth et al., 2012). However, when the systematic review includes numerous questions and anticipated to require substantial refinement and modifications, registration is omitted. The protocol is included as a supplement to the published practice parameter. (An example draft protocol can be viewed here)."
  },
  {
    "objectID": "03_syst_rev.html#outcome-importance",
    "href": "03_syst_rev.html#outcome-importance",
    "title": "3  Systematic Review",
    "section": "3.2 Outcome Importance",
    "text": "3.2 Outcome Importance\nOutcomes vary in importance according to patient values and preferences (Guyatt et al., 2011). From that perspective, following protocol completion the task force members rate outcome importance for decision-making. The ratings are reviewed by the task force and revised as necessary to achieve consensus. Outcomes are assigned a level — critical, important but not critical, low importance — and then ranked to prioritize those to include in the evidence synthesis. Figure 3.1 and Figure 3.2 illustrate the results obtained from ratings and rankings.\n\n\nFigure 3.1: Prioritization of outcomes for neuromuscular monitoring.\n\n\n\n\n\n\nFigure 3.2: Example of assessing outcome importance rankings in a geriatrics guideline. Rankings for the 5 most important outcomes across 7 key questions (11 respondents with maximum 77 for each outcome rank or any top 5 ranking).\n\n\n\n\n\n\nRows can be reordered according to ranking by clicking on column headers."
  },
  {
    "objectID": "03_syst_rev.html#identifying-literature",
    "href": "03_syst_rev.html#identifying-literature",
    "title": "3  Systematic Review",
    "section": "3.3 Identifying Literature",
    "text": "3.3 Identifying Literature\n\n3.3.1 Database Searches\nA librarian/information specialist develops search strategies after reviewing the protocol and participating in task force meetings. The primary bibliographic databases queried include PubMed, Embase®, Scopus®, and Cochrane Central Register of Controlled Trials. The task force also submits relevant references for consideration, including systematic reviews and guidelines for reference checking. To ensure that relevant publications have been captured, search result identification of references submitted by the task force is examined. Grey literature searches are topic-dependent relying on registries, conference abstracts, preprint servers, and FDA documents including advisory meeting transcripts. The search dates are determinied by the task force and consider sensitivity (Xu et al., 2022), applicability and generalizability to current practice, and resources required to conduct the review. Depending on the key question, searches may not be limited to English language publications (Egger et al., 1997; Jia et al., 2020; Jüni et al., 2002; Mao et al., 2020).\n\n\n3.3.2 Citation Searching\nBackwards searching for studies included in relevant systematic reviews, meta-analyses, and guidelines are considered eligible for inclusion. The selection process outlined below (Figure 3.3) is used to identify typically 2 to 3 reviews. Studies included in the those reviews are compiled in a bibliographic database. Those studies not identified in the primary search are subsequently assessed for eligibility. On a selective basis, forward citation searching is conducted using seminal studies to identify citing studies. Citationchaser (Haddaway et al., 2022) and/or Paperfecter (Pallath et al., 2023) are used to facility citation searching.\n\n\nFigure 3.3: Approach to backward citation searching.\n\n\n\n\n\n3.3.3 Task Force\nThe task force is given the opportunity to submit potentially relevant primary studies, guidelines, systematic reviews, and meta-analyses. The non-primary research are included in the reference checking process and the remainder considered in the standard selection process.\n\n\n3.3.4 Retracted Publications\nIdentifying retracted publications is critical to assuring the integrity of the systematic review. Accordingly, searches for retractions of included studies are conducted using relevant search terms (eg, see this guide) and the Retraction Watch Database (can be facilitated using Zotero’s Retracted Items feature).\n\n\n3.3.5 Deduplication\nDeduplication is performed using EndNote™ (used as the primary bibliographic database) and a dedicated systematic review software platform (DistillerSR)."
  },
  {
    "objectID": "03_syst_rev.html#study-selection",
    "href": "03_syst_rev.html#study-selection",
    "title": "3  Systematic Review",
    "section": "3.4 Study Selection",
    "text": "3.4 Study Selection\nBased on the inclusion-exclusion criteria (study design and PICOTS), study selection is performed by reviewing titles and abstracts. The semi-automated predictive tool for title and abstract screening implemented in DistillerSR is utilized. (Polanin et al., 2019) If the number of references is exceedingly large (eg, &gt; 10,000 or 15,000), screening may be truncated when inclusion predictions for the remaining unscreened references are low (eg, less than 2% to 3%). Full-text review of potentially relevant publications is then conducted with reasons for exclusion at the full-text stage are recorded using a standard set of justifications.\nStudy designs considered eligible for specific key questions are determined by the questions, PICOTS, and evidence availability. For example, although randomized designs generally offer the most convincing evidence, if few address a particular question/PICOTS, nonrandomized designs may be included. Similarly, nonrandomized designs may be included for evaluation of harms. Case reports and case series, conference abstracts, letters not considered brief research reports, non-English publications, and animal studies are generally not considered eligible.\nTwo reviewers independently apply inclusion-exclusion criteria at each stage with discrepancies resolved by consensus including a third reviewer. Training sets are used to develop agreement concerning the application of inclusion-exclusion criteria."
  },
  {
    "objectID": "03_syst_rev.html#data-extractionmanagement",
    "href": "03_syst_rev.html#data-extractionmanagement",
    "title": "3  Systematic Review",
    "section": "3.5 Data Extraction/Management",
    "text": "3.5 Data Extraction/Management\nAccurate data extraction, quality control, and data management enhance reproducibility and support valid evidence synthesis. The workflow standardizes data extraction into a dedicated database with an audit log, and once entered, minimizes manual data manipulation (eg, cutting and pasting).\nA standard review-specific set of data entry forms, modified for each systematic review, are used:\n\nStudy characteristics\nStudy arm data\nDichotomous outcomes including ordinal data/&gt;\nContinuous outcomes\nLikert or other rating scale outcomes\nRisk of bias\n\nData are abstracted by a single reviewer with verification (PCORI, 2019, pp. SR–1) of data relevant for quantitative synthesis and rating (GRADEing) the strength of evidence. Figures are digitized as necessary to obtain results for synthesis. Data are maintained and edited in DistillerSR, a data dictionary compiled, and then transferred to a local repository for evidence synthesis or reports created using DistillerSR. A sample study characteristics form can be seen here."
  },
  {
    "objectID": "03_syst_rev.html#study-risk-of-bias-assessment",
    "href": "03_syst_rev.html#study-risk-of-bias-assessment",
    "title": "3  Systematic Review",
    "section": "3.6 Study Risk of Bias Assessment",
    "text": "3.6 Study Risk of Bias Assessment\nRisk of bias for individual studies are evaluated using tools relevant for the study design. The most commonly used tools include:\n\nRandomized clinical trials — Cochrane risk of bias tool 2.0\nNonrandomized studies of interventions — ROBINS-I (Risk Of Bias In Non-randomized Studies of Interventions)\nDiagnostic studies – QUADAS-2 (Quality Assessment of Diagnostic Accuracy Studies)\n\nRisk of bias assessments are performed independently by 2 reviewers. Discordances across domain and signalling question results reconciled by consensus including a third reviewer as necessary. Separate risk of bias assessments are conducted for important clinical and patient reported outcomes, but may not be conducted for each of the typically multiple outcomes examined.\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\nDate\nModifications\nVersion\nNote\n\n\n\n\n2023-09-11\nNone\n0.1\nInitial version\n\n\n\n\n\n\n\n\n\n\nBooth, A., Clarke, M., Dooley, G., Ghersi, D., Moher, D., Petticrew, M., & Stewart, L. (2012). The nuts and bolts of PROSPERO: An international prospective register of systematic reviews. Syst Rev, 1, 2. doi: 10.1186/2046-4053-1-2\n\n\nEden, J. (2011). Finding what works in health care: Standards for systematic reviews. Washington, D.C.: National Academies Press.\n\n\nEgger, M., Zellweger-Zähner, T., Schneider, M., Junker, C., Lengeler, C., & Antes, G. (1997). Language bias in randomised controlled trials published in english and german. Lancet, 350(9074), 326–329. doi: 10.1016/S0140-6736(97)02419-7\n\n\nGraham, R. (2011). Clinical practice guidelines we can trust. Washington, DC: National Academies Press.\n\n\nGuyatt, G. H., Oxman, A. D., Kunz, R., Atkins, D., Brozek, J., Vist, G., Alderson, P., Glasziou, P., Falck-Ytter, Y., & Schunemann, H. J. (2011). GRADE guidelines: 2. Framing the question and deciding on important outcomes. J Clin Epidemiol, 64(4), 395–400. doi: 10.1016/j.jclinepi.2010.09.012\n\n\nHaddaway, N. R., Grainger, M. J., & Gray, C. T. (2022). Citationchaser: A tool for transparent and efficient forward and backward citation chasing in systematic searching. Res Synth Methods, 13(4), 533–545. doi: 10.1002/jrsm.1563\n\n\nJia, Y., Huang, D., Wen, J., Wang, Y., Rosman, L., Chen, Q., Robinson, K. A., Gagnier, J. J., Ehrhardt, S., & Celentano, D. D. (2020). Assessment of language and indexing biases among chinese-sponsored randomized clinical trials. JAMA Netw Open, 3(5), e205894. doi: 10.1001/jamanetworkopen.2020.5894\n\n\nJüni, P., Holenstein, F., Sterne, J., Bartlett, C., & Egger, M. (2002). Direction and impact of language bias in meta-analyses of controlled trials: Empirical study. Int J Epidemiol, 31(1), 115–123. doi: 10.1093/ije/31.1.115\n\n\nMao, C., & Li, M. (2020). Language bias among chinese-sponsored randomized clinical trials in systematic reviews and meta-analyses-can anything be done? JAMA Netw Open, 3(5), e206370. doi: 10.1001/jamanetworkopen.2020.6370\n\n\nPallath, A., & Zhang, Q. (2023). Paperfetcher: A tool to automate handsearching and citation searching for systematic reviews. Res Synth Methods, 14(2), 323–335. doi: 10.1002/jrsm.1604\n\n\nPCORI. (2019). Patient-centered outcomes research institute methodology standards 11: Standards for systematic reviews. PCORI. Retrieved from https://www.pcori.org/research/about-our-research/research-methodology/pcori-methodology-standards#Systematic%20Reviews\n\n\nPolanin, J. R., Pigott, T. D., Espelage, D. L., & Grotpeter, J. K. (2019). Best practice guidelines for abstract screening large‐evidence systematic reviews and meta‐analyses. Research Synthesis Methods, 10(3), 330–342. Retrieved from http://europepmc.org/abstract/PMC/PMC6771536\n\n\nXu, C., Ju, K., Lin, L., Jia, P., Kwong, J. S. W., Syed, A., & Furuya-Kanamori, L. (2022). Rapid evidence synthesis approach for limits on the search date: How rapid could it be? Res Synth Methods, 13(1), 68–76. doi: 10.1002/jrsm.1525"
  },
  {
    "objectID": "03_syst_rev.html#footnotes",
    "href": "03_syst_rev.html#footnotes",
    "title": "3  Systematic Review",
    "section": "",
    "text": "Populations, interventions, comparators, outcomes, timing, and setting.↩︎"
  },
  {
    "objectID": "04_evidence_syn.html#introduction",
    "href": "04_evidence_syn.html#introduction",
    "title": "4  Evidence Synthesis",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nA single study is rarely sufficient to inform a guideline or policy recommendation1 (Spiegelhalter et al., 2004, p. 267); a synthesis of evidence obtained from multiple studies is required. The evidence synthesis may be qualitative or quantitative ranging from narrative descriptions of study results to pairwise meta-analysis (a single intervention and comparator) or network meta-analysis (multiple interventions or comparators). Regardless of the approach, the purpose of an evidence synthesis is to summarize benefits, harms, and uncertainty (statistical and non-statistical) to inform decisions and recommendations.\nFigure 4.1 depicts how the evidence synthesis is structured for each key question and how results support recommendations. The figure implicitly emphasizes how guideline users have varied needs with respect to detail. Some are interested only in recommendations that include no quantitative information. Many (hopefully most) seek to understand the the summaries detail in the balance tables. Accordingly, these elements are included in the body of the published guideline. Others may want to understand details including GRADE domains, meta-analyses, and how specifics of the synthesis supports recommendations — provided as supplementary materials (eg, see example). Finally, a rare individual may wish to explore analysis or reproduce them — data and code are made available for that purpose.\nIt should be noted that explanatory text in the guideline is by necessity more limited than a singular publication devoted to each key question. However, the degree of detail provided should be sufficient to allow a transparent view to the most discerning or critical reader.\n\n\nFigure 4.1: Schematic of the evidence synthesis in relation to developing recommendations.\n\n\n\n Boxes in gray are included as supplements to the guideline; those in green are part of the publication."
  },
  {
    "objectID": "04_evidence_syn.html#decision-making-frameworks",
    "href": "04_evidence_syn.html#decision-making-frameworks",
    "title": "4  Evidence Synthesis",
    "section": "4.2 Decision-Making Frameworks",
    "text": "4.2 Decision-Making Frameworks\nThe decision-making required to develop recommendations requires a framework or model — a calculus of benefits and harms, how they are valued, and their respective uncertainties. The explicitness of the decision calculus varies (Meltzer et al., 2011). For example, a model can be conceptual existing only in the mind of a decision maker with little or nothing quantitative. On the other extreme, the model can decision-analytic with explicit quantitative inputs and outputs. Like most guideline enterprises, the ASA adopts an approach with qualitative and quantitative elements between the extremes.\nAs outlined in Figure 4.2, after formulating key questions and important outcomes specified, relevant studies are identified, data extracted, and risk of bias appraised. Next, using a quantitative or qualitative synthesis, the strength (certainty/GRADE) of evidence for each outcome is rated. Outcomes are then weighted according to patient values and preferences, considered as a whole, and recommendations formulated. \n\n\nFigure 4.2: Model and approach to evidence synthesis for making recommendations.\n\n\n\nBut “all models are wrong” (Box, 1976) — including GRADE."
  },
  {
    "objectID": "04_evidence_syn.html#quantitative-synthesis",
    "href": "04_evidence_syn.html#quantitative-synthesis",
    "title": "4  Evidence Synthesis",
    "section": "4.3 Quantitative Synthesis",
    "text": "4.3 Quantitative Synthesis\n\nBased on the key question, included studies, clinical and methodological diversity, results are pooled in either pairwise or network meta-analyses. Random effects models are fitted given the goal of estimating unconditional effects (ie, effects not relevant only to the pooled studies) (Hedges et al., 1998). For binomial outcomes, default models use the Mantel-Haenszel method; for continuous outcomes inverse variance weighting. The restricted maximum likelihood estimator is used to estimate between-study variance (Viechtbauer, 2005). For continuous or scale-reported outcomes, if means and standard deviations are unavailable for they are imputed if authors reported medians, interquartile and/or overall ranges for the effects of interest; and if necessary P-values are used to estimate missing standard deviations (Shi et al., 2020). When five or more studies are pooled, the Hartung-Knapp adjustment is applied (Cornell et al., 2014). Network meta-analyses are conducted using frequentist (Balduzzi et al., 2023) or Bayesian methods (Béliveau et al., 2019; Dias et al., 2018) with non-informative priors. Consistency is examined by comparing direct to indirect evidence in the frequentist network meta-analyses and inconsistency models in the Bayesian approach.\nRelative effects as reported as risk ratios for clinical interpretability and continuous outcomes as mean differences or standardized mean differences for outcomes reported with differing scales. When feasible, standardized mean differences are re-expressed on the most common scale used. Statistical heterogeneity is examined using the between study variance and I \\(^2\\) (Rücker et al., 2008) and when relevant and practicable explored in subgroup analysis or meta-regression (Schwarzer et al., 2015; Simon G. Thompson et al., 2002) Small-study effects and the potential for publication bias are examined using funnel plots (comparison-adjusted for network meta-analyses), regression-based tests, and adjustment methods (Balduzzi et al., 2019; Harrer et al., 2021). Owing to its statistical properties, results using odds ratios are used reported for when examining small-study effects for relative effects; sensitivity analyses are performed using risk ratios.\nAnalyses are conducted using R (R Core Team, 2023) in a reproducible manner and made publicly available when the practice parameter is completed.\n\n4.3.1 Combining\n\n\n4.3.2 Harms/Adverse Events\nComparative harms are pooled as either relative or absolute effects according to the frequency of events. For rare events (eg, mortality) risk differences are most often used.\nAbsolute risk differences may be examined, typically for complications, but because event rates can vary across studies relative effects are the basis for assessing the strength of evidence.\n\n\n4.3.3 Sensitivity Analyses\nAlthough useful, meta-analytic results are not without limitations (Ioannidis, 2016; Maclure et al., 2001) The robustness of meta-analytic results requires consideration. Aspects include model decisions, small-study effects, influential studies and other factors (eg, secular trends, subgroups).\nModeling decisions include the choice of effect measure, estimators, use of Hartung-Knapp adjustment, continuity corrections for rare events, and even considering studies without events (can be important for harms). As relevant, particularly when uncertainty in a pooled effect appears unclear, how each of these choices impact the range of plausible effects may be examined.\nSmall-study effects are particularly important as they may represent publication bias or selective reporting and can affect the strength of evidence. There is, however, no test for publication bias or selective reporting. The presence of small-study effects offer clues to their potential presence, but require a sufficient number of studies (eg, 10 or more) to effectively examine. Additionally, there are multiple tests (Begg et al., 1994; Egger et al., 1997; Macaskill et al., 2001; Peters et al., 2006; Pustejovsky et al., 2019; Sterne et al., 2011; S. G. Thompson et al., 1999) and adjustment methods — trim and fill (Duval et al., 2000), PET-PEESE (Stanley et al., 2014), limit meta-analyis (Rücker et al., 2011), P-curves (Simonsohn et al., 2014), and selection models (Copas et al., 2014) that can utilized — sometimes offering conflicting results.\nWe adopt a pragmatic approach to sensitivity analyses. If a pooled result is obtained from 10 or more studies, the most appropriate choice of a regression-based test is reported. But if more than one could be used and results differ both are noted. For adjustment, a limit meta-analysis is our method of choice superimposed on a funnel plot (others may be used as sensitivity checks). Although the Hartung-Knapp adjustment is applied given 5 or more studies, we recognize that published meta-analyses typically do not. It is therefore important to understand sensitivity of results to its use and note in the interpretation of evidence and GRADEing.\nFinally, despite the wide range of analytic choices our perspective is that a convincing body of evidence should not be materially impacted for a strong recommendation. If it is, then consideration must be incorporated in both rating the strength of evidence and recommendation."
  },
  {
    "objectID": "04_evidence_syn.html#rating-the-strength-of-evidence",
    "href": "04_evidence_syn.html#rating-the-strength-of-evidence",
    "title": "4  Evidence Synthesis",
    "section": "4.4 Rating the Strength of Evidence",
    "text": "4.4 Rating the Strength of Evidence\nThe strength (certainty) of evidence for important outcomes is appraised using the Grades of Recommendation, Assessment, Development, and Evaluation (GRADE Schünemann et al., 2013) and American College of Cardiology/American Heart Association (ACC/AHA Halperin et al., 2016) frameworks.\nDifferent conceptual models (Spiegelhalter et al., 2011) underpin these strength of evidence frameworks — certainty of evidence (GRADE) and evidence hierarchy (ACC/AHA). The longstanding evidence hierarchy (pyramid) model asserts that systematic reviews and meta-analyses of randomized clinical trials provide the most convincing evidence followed by randomized clinical trials, observational studies, and case series or case reports. The certainty of evidence model incorporates the hierarchy insofar as it reflects study validity, but defines the strength of evidence in terms of how convinced reviewers are that estimates are close to some “true effect”.\n\n4.4.1 GRADE\n\n\nTable 4.1: GRADE levels of evidence.\n\n\n\n\n\n\nGRADE\nDefinition\n\n\n\n\nHigh\nWe are very confident that the true effect lies close to that of the estimate of the effect.\n\n\nModerate\nWe are moderately confident in the effect estimate: The true effect is likely to be close to the estimate of the effect, but there is a possibility that it is substantially different.\n\n\nLow\nOur confidence in the effect estimate is limited: The true effect may be substantially different from the estimate of the effect.\n\n\nVery low\nWe have very little confidence in the effect estimate: The true effect is likely to be substantially different from the estimate of effect.\n\n\n\n\nIn the GRADE approach, a strength of evidence is determined using an algorithm that includes limitations in the body of evidence (bias, inconsistency, imprecision, indirectness, publication bias) and factors that can increase confidence in effects2 (large or very large effect magnitude, dose-response, and plausible residual confounding). According to study limitations, the strength of evidence may be rated down 1 or 2 levels according to study limitations from a starting rating of high for RCTs. Evidence from observational studies begin with a low rating and may be rated down for limitations or rated up because of effect magnitude, dose-response, or the impact of plausible residual confounding. GRADE guidance for rating the certainty of evidence up or down is followed, with some additions. Inconsistency (unexplained heterogeneity) of pooled effects is judged by examining heterogeneity based on the between-study variance \\(\\tau^2\\) and prediction intervals. (Borenstein, 2023) Owing to its well-described limitations, I \\(^2\\) and some categorization of it’s magnitude (eg, small, moderate, or large), is not used as a determinant of heterogeneity (Rücker et al., 2008). Note also that \\(\\tau^2\\) and I \\(^2\\) vary by effect (associational) measures (eg, risk versus odds ratios and mean versus standardized mean differences).\n\n\nFigure 4.3: Process of GRADEing the strength (certainty) of evidence (after Balshem et al., 2011).\n\n\n\n GRADE defines a risk ratio risk exceeding 2.0 (or less than 0.5) as a large effect and greater than 5.0 (less than 0.2) very large. \n\n4.4.1.1 Strength of Evidence Network Meta-analyses\nThere different approaches to assessing whether evidence from network meta-analyses is convincing including GRADE, (Brignardello-Petersen et al., 2018, 2021; Brignardello-Petersen, Murad, et al., 2019; Brignardello-Petersen, Mustafa, et al., 2019; Hao et al., 2023) CINeMA, (Nikolakopoulou et al., 2020; Papakonstantinou et al., 2020) and threshold analysis (Phillippo et al., 2019). A concise summary of using each in developing recommendations can be found in Welton et al (Welton et al., 2020) (from the NICE Decision Support Unit). We use CINeMA following as a coherent and practicable approach.3\n\n\n4.4.1.2 Note on Nonrandomized Designs, ROBINS-I, and GRADE\nIn 2019, the GRADE working group offered “guidance regarding how systematic review authors, guideline developers, and health technology assessment practitioners using GRADE might approach the use of ROBINS-I as part of the certainty rating process” (Schünemann et al., 2019). They suggested owing to differences in the ROBINS-I tool (referent to trial emulation) that when it is used, the GRADE for a body of evidence from non-randomized designs should start at high, not low strength of evidence. Although a rationale is offered, our view is that this guidance is not appropriate (potential for additional down GRADEing due to confounding and selection bias). No matter how careful the conduct and analysis of a non-randomized study, the assumption of no unmeasured confounding is unverifiable (Schulz et al., 2023) (in stark contrast to randomized designs). On this basis, it is logically inconsistent to equate randomized and non-randomized designs. Additionally, the guidance introduces dependence of GRADE on the risk of bias tool — absent in its original formulation. We choose to review carefully our appraisals of nonrandomized evidence to avoid overzealous down GRADEing, but retain GRADE in its original formulation.\n\n\n4.4.1.3 Other particulars regarding application of GRADE domain ratings and summary\n\nInconsistency:\n\nWhen there is a single study inconsistency is rated very serious and for 2 studies serious. The rationale is that at least 3 studies are required to begin to assess the direction of effect. This rule can be modified for large multi-center studies.\nPrediction intervals are used to assess inconsistency. I 2 values are included in forest plots as part of the evidence synthesis.\n\nIf randomized and nonrandomized results are combined (eg, for harms) the body of evidence is treated as observational beginning at a low GRADE.\nA result described as inconclusive is intended to convey that, in our judgment, the confidence interval allows concluding either comparator was clinically meaningfully superior.\nA result described as favoring neither comparator reflects that the range of effects was judged to exclude a clinically meaningful difference.\n\n\n\n4.4.1.4 Comment on GRADE\nGRADE is complex. The web-based handbook spans over 40,000 words and there are now over 30 explanatory publications. The 4-level quality of evidence ratings imposes cutoffs for what is in reality a continuous scale — ratings near the cutoffs are less certain than might be evident. We sometimes struggle assigning a GRADE. Accordingly, the categorical GRADE (high, moderate, low, very low) reflects in effect the mode of a distribution. Uncertainty is not conveyed — for example (Llewellyn et al., 2015; Stewart et al., 2015).\n\n\n\n4.4.2 ACC/AHA\n\n\nTable 4.2: ACC/AHA levels of evidence.\n\n\n\n\n\n\nLevel\nDefinition\n\n\n\n\nA\nHigh-quality evidence from more than 1 RCTs. Meta-analyses of high-quality RCTs. One or more RCTs corroborated by high-quality registry studies.\n\n\nB-R\nModerate-quality evidence from 1 or more randomized controlled trials. Meta-analyses of moderate-quality RCTs.\n\n\nB-NR\nModerate-quality evidence from 1 or more well-designed, well-executed nonrandomized studies, observational studies, or registry studies. Meta-analyses of such studies.\n\n\nC-LD\nRandomized or nonrandomized observational or registry studies with limitations of design or execution. Meta-analyses of such studies. Physiological or mechanistic studies in human subjects.\n\n\nC-EO\nConsensus of expert opinion based on clinical experience when evidence is insufficient, vague, or conflicting.\n\n\n\n\n RCT: randomized clinical trial; NR: nonrandomized; LD: limited data; EO: expert opinion \nACC/AHA ratings are based on an evidence hierarchy approach. Its 5-level scheme (A, B-R, B-NR, C-LD, C-EO) considers study design (RCT, observational, mechanistic) and quality, number of studies, meta-analytic results, expert opinion and clinical experience (Table 4.2). The framework explicitly allows for consideration of mechanistic studies conducted in humans (Goodman et al., 2013). Unlike GRADE guidance for arriving at a strength of evidence is limited to the level of evidence definitions. The ratings were recently revised from an A, B, C scheme by expanding B and C into 2 subcategories and adding E for expert opinion (Jacobs et al., 2014).\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nModifications\nVersion\nNote\n\n\n\n\n2023-09-11\nNone\n0.1\nInitial version\n\n\n2024-02-15\nNotes concerning GRADE\n\nUpdated\n\n\n\nNetwork strength of evidence.\n\nUpdated\n\n\n\n\n\n\n\n\n\n\nBalduzzi, S., Rücker, G., Nikolakopoulou, A., Papakonstantinou, T., Salanti, G., Efthimiou, O., & Schwarzer, G. (2023). Netmeta: An r package for network meta-analysis using frequentist methods. Journal of Statistical Software, 106, 1–40.\n\n\nBalduzzi, S., Rücker, G., & Schwarzer, G. (2019). How to perform a meta-analysis with r: A practical tutorial. BMJ Ment Health, 22(4), 153–160.\n\n\nBalshem, H., Helfand, M., Schunemann, H. J., Oxman, A. D., Kunz, R., Brozek, J., Vist, G. E., Falck-Ytter, Y., Meerpohl, J., Norris, S., & Guyatt, G. H. (2011). GRADE guidelines: 3. Rating the quality of evidence [Journal Article]. J Clin Epidemiol, 64(4), 401–406. doi: 10.1016/j.jclinepi.2010.07.015\n\n\nBegg, C. B., & Mazumdar, M. (1994). Operating characteristics of a rank correlation test for publication bias. Biometrics, 50(4), 1088–1101.\n\n\nBéliveau, A., Boyne, D. J., Slater, J., Brenner, D., & Arora, P. (2019). BUGSnet: An r package to facilitate the conduct and reporting of bayesian network meta-analyses. BMC Med Res Methodol, 19(1), 196. doi: 10.1186/s12874-019-0829-2\n\n\nBorenstein, M. (2023). Avoiding common mistakes in meta-analysis: Understanding the distinct roles of q, i-squared, tau-squared, and the prediction interval in reporting heterogeneity. Res Synth Methods. doi: 10.1002/jrsm.1678\n\n\nBox, G. E. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.\n\n\nBrignardello-Petersen, R., Bonner, A., Alexander, P. E., Siemieniuk, R. A., Furukawa, T. A., Rochwerg, B., Hazlewood, G. S., Alhazzani, W., Mustafa, R. A., Murad, M. H., Puhan, M. A., Schünemann, H. J., & Guyatt, G. H. (2018). Advances in the GRADE approach to rate the certainty in estimates from a network meta-analysis. J Clin Epidemiol, 93, 36–44. doi: 10.1016/j.jclinepi.2017.10.005\n\n\nBrignardello-Petersen, R., Guyatt, G. H., Mustafa, R. A., Chu, D. K., Hultcrantz, M., Schünemann, H. J., & Tomlinson, G. (2021). GRADE guidelines 33: Addressing imprecision in a network meta-analysis. J Clin Epidemiol, 139, 49–56. doi: 10.1016/j.jclinepi.2021.07.011\n\n\nBrignardello-Petersen, R., Murad, M. H., Walter, S. D., McLeod, S., Carrasco-Labra, A., Rochwerg, B., Schünemann, H. J., Tomlinson, G., & Guyatt, G. H. (2019). GRADE approach to rate the certainty from a network meta-analysis: Avoiding spurious judgments of imprecision in sparse networks. J Clin Epidemiol, 105, 60–67. doi: 10.1016/j.jclinepi.2018.08.022\n\n\nBrignardello-Petersen, R., Mustafa, R. A., Siemieniuk, R. A. C., Murad, M. H., Agoritsas, T., Izcovich, A., Schünemann, H. J., & Guyatt, G. H. (2019). GRADE approach to rate the certainty from a network meta-analysis: Addressing incoherence. J Clin Epidemiol, 108, 77–85. doi: 10.1016/j.jclinepi.2018.11.025\n\n\nCopas, J., Dwan, K., Kirkham, J., & Williamson, P. (2014). A model-based correction for outcome reporting bias in meta-analysis. Biostatistics, 15(2), 370–383. doi: 10.1093/biostatistics/kxt046\n\n\nCornell, J. E., Mulrow, C. D., Localio, R., Stack, C. B., Meibohm, A. R., Guallar, E., & Goodman, S. N. (2014). Random-effects meta-analysis of inconsistent effects: A time for change. Ann Intern Med, 160(4), 267–270. doi: 10.7326/M13-2886\n\n\nDias, S., Ades, A. E., Welton, N. J., Jansen, J. P., & Sutton, A. J. (2018). Network meta-analysis for decision-making. John Wiley & Sons.\n\n\nDuval, S., & Tweedie, R. (2000). Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis. Biometrics, 56(2), 455–463. doi: 10.1111/j.0006-341x.2000.00455.x\n\n\nEgger, M., Zellweger-Zähner, T., Schneider, M., Junker, C., Lengeler, C., & Antes, G. (1997). Language bias in randomised controlled trials published in english and german. Lancet, 350(9074), 326–329. doi: 10.1016/S0140-6736(97)02419-7\n\n\nGoodman, S. N., & Gerson, J. (2013). Mechanistic evidence in evidence-based medicine: A conceptual framework. In AHRQ Methods for Effective Health Care. AHRQ Methods for Effective Health Care, Agency for Healthcare Research; Quality (US), Rockville (MD).\n\n\nHalperin, J. L., Levine, G. N., Al-Khatib, S. M., Birtcher, K. K., Bozkurt, B., Brindis, R. G., Cigarroa, J. E., Curtis, L. H., Fleisher, L. A., Gentile, F., Gidding, S., Hlatky, M. A., Ikonomidis, J., Joglar, J., Pressler, S. J., & Wijeysundera, D. N. (2016). Further evolution of the ACC/AHA clinical practice guideline recommendation classification system: A report of the american college of cardiology/american heart association task force on clinical practice guidelines. J Am Coll Cardiol, 67(13), 1572–1574. doi: 10.1016/j.jacc.2015.09.001\n\n\nHao, Q., Gao, Y., Zhao, Y., Murad, M. H., Mustafa, R., Ansari, M. T., Schünemann, H. J., Rind, D. M., Brignardello-Petersen, R., & Guyatt, G. (2023). GRADE concept 6: A novel application of external indirect evidence into GRADE ratings of evidence certainty in network meta-analysis. J Clin Epidemiol, 163, 95–101. doi: 10.1016/j.jclinepi.2023.09.006\n\n\nHarrer, M., Cuijpers, P., Furukawa, T., & Ebert, D. (2021). Doing meta-analysis with r: A hands-on guide. Chapman; Hall/CRC.\n\n\nHedges, L. V., & Vevea, J. L. (1998). Fixed-and random-effects models in meta-analysis. Psychological Methods, 3(4), 486. doi: http://dx.doi.org/10.1037/1082-989X.3.4.486\n\n\nIoannidis, J. P. A. (2016). The mass production of redundant, misleading, and conflicted systematic reviews and meta-analyses. Milbank Q, 94(3), 485–514. doi: 10.1111/1468-0009.12210\n\n\nJacobs, A. K., Anderson, J. L., & Halperin, J. L. (2014). The evolution and future of ACC/AHA clinical practice guidelines: A 30-year journey: A report of the American College of Cardiology/American Heart Association Task Force on Practice Guidelines. J Am Coll Cardiol, 64(13), 1373–1384. doi: 10.1016/j.jacc.2014.06.001\n\n\nLlewellyn, A., Whittington, C., Stewart, G., Higgins, J. P., & Meader, N. (2015). The use of Bayesian networks to assess the quality of evidence from research synthesis: 2. Inter-rater reliability and comparison with standard GRADE assessment. PLoS One, 10(12), e0123511. doi: 10.1371/journal.pone.0123511\n\n\nMacaskill, P., Walter, S. D., & Irwig, L. (2001). A comparison of methods to detect publication bias in meta-analysis. Stat Med, 20(4), 641–654. doi: 10.1002/sim.698\n\n\nMaclure, M., & Schneeweiss, S. (2001). Causation of bias: The episcope. Epidemiology, 12(1), 114–122. doi: 10.1097/00001648-200101000-00019\n\n\nMeltzer, D. O., Hoomans, T., Chung, J. W., & Basu, A. (2011). Minimal modeling approaches to value of information analysis for health research. Med Decis Making, 31(6), E1–E22. doi: 10.1177/0272989X11412975\n\n\nNikolakopoulou, A., Higgins, J. P. T., Papakonstantinou, T., Chaimani, A., Del Giovane, C., Egger, M., & Salanti, G. (2020). CINeMA: An approach for assessing confidence in the results of a network meta-analysis. PLoS Med, 17(4), e1003082. doi: 10.1371/journal.pmed.1003082\n\n\nPapakonstantinou, T., Nikolakopoulou, A., Higgins, J. P. T., Egger, M., & Salanti, G. (2020). CINeMA: Software for semiautomated assessment of the confidence in the results of network meta-analysis. Campbell Syst Rev, 16(1), e1080. doi: 10.1002/cl2.1080\n\n\nPeters, J. L., Sutton, A. J., Jones, D. R., Abrams, K. R., & Rushton, L. (2006). Comparison of two methods to detect publication bias in meta-analysis. JAMA, 295(6), 676–680. doi: 10.1001/jama.295.6.676\n\n\nPhillippo, D. M., Dias, S., Welton, N. J., Caldwell, D. M., Taske, N., & Ades, A. E. (2019). Threshold analysis as an alternative to GRADE for assessing confidence in guideline recommendations based on network meta-analyses. Ann Intern Med. doi: 10.7326/M18-3542\n\n\nPustejovsky, J. E., & Rodgers, M. A. (2019). Testing for funnel plot asymmetry of standardized mean differences. Res Synth Methods, 10(1), 57–71. doi: 10.1002/jrsm.1332\n\n\nR Core Team. (2023). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from https://www.R-project.org/\n\n\nRücker, G., Schwarzer, G., Carpenter, J. R., Binder, H., & Schumacher, M. (2011). Treatment-effect estimates adjusted for small-study effects via a limit meta-analysis. Biostatistics, 12(1), 122–142. doi: 10.1093/biostatistics/kxq046\n\n\nRücker, G., Schwarzer, G., Carpenter, J. R., & Schumacher, M. (2008). Undue reliance on I\\(^2\\) in assessing heterogeneity may mislead. BMC Med Res Methodol, 8, 79. doi: 10.1186/1471-2288-8-79\n\n\nSchulz, J., Moodie, E. E. M., & Shortreed, S. M. (2023). NO UNMEASURED CONFOUNDING: KNOWN UNKNOWNS OR… NOT? Am J Epidemiol, 192(9), 1604–1605. doi: 10.1093/aje/kwad133\n\n\nSchünemann, H., Broz̀ek, J., Guyatt, G., & Oxman, A. (2013). GRADE Handbook. Retrieved from https://gdt.gradepro.org/app/handbook/handbook.html\n\n\nSchünemann, H., Cuello, C., Akl, E. A., Mustafa, R. A., Meerpohl, J. J., Thayer, K., Morgan, R. L., Gartlehner, G., Kunz, R., Katikireddi, S. V., Sterne, J., Higgins, J. P., Guyatt, G., & GRADE Working Group. (2019). GRADE guidelines: 18. How ROBINS-i and other tools to assess risk of bias in nonrandomized studies should be used to rate the certainty of a body of evidence. J Clin Epidemiol, 111, 105–114. doi: 10.1016/j.jclinepi.2018.01.012\n\n\nSchwarzer, G., Carpenter, J. R., & Rücker, G. (2015). Meta-analysis with R. Springer.\n\n\nShi, J., Luo, D., Weng, H., Zeng, X.-T., Lin, L., Chu, H., & Tong, T. (2020). Optimally estimating the sample standard deviation from the five-number summary. Res Synth Methods, 11(5), 641–654. doi: 10.1002/jrsm.1429\n\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve and effect size: Correcting for publication bias using only significant results. Perspect Psychol Sci, 9(6), 666–681. doi: 10.1177/1745691614553988\n\n\nSpiegelhalter, D. J., Abrams, K. R., & Myles, J. P. (2004). Bayesian approaches to clinical trials and health care evaluation. Wiley.\n\n\nSpiegelhalter, D. J., & Riesch, H. (2011). Don’t know, can’t know: Embracing deeper uncertainties when analysing risks. Philos Trans A Math Phys Eng Sci, 369(1956), 4730–4750. doi: 10.1098/rsta.2011.0163\n\n\nStanley, T. D., & Doucouliagos, H. (2014). Meta-regression approximations to reduce publication selection bias. Res Synth Methods, 5(1), 60–78. doi: 10.1002/jrsm.1095\n\n\nSterne, J. A. C., Sutton, A. J., Ioannidis, J. P. A., Terrin, N., Jones, D. R., Lau, J., Carpenter, J., Rücker, G., Harbord, R. M., Schmid, C. H., Tetzlaff, J., Deeks, J. J., Peters, J., Macaskill, P., Schwarzer, G., Duval, S., Altman, D. G., Moher, D., & Higgins, J. P. T. (2011). Recommendations for examining and interpreting funnel plot asymmetry in meta-analyses of randomised controlled trials. BMJ, 343, d4002. doi: 10.1136/bmj.d4002\n\n\nStewart, G. B., Higgins, J. P. T., Schünemann, H., & Meader, N. (2015). The use of bayesian networks to assess the quality of evidence from research synthesis: 1. PLoS One, 10(3), e0114497. doi: 10.1371/journal.pone.0114497\n\n\nThompson, Simon G., & Higgins, J. P. T. (2002). How should meta-regression analyses be undertaken and interpreted? Stat Med, 21(11), 1559–1573. doi: 10.1002/sim.1187\n\n\nThompson, S. G., & Sharp, S. J. (1999). Explaining heterogeneity in meta-analysis: A comparison of methods. Stat Med, 18(20), 2693–2708. doi: 10.1002/(sici)1097-0258(19991030)18:20&lt;2693::aid-sim235&gt;3.0.co;2-v\n\n\nViechtbauer, W. (2005). Bias and efficiency of meta-analytic variance estimators in the random-effects model. Journal of Educational and Behavioral Statistics, 30(3), 261–293. doi: 10.3102/10769986030003261\n\n\nWelton, N. J., Phillippo, D. M., Owen, R., Jones, H. J., Dias, S., Bujkiewicz, S., Ades, A. E., & Abrams, K. R. (2020). CHTE2020 sources and synthesis of evidence: Update to evidence synthesis methods. Retrieved from https://www.sheffield.ac.uk/sites/default/files/2022-02/CHTE-2020_final_20April2020_final.pdf"
  },
  {
    "objectID": "04_evidence_syn.html#footnotes",
    "href": "04_evidence_syn.html#footnotes",
    "title": "4  Evidence Synthesis",
    "section": "",
    "text": "“It is unusual for a policy question to be informed by a single study.”↩︎\nApplies primarily to evidence obtained from observational studies.↩︎\nWelton et al note, “In comparison to GRADE-NMA, which is non-sensical, the methodology [CINeMA] is rigorous, easier and less time-consuming to use, and cannot produce incoherent conclusions.”↩︎"
  },
  {
    "objectID": "05_recommendations.html#formulating-recommendations",
    "href": "05_recommendations.html#formulating-recommendations",
    "title": "5  Recommendations",
    "section": "5.1 Formulating Recommendations",
    "text": "5.1 Formulating Recommendations\nRecommendations are formulated using the GRADE approach. The categories include strong in favor, conditional in favor, conditional against, and strong against an intervention. Strong recommendations reflect the Task Force believing all or almost all clinicians would choose (or not choose) the specific action or approach. Conditional recommendations are those where most, but not all, would choose (or not choose) the action or approach. Recommendations are presented in tabular format accompanied by the strength of recommendation and evidence."
  },
  {
    "objectID": "05_recommendations.html#best-practice-statements",
    "href": "05_recommendations.html#best-practice-statements",
    "title": "5  Recommendations",
    "section": "5.2 Best Practice Statements",
    "text": "5.2 Best Practice Statements\nAlthough not included in the formal GRADE guidance, its developers recommend guideline panels distinguish good or best practice statements from guideline recommendations. Best practice statements are those where evidence is inappropriate for a strength of evidence rating.\nBrito et al. (2013)\n\nWhen low quality of evidence suggests benefit in a life-threatening situation.\nWhen low quality of evidence suggests benefit and high quality of evidence suggests harm or with a very high cost.\nWhen low quality of evidence suggests equivalence of 2 alternatives, but high quality evidence of less harm for one of the competing alternatives.\nWhen high quality of evidence suggest equivalence of 2 alternatives and low quality of evidence suggests harm in one alternative.\nWhen low to high quality of evidence suggests benefits in one important outcome (outcome A) and low/very low quality of evidence suggests possibility of harm in critical (outcome B) and the harm regarding outcome B is valued much more highly than any benefit vis-a-vis."
  },
  {
    "objectID": "05_recommendations.html#no-recommendation",
    "href": "05_recommendations.html#no-recommendation",
    "title": "5  Recommendations",
    "section": "5.3 No Recommendation",
    "text": "5.3 No Recommendation\nBretthauer et al. (2018)\nNeumann et al. (2020)"
  },
  {
    "objectID": "05_recommendations.html#soe-sor-discordance",
    "href": "05_recommendations.html#soe-sor-discordance",
    "title": "5  Recommendations",
    "section": "5.4 SOE SOR Discordance",
    "text": "5.4 SOE SOR Discordance\n\n\n\n\nBretthauer, M., & Kalager, M. (2018). When no guideline recommendation is the best recommendation. Lancet, 392(10151), 898–899. doi: 10.1016/S0140-6736(18)31671-4\n\n\nBrito, J. P., Domecq, J. P., Murad, M. H., Guyatt, G. H., & Montori, V. M. (2013). The endocrine society guidelines: When the confidence cart goes before the evidence horse. J Clin Endocrinol Metab, 98(8), 3246–3252. doi: 10.1210/jc.2013-1814\n\n\nNeumann, I., & Schünemann, H. J. (2020). Guideline groups should make recommendations even if the evidence is considered insufficient. CMAJ, 192(2), E23–E24. doi: 10.1503/cmaj.190144"
  },
  {
    "objectID": "06_endorsement.html",
    "href": "06_endorsement.html",
    "title": "6  Support – External Guidance",
    "section": "",
    "text": "Guidance documents submitted by external organizations for support from the ASA are reviewed by the CPP and a committee with content expertise. Support is offered in 3 categories:\n\nEndorsement — The document should generally satisfy ASA’s guideline development requirements and there is general agreement with all recommendations in the document. Guidance developed with an official representative from the ASA and approved through governance procedures are considered in this category.\nAffirmation of Value — External organization’s guidelines or practice parameters that have merit and value but either do not generally satisfy ASA’s guideline development requirements or there is not general agreement with all recommendations in the document. Statements and policy papers may be considered for affirmation of value.\nNo Endorsement or Affirmation of Value — The external organization’s document does not meet ASA guideline development requirements and is not felt to be of benefit to the ASA membership.\n\nAlthough a complete description of the process for determining the category of support is beyond the scope here, the review conducted by the methodologists is outlined as it is an important function.\nAn objective methodological review of approaches used in developing guidance facilitates endorsement decisions. The purpose of the methodological review is not to offer a judgment on whether the guidance document should be endorsed or supported, but to provide a sufficiently detailed appraisal of the methods to allow informed decisions according to the administrative procedures.\nThe methodological review includes two main sections:\n\nAppraisal of conforming to standards for systematic review conduct (Eden, 2011) and developing recommendations (Graham, 2011) using the NEATS (Jue et al., 2019) tool (National Guideline Clearinghouse Extent of Adherence to Trustworthy Standards) for practice guidelines and AGREE-II (Brouwers et al., 2016) modified (Jacobs et al., 2014) for consensus statements.\nA narrative description of the limitations and strengths from a methodological perspective.\n\nDepending on the scope and complexity, reviews are conducted by one or two methodologists.\nExamples of the NEATS and modified AGREE-II appraisals\n\n\nFigure 6.1: Example of NEATS appraisal.\n\n\n\n\n\nFigure 6.2: Example of modified AGREE-II appraisal.\n\n\n\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\nDate\nModifications\nVersion\nNote\n\n\n\n\n2023-09-12\nNone\n0.1\nInitial version\n\n\n\n\n\n\n\n\n\n\nBrouwers, M. C., Kerkvliet, K., Spithoff, K., & AGREE Next Steps Consortium. (2016). The AGREE reporting checklist: A tool to improve reporting of clinical practice guidelines. BMJ, 352, i1152. doi: 10.1136/bmj.i1152\n\n\nEden, J. (2011). Finding what works in health care: Standards for systematic reviews. Washington, D.C.: National Academies Press.\n\n\nGraham, R. (2011). Clinical practice guidelines we can trust. Washington, DC: National Academies Press.\n\n\nJacobs, C., Graham, I. D., Makarski, J., Chassé, M., Fergusson, D., Hutton, B., & Clemons, M. (2014). Clinical practice guidelines and consensus statements in oncology–an assessment of their methodological quality. PLoS One, 9(10), e110469. doi: 10.1371/journal.pone.0110469\n\n\nJue, J. J., Cunningham, S., Lohr, K., Shekelle, P., Shiffman, R., Robbins, C., Nix, M., Coates, V., & Schoelles, K. (2019). Developing and testing the agency for healthcare research and quality’s national guideline clearinghouse extent of adherence to trustworthy standards (NEATS) instrument. Ann Intern Med, 170(7), 480–487. doi: 10.7326/M18-2950"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Balduzzi, S., Rücker, G., Nikolakopoulou, A., Papakonstantinou, T.,\nSalanti, G., Efthimiou, O., & Schwarzer, G. (2023). Netmeta: An r\npackage for network meta-analysis using frequentist methods. Journal\nof Statistical Software, 106, 1–40.\n\n\nBalduzzi, S., Rücker, G., & Schwarzer, G. (2019). How to perform a\nmeta-analysis with r: A practical tutorial. BMJ Ment Health,\n22(4), 153–160.\n\n\nBalshem, H., Helfand, M., Schunemann, H. J., Oxman, A. D., Kunz, R.,\nBrozek, J., Vist, G. E., Falck-Ytter, Y., Meerpohl, J., Norris, S.,\n& Guyatt, G. H. (2011). GRADE guidelines: 3. Rating the quality of\nevidence [Journal Article]. J Clin Epidemiol, 64(4),\n401–406. doi: 10.1016/j.jclinepi.2010.07.015\n\n\nBegg, C. B., & Mazumdar, M. (1994). Operating\ncharacteristics of a rank correlation test for publication bias.\nBiometrics, 50(4), 1088–1101.\n\n\nBéliveau, A., Boyne, D. J., Slater, J., Brenner, D., & Arora, P.\n(2019). BUGSnet: An r package to facilitate the conduct and reporting of\nbayesian network meta-analyses. BMC Med Res Methodol,\n19(1), 196. doi: 10.1186/s12874-019-0829-2\n\n\nBooth, A., Clarke, M., Dooley, G., Ghersi, D., Moher, D., Petticrew, M.,\n& Stewart, L. (2012). The nuts and bolts of PROSPERO: An\ninternational prospective register of systematic reviews. Syst\nRev, 1, 2. doi: 10.1186/2046-4053-1-2\n\n\nBox, G. E. (1976). Science and statistics. Journal of the American\nStatistical Association, 71(356), 791–799.\n\n\nBretthauer, M., & Kalager, M. (2018). When no guideline\nrecommendation is the best recommendation. Lancet,\n392(10151), 898–899. doi: 10.1016/S0140-6736(18)31671-4\n\n\nBrignardello-Petersen, R., Bonner, A., Alexander, P. E., Siemieniuk, R.\nA., Furukawa, T. A., Rochwerg, B., Hazlewood, G. S., Alhazzani, W.,\nMustafa, R. A., Murad, M. H., Puhan, M. A., Schünemann, H. J., &\nGuyatt, G. H. (2018). Advances in the GRADE approach to rate the\ncertainty in estimates from a network meta-analysis. J Clin\nEpidemiol, 93, 36–44. doi: 10.1016/j.jclinepi.2017.10.005\n\n\nBrignardello-Petersen, R., Guyatt, G. H., Mustafa, R. A., Chu, D. K.,\nHultcrantz, M., Schünemann, H. J., & Tomlinson, G. (2021). GRADE\nguidelines 33: Addressing imprecision in a network meta-analysis. J\nClin Epidemiol, 139, 49–56. doi: 10.1016/j.jclinepi.2021.07.011\n\n\nBrignardello-Petersen, R., Murad, M. H., Walter, S. D., McLeod, S.,\nCarrasco-Labra, A., Rochwerg, B., Schünemann, H. J., Tomlinson, G.,\n& Guyatt, G. H. (2019). GRADE approach to rate the certainty from a\nnetwork meta-analysis: Avoiding spurious judgments of imprecision in\nsparse networks. J Clin Epidemiol, 105, 60–67. doi: 10.1016/j.jclinepi.2018.08.022\n\n\nBrignardello-Petersen, R., Mustafa, R. A., Siemieniuk, R. A. C., Murad,\nM. H., Agoritsas, T., Izcovich, A., Schünemann, H. J., & Guyatt, G.\nH. (2019). GRADE approach to rate the certainty from a network\nmeta-analysis: Addressing incoherence. J Clin Epidemiol,\n108, 77–85. doi: 10.1016/j.jclinepi.2018.11.025\n\n\nBrito, J. P., Domecq, J. P., Murad, M. H., Guyatt, G. H., & Montori,\nV. M. (2013). The endocrine society guidelines: When the confidence cart\ngoes before the evidence horse. J Clin Endocrinol Metab,\n98(8), 3246–3252. doi: 10.1210/jc.2013-1814\n\n\nBrouwers, M. C., Kerkvliet, K., Spithoff, K., & AGREE Next Steps\nConsortium. (2016). The AGREE reporting checklist: A tool\nto improve reporting of clinical practice guidelines. BMJ,\n352, i1152. doi: 10.1136/bmj.i1152\n\n\nCaplan, R. A., Benumof, J. L., Berry, F. A., Blitt, C. D., Bode, R. H.,\nCheney, F. W., Connis, R. T., Guidry, O. F., & Ovassapian, A.\n(1993). Practice guidelines for management of the difficult airway. A\nreport by the american society of anesthesiologists task force on\nmanagement of the difficult airway. Anesthesiology,\n78(3), 597–602.\n\n\nCopas, J., Dwan, K., Kirkham, J., & Williamson, P. (2014). A\nmodel-based correction for outcome reporting bias in meta-analysis.\nBiostatistics, 15(2), 370–383. doi: 10.1093/biostatistics/kxt046\n\n\nCornell, J. E., Mulrow, C. D., Localio, R., Stack, C. B., Meibohm, A.\nR., Guallar, E., & Goodman, S. N. (2014). Random-effects\nmeta-analysis of inconsistent effects: A time for change. Ann Intern\nMed, 160(4), 267–270. doi: 10.7326/M13-2886\n\n\nDias, S., Ades, A. E., Welton, N. J., Jansen, J. P., & Sutton, A. J.\n(2018). Network meta-analysis for decision-making. John Wiley\n& Sons.\n\n\nDuval, S., & Tweedie, R. (2000). Trim and fill: A simple\nfunnel-plot-based method of testing and adjusting for publication bias\nin meta-analysis. Biometrics, 56(2), 455–463. doi: 10.1111/j.0006-341x.2000.00455.x\n\n\nEden, J. (2011). Finding what works in health care: Standards for\nsystematic reviews. Washington, D.C.: National Academies Press.\n\n\nEgger, M., Zellweger-Zähner, T., Schneider, M., Junker, C., Lengeler,\nC., & Antes, G. (1997). Language bias in randomised controlled\ntrials published in english and german. Lancet,\n350(9074), 326–329. doi: 10.1016/S0140-6736(97)02419-7\n\n\nGoodman, S. N., & Gerson, J. (2013). Mechanistic evidence\nin evidence-based medicine: A conceptual framework. In AHRQ Methods\nfor Effective Health Care. AHRQ Methods for Effective Health Care,\nAgency for Healthcare Research; Quality (US), Rockville (MD).\n\n\nGraham, R. (2011). Clinical practice guidelines we can trust.\nWashington, DC: National Academies Press.\n\n\nGuyatt, Gordon H., Alonso-Coello, P., Schünemann, H. J., Djulbegovic,\nB., Nothacker, M., Lange, S., Murad, M. H., & Akl, E. A. (2016).\nGuideline panels should seldom make good practice statements: Guidance\nfrom the GRADE working group. J Clin Epidemiol, 80,\n3–7. doi: 10.1016/j.jclinepi.2016.07.006\n\n\nGuyatt, G. H., Oxman, A. D., Kunz, R., Atkins, D., Brozek, J., Vist, G.,\nAlderson, P., Glasziou, P., Falck-Ytter, Y., & Schunemann, H. J.\n(2011). GRADE guidelines: 2. Framing the question and deciding on\nimportant outcomes. J Clin Epidemiol, 64(4), 395–400.\ndoi: 10.1016/j.jclinepi.2010.09.012\n\n\nHaddaway, N. R., Grainger, M. J., & Gray, C. T. (2022).\nCitationchaser: A tool for transparent and efficient forward and\nbackward citation chasing in systematic searching. Res Synth\nMethods, 13(4), 533–545. doi: 10.1002/jrsm.1563\n\n\nHalperin, J. L., Levine, G. N., Al-Khatib, S. M., Birtcher, K. K.,\nBozkurt, B., Brindis, R. G., Cigarroa, J. E., Curtis, L. H., Fleisher,\nL. A., Gentile, F., Gidding, S., Hlatky, M. A., Ikonomidis, J., Joglar,\nJ., Pressler, S. J., & Wijeysundera, D. N. (2016). Further evolution\nof the ACC/AHA clinical practice guideline recommendation classification\nsystem: A report of the american college of cardiology/american heart\nassociation task force on clinical practice guidelines. J Am Coll\nCardiol, 67(13), 1572–1574. doi: 10.1016/j.jacc.2015.09.001\n\n\nHao, Q., Gao, Y., Zhao, Y., Murad, M. H., Mustafa, R., Ansari, M. T.,\nSchünemann, H. J., Rind, D. M., Brignardello-Petersen, R., & Guyatt,\nG. (2023). GRADE concept 6: A novel application of external indirect\nevidence into GRADE ratings of evidence certainty in network\nmeta-analysis. J Clin Epidemiol, 163, 95–101. doi: 10.1016/j.jclinepi.2023.09.006\n\n\nHarrer, M., Cuijpers, P., Furukawa, T., & Ebert, D. (2021).\nDoing meta-analysis with r: A hands-on guide. Chapman;\nHall/CRC.\n\n\nHealth Subcommittee Hearing. (1990). Hearing before the subcommittee\non health of the committee on ways and means house of representatives\none hundred first congress second session april 23, 1990 serial\n101-95.\n\n\nHedges, L. V., & Vevea, J. L. (1998). Fixed-and random-effects\nmodels in meta-analysis. Psychological Methods, 3(4),\n486. doi: http://dx.doi.org/10.1037/1082-989X.3.4.486\n\n\nIoannidis, J. P. A. (2016). The mass production of redundant,\nmisleading, and conflicted systematic reviews and meta-analyses.\nMilbank Q, 94(3), 485–514. doi: 10.1111/1468-0009.12210\n\n\nJacobs, A. K., Anderson, J. L., & Halperin, J. L. (2014). The\nevolution and future of ACC/AHA clinical practice\nguidelines: A 30-year journey: A report of the American College of Cardiology/American Heart Association\nTask Force on Practice Guidelines. J Am Coll Cardiol,\n64(13), 1373–1384. doi: 10.1016/j.jacc.2014.06.001\n\n\nJacobs, C., Graham, I. D., Makarski, J., Chassé, M., Fergusson, D.,\nHutton, B., & Clemons, M. (2014). Clinical practice guidelines and\nconsensus statements in oncology–an assessment of their methodological\nquality. PLoS One, 9(10), e110469. doi: 10.1371/journal.pone.0110469\n\n\nJia, Y., Huang, D., Wen, J., Wang, Y., Rosman, L., Chen, Q., Robinson,\nK. A., Gagnier, J. J., Ehrhardt, S., & Celentano, D. D. (2020).\nAssessment of language and indexing biases among chinese-sponsored\nrandomized clinical trials. JAMA Netw Open, 3(5),\ne205894. doi: 10.1001/jamanetworkopen.2020.5894\n\n\nJue, J. J., Cunningham, S., Lohr, K., Shekelle, P., Shiffman, R.,\nRobbins, C., Nix, M., Coates, V., & Schoelles, K. (2019). Developing\nand testing the agency for healthcare research and quality’s national\nguideline clearinghouse extent of adherence to trustworthy standards\n(NEATS) instrument. Ann Intern Med, 170(7), 480–487.\ndoi: 10.7326/M18-2950\n\n\nJüni, P., Holenstein, F., Sterne, J., Bartlett, C., & Egger, M.\n(2002). Direction and impact of language bias in meta-analyses of\ncontrolled trials: Empirical study. Int J Epidemiol,\n31(1), 115–123. doi: 10.1093/ije/31.1.115\n\n\nLlewellyn, A., Whittington, C., Stewart, G., Higgins, J. P., &\nMeader, N. (2015). The use of Bayesian networks to assess\nthe quality of evidence from research synthesis: 2.\nInter-rater reliability and comparison with standard\nGRADE assessment. PLoS One, 10(12),\ne0123511. doi: 10.1371/journal.pone.0123511\n\n\nMacaskill, P., Walter, S. D., & Irwig, L. (2001). A comparison of\nmethods to detect publication bias in meta-analysis. Stat Med,\n20(4), 641–654. doi: 10.1002/sim.698\n\n\nMaclure, M., & Schneeweiss, S. (2001). Causation of bias: The\nepiscope. Epidemiology, 12(1), 114–122. doi: 10.1097/00001648-200101000-00019\n\n\nMao, C., & Li, M. (2020). Language bias among chinese-sponsored\nrandomized clinical trials in systematic reviews and meta-analyses-can\nanything be done? JAMA Netw Open, 3(5), e206370. doi:\n10.1001/jamanetworkopen.2020.6370\n\n\nMeltzer, D. O., Hoomans, T., Chung, J. W., & Basu, A. (2011).\nMinimal modeling approaches to value of information analysis for health\nresearch. Med Decis Making, 31(6), E1–E22. doi: 10.1177/0272989X11412975\n\n\nNeumann, I., & Schünemann, H. J. (2020). Guideline groups should\nmake recommendations even if the evidence is considered insufficient.\nCMAJ, 192(2), E23–E24. doi: 10.1503/cmaj.190144\n\n\nNikolakopoulou, A., Higgins, J. P. T., Papakonstantinou, T., Chaimani,\nA., Del Giovane, C., Egger, M., & Salanti, G. (2020). CINeMA: An\napproach for assessing confidence in the results of a network\nmeta-analysis. PLoS Med, 17(4), e1003082. doi: 10.1371/journal.pmed.1003082\n\n\nPallath, A., & Zhang, Q. (2023). Paperfetcher: A tool to automate\nhandsearching and citation searching for systematic reviews. Res\nSynth Methods, 14(2), 323–335. doi: 10.1002/jrsm.1604\n\n\nPapakonstantinou, T., Nikolakopoulou, A., Higgins, J. P. T., Egger, M.,\n& Salanti, G. (2020). CINeMA: Software for semiautomated assessment\nof the confidence in the results of network meta-analysis. Campbell\nSyst Rev, 16(1), e1080. doi: 10.1002/cl2.1080\n\n\nPCORI. (2019). Patient-centered outcomes research institute\nmethodology standards 11: Standards for systematic reviews.\nPCORI. Retrieved from https://www.pcori.org/research/about-our-research/research-methodology/pcori-methodology-standards#Systematic%20Reviews\n\n\nPeters, J. L., Sutton, A. J., Jones, D. R., Abrams, K. R., &\nRushton, L. (2006). Comparison of two methods to detect publication bias\nin meta-analysis. JAMA, 295(6), 676–680. doi: 10.1001/jama.295.6.676\n\n\nPhillippo, D. M., Dias, S., Welton, N. J., Caldwell, D. M., Taske, N.,\n& Ades, A. E. (2019). Threshold analysis as an alternative to GRADE\nfor assessing confidence in guideline recommendations based on network\nmeta-analyses. Ann Intern Med. doi: 10.7326/M18-3542\n\n\nPolanin, J. R., Pigott, T. D., Espelage, D. L., & Grotpeter, J. K.\n(2019). Best practice guidelines for abstract screening large‐evidence\nsystematic reviews and meta‐analyses. Research Synthesis\nMethods, 10(3), 330–342. Retrieved from http://europepmc.org/abstract/PMC/PMC6771536\n\n\nPustejovsky, J. E., & Rodgers, M. A. (2019). Testing for funnel plot\nasymmetry of standardized mean differences. Res Synth Methods,\n10(1), 57–71. doi: 10.1002/jrsm.1332\n\n\nR Core Team. (2023). R: A language and environment for statistical\ncomputing. Vienna, Austria: R Foundation for Statistical Computing.\nRetrieved from https://www.R-project.org/\n\n\nRoizen, M. F., Berger, D. I., Gabel, R. A., Gerson, J., Mark, J. B.,\nParks, R. I., Paulus, D. A., Smith, J. S., & Woolf, S. H. (1993).\nPractice guidelines for pulmonary artery catheterization. A report by\nthe american society of anesthesiologists task force on pulmonary artery\ncatheterization. Anesthesiology, 78(2), 380–394.\n\n\nRücker, G., Schwarzer, G., Carpenter, J. R., Binder, H., &\nSchumacher, M. (2011). Treatment-effect estimates adjusted for\nsmall-study effects via a limit meta-analysis. Biostatistics,\n12(1), 122–142. doi: 10.1093/biostatistics/kxq046\n\n\nRücker, G., Schwarzer, G., Carpenter, J. R., & Schumacher, M.\n(2008). Undue reliance on I2 in assessing heterogeneity may\nmislead. BMC Med Res Methodol, 8, 79. doi: 10.1186/1471-2288-8-79\n\n\nSchulz, J., Moodie, E. E. M., & Shortreed, S. M. (2023). NO\nUNMEASURED CONFOUNDING: KNOWN UNKNOWNS OR… NOT? Am J\nEpidemiol, 192(9), 1604–1605. doi: 10.1093/aje/kwad133\n\n\nSchünemann, H., Broz̀ek, J., Guyatt, G., & Oxman, A. (2013).\nGRADE Handbook. Retrieved from https://gdt.gradepro.org/app/handbook/handbook.html\n\n\nSchünemann, H., Cuello, C., Akl, E. A., Mustafa, R. A., Meerpohl, J. J.,\nThayer, K., Morgan, R. L., Gartlehner, G., Kunz, R., Katikireddi, S. V.,\nSterne, J., Higgins, J. P., Guyatt, G., & GRADE Working Group.\n(2019). GRADE guidelines: 18. How ROBINS-i and other tools to assess\nrisk of bias in nonrandomized studies should be used to rate the\ncertainty of a body of evidence. J Clin Epidemiol,\n111, 105–114. doi: 10.1016/j.jclinepi.2018.01.012\n\n\nSchwarzer, G., Carpenter, J. R., & Rücker, G. (2015).\nMeta-analysis with R. Springer.\n\n\nShi, J., Luo, D., Weng, H., Zeng, X.-T., Lin, L., Chu, H., & Tong,\nT. (2020). Optimally estimating the sample standard deviation from the\nfive-number summary. Res Synth Methods, 11(5),\n641–654. doi: 10.1002/jrsm.1429\n\n\nSimonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). P-curve and\neffect size: Correcting for publication bias using only significant\nresults. Perspect Psychol Sci, 9(6), 666–681. doi: 10.1177/1745691614553988\n\n\nSpiegelhalter, D. J., Abrams, K. R., & Myles, J. P. (2004).\nBayesian approaches to clinical trials and health care\nevaluation. Wiley.\n\n\nSpiegelhalter, D. J., & Riesch, H. (2011). Don’t know, can’t know:\nEmbracing deeper uncertainties when analysing risks. Philos Trans A\nMath Phys Eng Sci, 369(1956), 4730–4750. doi: 10.1098/rsta.2011.0163\n\n\nStanley, T. D., & Doucouliagos, H. (2014). Meta-regression\napproximations to reduce publication selection bias. Res Synth\nMethods, 5(1), 60–78. doi: 10.1002/jrsm.1095\n\n\nSterne, J. A. C., Sutton, A. J., Ioannidis, J. P. A., Terrin, N., Jones,\nD. R., Lau, J., Carpenter, J., Rücker, G., Harbord, R. M., Schmid, C.\nH., Tetzlaff, J., Deeks, J. J., Peters, J., Macaskill, P., Schwarzer,\nG., Duval, S., Altman, D. G., Moher, D., & Higgins, J. P. T. (2011).\nRecommendations for examining and interpreting funnel plot asymmetry in\nmeta-analyses of randomised controlled trials. BMJ,\n343, d4002. doi: 10.1136/bmj.d4002\n\n\nStewart, G. B., Higgins, J. P. T., Schünemann, H., & Meader, N.\n(2015). The use of bayesian networks to assess the quality of evidence\nfrom research synthesis: 1. PLoS One, 10(3), e0114497.\ndoi: 10.1371/journal.pone.0114497\n\n\nThompson, Simon G., & Higgins, J. P. T. (2002). How should\nmeta-regression analyses be undertaken and interpreted? Stat\nMed, 21(11), 1559–1573. doi: 10.1002/sim.1187\n\n\nThompson, S. G., & Sharp, S. J. (1999). Explaining heterogeneity in\nmeta-analysis: A comparison of methods. Stat Med,\n18(20), 2693–2708. doi: 10.1002/(sici)1097-0258(19991030)18:20&lt;2693::aid-sim235&gt;3.0.co;2-v\n\n\nViechtbauer, W. (2005). Bias and efficiency of meta-analytic variance\nestimators in the random-effects model. Journal of Educational and\nBehavioral Statistics, 30(3), 261–293. doi: 10.3102/10769986030003261\n\n\nWelton, N. J., Phillippo, D. M., Owen, R., Jones, H. J., Dias, S.,\nBujkiewicz, S., Ades, A. E., & Abrams, K. R. (2020).\nCHTE2020 sources and synthesis of evidence: Update to\nevidence synthesis methods. Retrieved from https://www.sheffield.ac.uk/sites/default/files/2022-02/CHTE-2020_final_20April2020_final.pdf\n\n\nWoolf, S. H. (1991). Manual for clinical practice guideline\ndevelopment. Rockville, MD: U.S. Dept. of Health; Human Services,\nPublic Health Service, Agency for Health Care Policy; Research ;\nSpringfield, VA: Available from the National Technical Information\nService.\n\n\nXu, C., Ju, K., Lin, L., Jia, P., Kwong, J. S. W., Syed, A., &\nFuruya-Kanamori, L. (2022). Rapid evidence synthesis approach for limits\non the search date: How rapid could it be? Res Synth Methods,\n13(1), 68–76. doi: 10.1002/jrsm.1525"
  }
]