[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ASA Methods Manual (version 0.1)",
    "section": "",
    "text": "Preface\nThis manual represents the methods used to develop the American Society of Anesthesiologists (ASA) practice parameters. It describes processes, procedures, and relevant policies overseen by the Committee on Practice Parameters (CPP).\nAs the methods and approaches evolve, modification are incorporated. Those representing ASA policy or falling under CPP’s authority are included only after administrative approval (eg, matters related to conflict of interest or the choice of strength of evidence framework). Other changes, for example evidence synthesis methods, are the purview of methodologists. They are updated as appropriate or when clarifications are necessary. A history of substantive modifications are listed at the end of each chapter (in the online version only).\nComments, suggestions for additions, or corrections can be sent to Mark Grant.\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\nDate\nModifications\nVersion\nNote\n\n\n\n\n2023-07-24\nNone\n0.1\nInitial version"
  },
  {
    "objectID": "01_intro.html#background",
    "href": "01_intro.html#background",
    "title": "1  Introduction",
    "section": "1.1 Background",
    "text": "1.1 Background\nPractice parameters are “strategies for patient management developed by the profession to assist physicians in clinical decision making” (Health Subcommittee Hearing, 1990). The methods described here apply to the development of ASA Practice Guidelines and Practice Advisories. They are similar in approach and methodologies but differ in that the evidence included in Advisories is limited in overall quantity, quality, and consistency. Classifying a guidance document as a Practice Advisory is accordingly based on the supporting systematic review. Differences notwithstanding, both types of guidance adhere to standards for trustworthy clinical practice guidelines (Graham, 2011).\nThe first ASA Practice Guidelines, published in 1993, included management of the difficult airway (Caplan, 1993) and pulmonary artery catheterization (Roizen, 1993). Initial guideline development followed an approach outlined in the Manual for Clinical Practice Guideline Development (Woolf, 1991) commissioned by the Agency for Health Care Policy and Research1. The approach was state of the art for guideline development at the time detailing 59 steps accompanied by worksheets, table formats, meeting schedules, and goals. While many of those steps became standard practice in ASA guideline development, others were omitted or modified. Over time, some changes to the guideline development process occurred slowly, while others were more frequent, including the strength of evidence ratings (1999, 6 categories;2 2009, 5 categories;3⁠ 2010, 4 categories;4 2013, 3 categories5). ⁠\nFollowing the release of Clinical Practice Guidelines we can Trust (Graham, 2011) from the National Academy of Medicine and Finding What Works in Health Care: Standards for Systematic Review (Eden, 2011), scrutiny of guideline development increased. In that context, the approach and methods outlined here reflect the evolution of the ASA practice parameter enterprise and their adherence to current standards."
  },
  {
    "objectID": "01_intro.html#overview",
    "href": "01_intro.html#overview",
    "title": "1  Introduction",
    "section": "1.2 Overview",
    "text": "1.2 Overview\nFigure 1.1 broadly outlines the structure and main steps followed in developing recommendation for each question and detailed in chapters 3 through # of this manual.\n\n\nFigure 1.1: The ASA process of developing recommendations.\n\n\n\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\nDate\nModification\nVersion\nNote\n\n\n\n\n2023-07-24\nNone\n0.1\nInitial version\n\n\n\n\n\n\n\n\n\n\nCaplan, R. A., Benumof, J. L., Berry, F. A., Blitt, C. D., Bode, R. H., Cheney, F. W., … Ovassapian, A. (1993). “Practice guidelines for management of the difficult airway. A report by the american society of anesthesiologists task force on management of the difficult airway.” Anesthesiology, 78(3), 597–602.\n\n\nEden, J. (2011). Finding what works in health care: Standards for systematic reviews. Washington, D.C.: National Academies Press.\n\n\nGraham, R. (2011). Clinical practice guidelines we can trust. Washington, DC: National Academies Press.\n\n\nHealth Subcommittee Hearing. (1990). “Hearing before the subcommittee on health of the committee on ways and means house of representatives one hundred first congress second session april 23, 1990 serial 101-95.”\n\n\nRoizen, M. F., Berger, D. I., Gabel, R. A., Gerson, J., Mark, J. B., Parks, R. I., … Woolf, S. H. (1993). “Practice guidelines for pulmonary artery catheterization. A report by the american society of anesthesiologists task force on pulmonary artery catheterization.” Anesthesiology, 78(2), 380–94.\n\n\nWoolf, S. H. (1991). Manual for clinical practice guideline development. Rockville, MD: U.S. Dept. of Health; Human Services, Public Health Service, Agency for Health Care Policy; Research ; Springfield, VA: Available from the National Technical Information Service."
  },
  {
    "objectID": "01_intro.html#footnotes",
    "href": "01_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Predecessor to the Agency for Healthcare Research and Quality (AHRQ).↩︎\nSupportive, suggestive, equivocal, insufficient, inconclusive, silent.↩︎\nA: supportive literature, B: suggestive literature, C: equivocal literature, D: insufficient evidence from literature, Inadequate.↩︎\nA: supportive literature, B: suggestive literature, C: equivocal literature, D: insufficient evidence from literature.↩︎\nCategory A, Category B, Insufficient Evidence.↩︎"
  },
  {
    "objectID": "02_organization.html#committee-on-practice-parameters",
    "href": "02_organization.html#committee-on-practice-parameters",
    "title": "2  Organization",
    "section": "2.1 Committee on Practice Parameters",
    "text": "2.1 Committee on Practice Parameters\nThe Committee on Practice Parameters (CPP) oversees the development of practice parameters, including topic prioritization, reviewing and approving drafts, developing relevant policies (eg, conflict of interest), and evaluating guidelines from other organizations for endorsement1 or affirmation of value2. CPP members are self-appointed and include six active ASA members representing geographically diverse areas, adjunct member(s), and ex officio members from four quality-focused ASA committees. The chair, self-appointed with the ASA president’s approval, is responsible for directing and coordinating all committee activities."
  },
  {
    "objectID": "02_organization.html#task-forces",
    "href": "02_organization.html#task-forces",
    "title": "2  Organization",
    "section": "2.2 Task Forces",
    "text": "2.2 Task Forces\nFollowing a decision to develop a new practice parameter or revise an existing one, the CPP chair forms a task force. A chair (and optional co-chair) leads the task force that includes clinicians, patient representative(s), a librarian/information specialist, methodologists, and the CPP chair. The clinician members are selected based on subject-matter expertise, guideline development and review methodology experience, potential conflicts of interest, and practice diversity. To minimize potential bias across the task force, membership selection seeks diversity in sex, gender, race, ethnicity, practice environment, area of expertise, and geographic region. The task force chair, co-chairs, and CPP chair oversee practice parameter scope, adherence to timelines and ASA methodology."
  },
  {
    "objectID": "02_organization.html#conflict-of-interest",
    "href": "02_organization.html#conflict-of-interest",
    "title": "2  Organization",
    "section": "2.3 Conflict of Interest",
    "text": "2.3 Conflict of Interest\nTask force members are required to disclose all personal and immediate household member3 relationships with industry and other entities that might pose a potential conflict of interest. Disclosures cover the 3 years preceding the first task force meeting and are updated annually through the year following practice parameter publication. Task force members are asked to avoid as much as possible changes in potential conflicts of interest from the time of appointment to the publication. They must verbally disclose any relevant relationships at the beginning of all conference calls and meetings. Employees of industry, part- or full-time, are prohibited from task force membership.\nA task force member has a relevant relationship which is considered a conflict of interest when:\n\nThe relationship or interest relates to the same or similar subject matter, intellectual property, asset, topic, or issue addressed in the practice parameter.\nThe company/entity with whom the relationship exists makes a drug, drug class, or device addressed by the task force makes a drug or device that competes for use with a product addressed in the practice parameter.\nThe person or household member has a reasonable possibility of financial, professional, or other personal gains as a result of the issues or content addressed by the task force — and is judged to create a risk that a relationship will unduly influence a person’s judgment.\n\nChairs and co-chairs, and at least half of the entire task force (chair, co-chair, other members) must be free of potential conflicts of interest. Task force members without conflicts of interest participate in discussions, drafting, and voting on recommendations. Members with potential conflicts participate in discussions and drafting of documents, but are recused from voting on recommendations related to those conflicts.\nThe disclosure policy can be viewed here."
  },
  {
    "objectID": "02_organization.html#practice-parameter-nomination-and-prioritization",
    "href": "02_organization.html#practice-parameter-nomination-and-prioritization",
    "title": "2  Organization",
    "section": "2.4 Practice Parameter Nomination and Prioritization",
    "text": "2.4 Practice Parameter Nomination and Prioritization\nThe process of deciding practice parameters to update or develop is outlined in Figure 2.1. Existing practice parameters are prioritized annually for updating by ASA leadership, the Anesthesia Patient Safety Foundation (APSF), committee chairs, and CPP members. Topic nominations are solicited from ASA leadership, committee chairs, and APSF in a standardized format. Nominations for new practice parameters are also accepted from others at any time (sent to the CPP chair or submitted to ASA Standards and Guidelines; see template for suggested content).\nApplying evaluation criteria (separate criteria for updating practice parameters and new topics) developed by CPP members, the committee next reviews potential practice parameter updates given the prioritization survey results and new topic nominations. In a final survey conducted following the meeting, each CPP member ranks four top choices.\n\n\nFigure 2.1: Depiction of the prioritization process."
  },
  {
    "objectID": "02_organization.html#updating",
    "href": "02_organization.html#updating",
    "title": "2  Organization",
    "section": "2.5 Updating",
    "text": "2.5 Updating\nHere or separate\nASA practice parameters are reviewed annually and updated after approximately 5 years, as determined (prioritized? at the discretion of?) by the Committee on Practice Parameters.\nASA practice parameters are reviewed by the Committee on Practice Parameters not later than 5 years after publication for potential updating. The primary considerations used to determine priority for updating are relevance and new evidence potentially changing care.\nUpdates to practice parameters are considered to those published approximately 5 years prior, or in the event that new data may influence a recommendation and follow the same practice parameter development process.\n\nChanges in evidence on the existing benefits and harms of interventions\nChanges in outcomes considered important\nChanges in available interventions\nChanges in evidence that current practice is optimal\nChanges in values placed on outcomes\nChanges in resources available for health care"
  },
  {
    "objectID": "02_organization.html#developing-practice-parameters",
    "href": "02_organization.html#developing-practice-parameters",
    "title": "2  Organization",
    "section": "2.5 Developing Practice Parameters",
    "text": "2.5 Developing Practice Parameters\nFigure 2.2 outlines the practice parameter development process. An introductory meeting serves to orient the task force chairs and co-chairs to the process, timeline, and the roles of methodologists. Subsequent task force meetings are then devoted to defining the PICOs (populations, interventions, comparators, and outcomes) and key questions questions. A protocol is then drafted by the methodologists and reviewed by the task force 2 to 4 weeks later. The systematic review and evidence synthesis is then conducted, during which time the task force is convened as needed for input and decisions concerning any issues that arise including modifications to the protocol. The methodologists complete the evidence synthesis to inform recommendations. Finally, the practice parameter is drafted, submitted to Anesthesiology for review, public comment is solicited, followed by submission to the ASA Board of Directors for approval and finally the House of Delegates.\n\n\nFigure 2.2: Depiction of the practice parameter development process.\n\n\n\n\n\n\n\n\n\nModification History\n\n\n\n\n\n\n\n\nDate\nModifications\nVersion\nNote\n\n\n\n\n2023-07-24\nNone\n0.1\nInitial version"
  },
  {
    "objectID": "02_organization.html#footnotes",
    "href": "02_organization.html#footnotes",
    "title": "2  Organization",
    "section": "",
    "text": "The document generally satisfies ASA’s guideline development requirements, and there is general agreement with all recommendations in the document.↩︎\nGuideline or practice parameter has merit and value but does not generally satisfy ASA’s guideline development requirements, or there is no general agreement with all recommendations in the document.↩︎\nPartner with whom participant has lived for ≥ 1 year in the same home. Dependent or any other related person (by blood or marriage) with whom participant has lived for ≥1 year in the same home.↩︎"
  },
  {
    "objectID": "03_syst_rev.html#protocol",
    "href": "03_syst_rev.html#protocol",
    "title": "3  Systematic Review",
    "section": "3.1 Protocol",
    "text": "3.1 Protocol\nThe protocol, developed collaboratively between the task force and methodologists, guides systematic review conduct, and provides documentation for updates. It includes background material, key questions, PICOTS,1 analytic framework, study inclusion and exclusion criteria, search strategy, and the anticipated approach to evidence synthesis. Depending on the anticipated scope, protocols may be registered on PROSPERO (Booth, 2012). However, when the systematic review includes numerous questions and anticipated to require substantial refinement and modifications, registration is omitted. The protocol is included as a supplement to the published practice parameter. (An example draft protocol can be viewed here)."
  },
  {
    "objectID": "03_syst_rev.html#outcome-importance",
    "href": "03_syst_rev.html#outcome-importance",
    "title": "3  Systematic Review",
    "section": "3.2 Outcome Importance",
    "text": "3.2 Outcome Importance\nOutcomes vary in importance for decision making and formulating recommendations. Importance incorporates patient preferences and values for those outcomes (Guyatt, 2011). Following protocol completion, task force members independently rank beneficial and harmful outcome importance for decision-making. The rankings are reviewed by the entire task force and revised to achieve consensus. Outcomes are then assigned a level of importance (critical, important but not critical, low importance) and a ranking from 1 to 5 to prioritize the evidence synthesis and inform recommendations.\n\n\nFigure 3.1: Prioritization of outcomes for neuromuscular monitoring.\n\n\n\n\n\n\nFigure 3.2: Example of assessing outcome importance rankings in a geriatrics guideline. Rankings for the 5 most important outcomes across 7 key questions (11 respondents with maximum 77 for each outcome rank or any top 5 ranking).\n\n\n\n\n\n\nNote that rows can be reordered according to ranking by clicking on column headers."
  },
  {
    "objectID": "03_syst_rev.html#identifying-literature",
    "href": "03_syst_rev.html#identifying-literature",
    "title": "3  Systematic Review",
    "section": "3.3 Identifying Literature",
    "text": "3.3 Identifying Literature\n\n3.3.1 Database Searches\nA librarian/information specialist develops search strategies after reviewing the protocol and participating in task force meetings. The primary bibliographic databases queried include PubMed, Embase®, Scopus®, and Cochrane Central Register of Controlled Trials. The task force also submits relevant references for consideration, including systematic reviews and guidelines for reference checking. To ensure that relevant publications have been captured, search result identification of references submitted by the task force is examined. Grey literature searches are topic-dependent relying on registries, conference abstracts, preprint servers, and FDA documents including advisory meeting transcripts. The search dates are determinied by the task force and consider sensitivity (Xu, 2022), applicability and generalizability to current practice, and resources required to conduct the review. Depending on the key question, searches may not be limited to English language publications (Egger, 1997; Jia, 2020; Jüni, 2002; Mao, 2020).\n\n\n3.3.2 Citation Searching\nBackwards searching for studies included in relevant systematic reviews, meta-analyses, and guidelines are considered eligible for inclusion. The selection process outlined below (Figure 3.3) is used to identify typically 2 to 3 reviews. Studies included in the those reviews are compiled in a bibliographic database. Those studies not identified in the primary search are subsequently assessed for eligibility. On a selective basis, forward citation searching is conducted using seminal studies to identify citing studies. Citationchaser is used to facility citation searching (Haddaway, 2022).\n\n\nFigure 3.3: Approach to backward citation searching.\n\n\n\n\n\n3.3.3 Task Force\nThe task force is given the opportunity to submit potentially relevant primary studies, guidelines, systematic reviews, and meta-analyses. The non-primary research are included in the reference checking process and they remainder considered in the standard selection process.\n\n\n3.3.4 Retracted Publications\nIdentifying retracted publications is critical to assuring the integrity of the systematic review. Accordingly, searches for retractions of included studies are conducted using relevant search terms (eg, see this guide) and the Retraction Watch Database (can be facilitated using Zotero’s Retracted Items feature).\n\n\n3.3.5 Deduplication\nDeduplication is performed using EndNote™ (used as the primary bibliographic database) and a dedicated systematic review software platform (DistillerSR)."
  },
  {
    "objectID": "03_syst_rev.html#study-selection",
    "href": "03_syst_rev.html#study-selection",
    "title": "3  Systematic Review",
    "section": "3.4 Study Selection",
    "text": "3.4 Study Selection\nBased on the inclusion-exclusion criteria (study design and PICOTS), study selection is performed by reviewing titles and abstracts. The semi-automated predictive tool for title and abstract screening implemented in DistillerSR is utilized. (Polanin, 2019) If the number of references is exceedingly large (eg, &gt; 10,000 or 15,000), screening may be truncated when inclusion predictions for the remaining unscreened references are low (eg, less than 2% to 3%). Full-text review of potentially relevant publications is then conducted with reasons for exclusion at the full-text stage are recorded using a standard set of justifications.\nStudy designs considered eligible for specific key questions are determined by the questions, PICOTS, and evidence availability. For example, although randomized designs generally offer the most convincing evidence, if few address a particular question/PICOTS, nonrandomized designs may be included. Similarly, nonrandomized designs may be included for evaluation of harms. Case reports and case series, conference abstracts, letters not considered brief research reports, non-English publications, and animal studies are generally not considered eligible.\nTwo reviewers independently apply independently inclusion-exclusion criteria at each stage with discrepancies resolved by consensus including a third reviewer. Training sets are used to develop agreement concerning the application of inclusion-exclusion criteria."
  },
  {
    "objectID": "03_syst_rev.html#data-extractionmanagement",
    "href": "03_syst_rev.html#data-extractionmanagement",
    "title": "3  Systematic Review",
    "section": "3.5 Data Extraction/Management",
    "text": "3.5 Data Extraction/Management\nAccurate data extraction, quality control, and data management enhance reproducibility and support valid evidence synthesis. The workflow standardizes data extraction into a dedicated database with an audit log, and once entered, minimizes manual data manipulation (eg, cutting and pasting).\nA standard review-specific set of data entry forms, modified for each systematic review, are used:\n\nStudy characteristics\nStudy arm data\nDichotomous outcomes (as reported)\nContinuous outcomes (as reported)\nLikert or other rating scale outcomes (as reported)\nRisk of bias\n\nData are abstracted by a single reviewer with verification (PCORI, 2019, pp. SR–1) of data relevant for quantitative synthesis and rating (GRADEing) the strength of evidence. Figures are digitized as necessary to obtain results for synthesis. Data are maintained and edited in DistillerSR, a data dictionary compiled, and then transferred to a local repository for evidence synthesis or reports created using DistillerSR. A sample study characteristics form can be seen here."
  },
  {
    "objectID": "03_syst_rev.html#study-risk-of-bias-assessments",
    "href": "03_syst_rev.html#study-risk-of-bias-assessments",
    "title": "3  Systematic Review",
    "section": "3.6 Study Risk of Bias Assessments",
    "text": "3.6 Study Risk of Bias Assessments\nRisk of bias for individual studies are evaluated using tools relevant for the study design. The most commonly used tools include:\n\nRandomized clinical trials — Cochrane risk of bias tool 2.0\nNonrandomized studies — ROBINS-I (Risk Of Bias In Non-randomized Studies of Interventions)\nDiagnostic studies – QUADAS-2 (Quality Assessment of Diagnostic Accuracy Studies)\n\nRisk of bias assessments are performed independently by 2 reviews and discordances in domain and signalling question results reconciled by concensus including a third reviewer as necessary. Separate risk of bias assessments are conducted for clinical and patient reported outcomes, but may not be conducted for each of the typically multiple outcomes examined.\n\n\n\n\nBooth, A., Clarke, M., Dooley, G., Ghersi, D., Moher, D., Petticrew, M., & Stewart, L. (2012). “The nuts and bolts of PROSPERO: An international prospective register of systematic reviews.” Syst Rev, 1, 2. https://doi.org/10.1186/2046-4053-1-2\n\n\nEden, J. (2011). Finding what works in health care: Standards for systematic reviews. Washington, D.C.: National Academies Press.\n\n\nEgger, M., Zellweger-Zähner, T., Schneider, M., Junker, C., Lengeler, C., & Antes, G. (1997). “Language bias in randomised controlled trials published in english and german.” Lancet, 350(9074), 326–9. https://doi.org/10.1016/S0140-6736(97)02419-7\n\n\nGraham, R. (2011). Clinical practice guidelines we can trust. Washington, DC: National Academies Press.\n\n\nGuyatt, G. H., Oxman, A. D., Kunz, R., Atkins, D., Brozek, J., Vist, G., … Schunemann, H. J. (2011). “GRADE guidelines: 2. Framing the question and deciding on important outcomes.” J Clin Epidemiol, 64(4), 395–400. https://doi.org/10.1016/j.jclinepi.2010.09.012\n\n\nHaddaway, N. R., Grainger, M. J., & Gray, C. T. (2022). “Citationchaser: A tool for transparent and efficient forward and backward citation chasing in systematic searching.” Res Synth Methods, 13(4), 533–545. https://doi.org/10.1002/jrsm.1563\n\n\nJia, Y., Huang, D., Wen, J., Wang, Y., Rosman, L., Chen, Q., … Celentano, D. D. (2020). “Assessment of language and indexing biases among chinese-sponsored randomized clinical trials.” JAMA Netw Open, 3(5), e205894. https://doi.org/10.1001/jamanetworkopen.2020.5894\n\n\nJüni, P., Holenstein, F., Sterne, J., Bartlett, C., & Egger, M. (2002). “Direction and impact of language bias in meta-analyses of controlled trials: Empirical study.” Int J Epidemiol, 31(1), 115–23. https://doi.org/10.1093/ije/31.1.115\n\n\nMao, C., & Li, M. (2020). “Language bias among chinese-sponsored randomized clinical trials in systematic reviews and meta-analyses-can anything be done?” JAMA Netw Open, 3(5), e206370. https://doi.org/10.1001/jamanetworkopen.2020.6370\n\n\nPCORI. (2019). “Patient-centered outcomes research institute methodology standards 11: Standards for systematic reviews.” PCORI. Retrieved from https://www.pcori.org/research/about-our-research/research-methodology/pcori-methodology-standards#Systematic%20Reviews\n\n\nPolanin, J. R., Pigott, T. D., Espelage, D. L., & Grotpeter, J. K. (2019). “Best practice guidelines for abstract screening large‐evidence systematic reviews and meta‐analyses.” Research Synthesis Methods, 10(3), 330–342. Retrieved from http://europepmc.org/abstract/PMC/PMC6771536\n\n\nXu, C., Ju, K., Lin, L., Jia, P., Kwong, J. S. W., Syed, A., & Furuya-Kanamori, L. (2022). “Rapid evidence synthesis approach for limits on the search date: How rapid could it be?” Res Synth Methods, 13(1), 68–76. https://doi.org/10.1002/jrsm.1525"
  },
  {
    "objectID": "03_syst_rev.html#footnotes",
    "href": "03_syst_rev.html#footnotes",
    "title": "3  Systematic Review",
    "section": "",
    "text": "Populations, interventions, comparators, outcomes, timing, and setting.↩︎"
  },
  {
    "objectID": "04_evidence_syn.html#introduction",
    "href": "04_evidence_syn.html#introduction",
    "title": "4  Evidence Synthesis",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nA single study is rarely sufficient to inform a guideline or policy recommendation (Spiegelhalter, 2004); a synthesis of evidence obtained from multiple studies is required. The evidence synthesis may be qualitative or quantitative ranging from narrative descriptions of study results to pairwise meta-analysis (a single intervention and comparator) or network meta-analysis (multiple interventions or comparators). Regardless of the approach, the purpose of an evidence synthesis is to summarize benefits, harms, and uncertainty (statistical and non-statistical) to inform decisions and recommendations."
  },
  {
    "objectID": "04_evidence_syn.html#philosophy",
    "href": "04_evidence_syn.html#philosophy",
    "title": "4  Evidence Synthesis",
    "section": "4.2 Philosophy",
    "text": "4.2 Philosophy\nAn evidence synthesis strives to make the decision calculus as explicit as possible. It should come as close to addressing the key questions as possible. Sometimes direct evidence and indirect evidence can appropriately be examined, it may be valuable.\nAnd when possible, avoid ad-hoc or highly subjective interpretation of evidence.\nNetwork meta-analyses incorporate direct and indirect evidence and may well be the only option.\nAlthough some question the validity of NMA, we are aware of no evidence to support that contention. Moreover, a pairwise meta-analysis is in effect the simpilist case of an NMA.\nObservational data and causal effects. Randomized clinical trials provide the most convincing evidence, but observational studies can offer a decision-maker useful information and sometimes critical.\nBroad structured, but not rigid.\nGoal is good decisions\nCareful sensitivity analyses.\nCan be a blunt instrument at times, but having nothing must leave the decision maker to do the calculus in his or her head, which can be fraught.\nTo that end,\nDecision calculus best to be as explicit as possible;\nCompatibility intervals; probability of the estimate being true = 0"
  },
  {
    "objectID": "04_evidence_syn.html#frameworks-for-decision-making",
    "href": "04_evidence_syn.html#frameworks-for-decision-making",
    "title": "4  Evidence Synthesis",
    "section": "4.3 Frameworks for Decision-Making",
    "text": "4.3 Frameworks for Decision-Making\nThe decision-making required to develop recommendations requires a framework or model — a calculus to summarize the balance of benefits and harms, how each is valued, and their respective uncertainties. The explicitness of this decision calculus varies (Meltzer, 2011). For example, a model can be conceptual in a decision makers mind with little or nothing quantitative. On the other extreme, the model can decision-analytic. Like almost all guideline enterprises, the ASA adopts an approach somewhere in the middle of the two extremes with qualitative and quantitative elements (outlined in Figure 3 using the GRADE approach).\nAfter the key questions are formulated and important outcomes specified, relevant studies are identified, data abstracted, and risk of bias appraised. Based on a quantitative (e.g., meta-analysis) or qualitative synthesis, the strength of evidence for each outcome is rated. Outcomes are then weighted according to patient values and preferences and considered as a whole, in turn determining the strength of a recommendation. What follows are descriptions and examples of how individual elements of the evidence synthesis are implemented.\n\n\nFigure 4.1: Model and approach to evidence synthesis for making recommendations."
  },
  {
    "objectID": "04_evidence_syn.html#risk-of-bias-of-individual-studies",
    "href": "04_evidence_syn.html#risk-of-bias-of-individual-studies",
    "title": "4  Evidence Synthesis",
    "section": "4.4 Risk of Bias of Individual Studies",
    "text": "4.4 Risk of Bias of Individual Studies\nRisk of bias assessment for randomized trials uses the Cochrane tool (J. A. C. Sterne, 2019). For non-randomized studies of interventions (eg, observational studies of interventions including cohort, case-control, and quasi-randomized designs), ROBINS-I (Risk Of Bias In Non-randomised Studies of Interventions) is used (J. A. Sterne, 2016). For diagnostic accuracy studies, risk of bias for diagnostic accuracy studies are appraised with the QUADAS 2 tool (Quality Assessment of Diagnostic Accuracy Studies) (Whiting, 2011). Other tools may be used as relevant. Risk of bias is assessed independently by two reviewers with discrepancies resolved by discussion, or a third reviewer as needed."
  },
  {
    "objectID": "04_evidence_syn.html#study-design-classification",
    "href": "04_evidence_syn.html#study-design-classification",
    "title": "4  Evidence Synthesis",
    "section": "4.5 Study Design Classification",
    "text": "4.5 Study Design Classification\nRandomized controlled trial (parallel) Cluster randomized Crossover trial Non-randomized trial (non-randomized studies of interventions) Quasi-experimental (before-after, time series) Prospective cohort (observational) Retrospective cohort (observational) Cross-sectional Case-control Fully paired (diagnostic) Case series\nOther designs specific to question (eg, fully paired)"
  },
  {
    "objectID": "04_evidence_syn.html#inclusion-of-observational-studies",
    "href": "04_evidence_syn.html#inclusion-of-observational-studies",
    "title": "4  Evidence Synthesis",
    "section": "4.6 Inclusion of Observational Studies",
    "text": "4.6 Inclusion of Observational Studies\nHarms\nEfficacy rcts sufficient\nElse add observational"
  },
  {
    "objectID": "04_evidence_syn.html#quantitative-synthesis",
    "href": "04_evidence_syn.html#quantitative-synthesis",
    "title": "4  Evidence Synthesis",
    "section": "4.7 Quantitative Synthesis",
    "text": "4.7 Quantitative Synthesis\nAs appropriate, based on clinical and methodological heterogeneity, study results are pooled in either pairwise or network meta-analyses. Random effects models are generally used as the goal of pooling is to estimate unconditional effects.{Hedges, 1998 #13} Statistical heterogeneity is evaluated using I2, and for values exceeding 25%, meta-regression is considered.{Thompson, 2002 #14} Small study effects and the potential for publication bias are evaluated using funnel plots, regression-based tests, and adjustment methods.{Schwarzer, 2015 #15} Relative effects are pooled as odds ratios{Doi, 2020 #27} and continuous measures as mean differences or standardized mean differences when studies use differing scales. Analyses are conducted using R in a reproducible manner{Team, 2020 #16;Blischak, 2019 #197} and are made publicly available when the Practice Parameter is completed.\n\n4.7.1 Harms/Adverse Events\nComparative harms are examined as either relative or absolute effects according to the frequency of events. For rare events (eg, mortality) risk differences are used. Risk differences ares"
  },
  {
    "objectID": "04_evidence_syn.html#network-meta-analyses",
    "href": "04_evidence_syn.html#network-meta-analyses",
    "title": "4  Evidence Synthesis",
    "section": "4.8 Network Meta-Analyses",
    "text": "4.8 Network Meta-Analyses\nAbsent compelling reasons for a Bayesian approach (e.g., to incorporate regularizing/informative prior[s]), network meta-analyses using a frequentist approach are conducted."
  },
  {
    "objectID": "04_evidence_syn.html#grading-the-strength-of-evidence",
    "href": "04_evidence_syn.html#grading-the-strength-of-evidence",
    "title": "4  Evidence Synthesis",
    "section": "4.9 Grading the Strength of Evidence",
    "text": "4.9 Grading the Strength of Evidence\nThe strength (certainty) of evidence for important outcomes is appraised using GRADE,{Schunemann, 2019 #18} and ACC/AHA{, 2010 #21} frameworks.\nIn the GRADE approach (likely to be adopted), a strength of evidence is determined using an algorithm that includes limitations in the body of evidence (bias, inconsistency, imprecision, indirectness, publication bias) together with factors that can increase confidence in effects obtained from observational studies (large or very large effect magnitude, dose-response, extent of plausible residual confounding). According to study limitations, the strength of evidence may be rated down 1 or 2 levels according to study limitations from a starting rating of high for RCTs. Evidence from observational studies begin with a low rating and may be rated down for limitations or rated up because of effect magnitude, dose-response, or the impact of plausible residual confounding. GRADE guidance for rating the certainty of evidence up or down is followed, with some additions. Inconsistency (unexplained heterogeneity) of pooled effects is judged by examining statistical measures (I2 and between-study variance 𝜏2) alongside prediction intervals when there are sufficient studies. Statistical measures can vary by effect (associational) measures (eg, risk ratio and odds ratio) and are examined for differences in choice."
  },
  {
    "objectID": "04_evidence_syn.html#strength-of-recommendations",
    "href": "04_evidence_syn.html#strength-of-recommendations",
    "title": "4  Evidence Synthesis",
    "section": "4.10 Strength of Recommendations",
    "text": "4.10 Strength of Recommendations\nThe categories of recommendations in the GRADE approach include strong in favor, weak in favor, weak against, and strong against an intervention. Strong recommendations reflect Task Force believing all or almost all clinicians would choose the specific action or approach. Weak recommendations are those where most, but not all, would choose the action or approach."
  },
  {
    "objectID": "04_evidence_syn.html#references",
    "href": "04_evidence_syn.html#references",
    "title": "4  Evidence Synthesis",
    "section": "4.11 References",
    "text": "4.11 References\n\n\n\n\nMeltzer, D. O., Hoomans, T., Chung, J. W., & Basu, A. (2011). “Minimal modeling approaches to value of information analysis for health research.” Med Decis Making, 31(6), E1–E22. https://doi.org/10.1177/0272989X11412975\n\n\nSpiegelhalter, D. J., Abrams, K. R., & Myles, J. P. (2004). Bayesian approaches to clinical trials and health care evaluation. Wiley.\n\n\nSterne, J. A. C., Savovic, J., Page, M. J., Elbers, R. G., Blencowe, N. S., Boutron, I., … Higgins, J. P. T. (2019). “RoB 2: A revised tool for assessing risk of bias in randomised trials.” BMJ, 366, l4898. https://doi.org/10.1136/bmj.l4898\n\n\nSterne, J. A., Hernán, M. A., Reeves, B. C., Savović, J., Berkman, N. D., Viswanathan, M., … Higgins, J. P. (2016). “ROBINS-i: A tool for assessing risk of bias in non-randomised studies of interventions.” BMJ, 355, i4919. https://doi.org/10.1136/bmj.i4919\n\n\nWhiting, P. F., Rutjes, A. W., Westwood, M. E., Mallett, S., Deeks, J. J., Reitsma, J. B., … Group, Q.-. (2011). “QUADAS-2: A revised tool for the quality assessment of diagnostic accuracy studies.” Ann Intern Med, 155(8), 529–36. https://doi.org/10.7326/0003-4819-155-8-201110180-00009"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Booth, A., Clarke, M., Dooley, G., Ghersi, D., Moher, D., Petticrew, M.,\n& Stewart, L. (2012). “The nuts and bolts of PROSPERO: An\ninternational prospective register of systematic reviews.” Syst\nRev, 1, 2. https://doi.org/10.1186/2046-4053-1-2\n\n\nCaplan, R. A., Benumof, J. L., Berry, F. A., Blitt, C. D., Bode, R. H.,\nCheney, F. W., … Ovassapian, A. (1993). “Practice guidelines for\nmanagement of the difficult airway. A report by the american society of\nanesthesiologists task force on management of the difficult\nairway.” Anesthesiology, 78(3), 597–602.\n\n\nEden, J. (2011). Finding what works in health care: Standards for\nsystematic reviews. Washington, D.C.: National Academies Press.\n\n\nEgger, M., Zellweger-Zähner, T., Schneider, M., Junker, C., Lengeler,\nC., & Antes, G. (1997). “Language bias in randomised\ncontrolled trials published in english and german.” Lancet,\n350(9074), 326–9. https://doi.org/10.1016/S0140-6736(97)02419-7\n\n\nGraham, R. (2011). Clinical practice guidelines we can trust.\nWashington, DC: National Academies Press.\n\n\nGuyatt, G. H., Oxman, A. D., Kunz, R., Atkins, D., Brozek, J., Vist, G.,\n… Schunemann, H. J. (2011). “GRADE guidelines: 2. Framing the\nquestion and deciding on important outcomes.” J Clin Epidemiol,\n64(4), 395–400. https://doi.org/10.1016/j.jclinepi.2010.09.012\n\n\nHaddaway, N. R., Grainger, M. J., & Gray, C. T. (2022).\n“Citationchaser: A tool for transparent and efficient forward and\nbackward citation chasing in systematic searching.” Res Synth\nMethods, 13(4), 533–545. https://doi.org/10.1002/jrsm.1563\n\n\nHealth Subcommittee Hearing. (1990). “Hearing before the\nsubcommittee on health of the committee on ways and means house of\nrepresentatives one hundred first congress second session april 23, 1990\nserial 101-95.”\n\n\nJia, Y., Huang, D., Wen, J., Wang, Y., Rosman, L., Chen, Q., …\nCelentano, D. D. (2020). “Assessment of language and indexing\nbiases among chinese-sponsored randomized clinical trials.” JAMA\nNetw Open, 3(5), e205894. https://doi.org/10.1001/jamanetworkopen.2020.5894\n\n\nJüni, P., Holenstein, F., Sterne, J., Bartlett, C., & Egger, M.\n(2002). “Direction and impact of language bias in meta-analyses of\ncontrolled trials: Empirical study.” Int J Epidemiol, 31(1),\n115–23. https://doi.org/10.1093/ije/31.1.115\n\n\nMao, C., & Li, M. (2020). “Language bias among\nchinese-sponsored randomized clinical trials in systematic reviews and\nmeta-analyses-can anything be done?” JAMA Netw Open, 3(5),\ne206370. https://doi.org/10.1001/jamanetworkopen.2020.6370\n\n\nMeltzer, D. O., Hoomans, T., Chung, J. W., & Basu, A. (2011).\n“Minimal modeling approaches to value of information analysis for\nhealth research.” Med Decis Making, 31(6), E1–E22. https://doi.org/10.1177/0272989X11412975\n\n\nPCORI. (2019). “Patient-centered outcomes research institute\nmethodology standards 11: Standards for systematic reviews.”\nPCORI. Retrieved from https://www.pcori.org/research/about-our-research/research-methodology/pcori-methodology-standards#Systematic%20Reviews\n\n\nPolanin, J. R., Pigott, T. D., Espelage, D. L., & Grotpeter, J. K.\n(2019). “Best practice guidelines for abstract screening\nlarge‐evidence systematic reviews and meta‐analyses.” Research\nSynthesis Methods, 10(3), 330–342. Retrieved from http://europepmc.org/abstract/PMC/PMC6771536\n\n\nRoizen, M. F., Berger, D. I., Gabel, R. A., Gerson, J., Mark, J. B.,\nParks, R. I., … Woolf, S. H. (1993). “Practice guidelines for\npulmonary artery catheterization. A report by the american society of\nanesthesiologists task force on pulmonary artery\ncatheterization.” Anesthesiology, 78(2), 380–94.\n\n\nSpiegelhalter, D. J., Abrams, K. R., & Myles, J. P. (2004). Bayesian\napproaches to clinical trials and health care evaluation. Wiley.\n\n\nSterne, J. A. C., Savovic, J., Page, M. J., Elbers, R. G., Blencowe, N.\nS., Boutron, I., … Higgins, J. P. T. (2019). “RoB 2: A revised\ntool for assessing risk of bias in randomised trials.” BMJ, 366,\nl4898. https://doi.org/10.1136/bmj.l4898\n\n\nSterne, J. A., Hernán, M. A., Reeves, B. C., Savović, J., Berkman, N.\nD., Viswanathan, M., … Higgins, J. P. (2016). “ROBINS-i: A tool\nfor assessing risk of bias in non-randomised studies of\ninterventions.” BMJ, 355, i4919. https://doi.org/10.1136/bmj.i4919\n\n\nWhiting, P. F., Rutjes, A. W., Westwood, M. E., Mallett, S., Deeks, J.\nJ., Reitsma, J. B., … Group, Q.-. (2011). “QUADAS-2: A revised\ntool for the quality assessment of diagnostic accuracy studies.”\nAnn Intern Med, 155(8), 529–36. https://doi.org/10.7326/0003-4819-155-8-201110180-00009\n\n\nWoolf, S. H. (1991). Manual for clinical practice guideline development.\nRockville, MD: U.S. Dept. of Health; Human Services, Public Health\nService, Agency for Health Care Policy; Research ; Springfield, VA:\nAvailable from the National Technical Information Service.\n\n\nXu, C., Ju, K., Lin, L., Jia, P., Kwong, J. S. W., Syed, A., &\nFuruya-Kanamori, L. (2022). “Rapid evidence synthesis approach for\nlimits on the search date: How rapid could it be?” Res Synth\nMethods, 13(1), 68–76. https://doi.org/10.1002/jrsm.1525"
  }
]