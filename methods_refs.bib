%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Mark Grant at 2023-09-12 15:02:26 -0500 


%% Saved with string encoding Unicode (UTF-8) 



@article{Bretthauer2018,
	author = {Bretthauer, Michael and Kalager, Mette},
	date-added = {2023-09-12 14:55:59 -0500},
	date-modified = {2023-09-12 14:55:59 -0500},
	doi = {10.1016/S0140-6736(18)31671-4},
	journal = {Lancet},
	journal-full = {Lancet (London, England)},
	mesh = {Clinical Decision-Making; Evidence-Based Practice; Humans; Practice Guidelines as Topic},
	month = {Sep},
	number = {10151},
	pages = {898-899},
	pmid = {30238877},
	pst = {ppublish},
	title = {When no guideline recommendation is the best recommendation},
	volume = {392},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1016/S0140-6736(18)31671-4}}

@article{Neumann2020,
	author = {Neumann, Ignacio and Sch{\"u}nemann, Holger J},
	date-added = {2023-09-12 14:55:10 -0500},
	date-modified = {2023-09-12 14:55:10 -0500},
	doi = {10.1503/cmaj.190144},
	journal = {CMAJ},
	journal-full = {CMAJ : Canadian Medical Association journal = journal de l'Association medicale canadienne},
	mesh = {Evidence-Based Medicine; Humans; Practice Guidelines as Topic},
	month = {Jan},
	number = {2},
	pages = {E23-E24},
	pmc = {PMC6957326},
	pmid = {31932336},
	pst = {ppublish},
	title = {Guideline groups should make recommendations even if the evidence is considered insufficient},
	volume = {192},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1503/cmaj.190144}}

@article{Guyatt2015,
	author = {Guyatt, Gordon H and Sch{\"u}nemann, Holger J and Djulbegovic, Benjamin and Akl, Elie A},
	date-added = {2023-09-12 14:53:05 -0500},
	date-modified = {2023-09-12 14:53:05 -0500},
	doi = {10.1016/j.jclinepi.2014.12.011},
	journal = {J Clin Epidemiol},
	journal-full = {Journal of clinical epidemiology},
	mesh = {Evidence-Based Medicine; Female; Guideline Adherence; Humans; Male; Practice Guidelines as Topic; Quality Assurance, Health Care; Review Literature as Topic},
	month = {May},
	number = {5},
	pages = {597-600},
	pmid = {25660962},
	pst = {ppublish},
	title = {Guideline panels should not GRADE good practice statements},
	volume = {68},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1016/j.jclinepi.2014.12.011}}

@article{Guyatt2016,
	author = {Guyatt, Gordon H and Alonso-Coello, Pablo and Sch{\"u}nemann, Holger J and Djulbegovic, Benjamin and Nothacker, Monika and Lange, Stefan and Murad, M Hassan and Akl, Elie A},
	date-added = {2023-09-12 14:53:05 -0500},
	date-modified = {2023-09-12 14:53:05 -0500},
	doi = {10.1016/j.jclinepi.2016.07.006},
	journal = {J Clin Epidemiol},
	journal-full = {Journal of clinical epidemiology},
	mesh = {Evidence-Based Medicine; Practice Guidelines as Topic},
	month = {12},
	pages = {3-7},
	pmid = {27452192},
	pst = {ppublish},
	title = {Guideline panels should seldom make good practice statements: guidance from the GRADE Working Group},
	volume = {80},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1016/j.jclinepi.2016.07.006}}

@article{Brito2013,
	abstract = {CONTEXT: In 2005, the Endocrine Society (TES) adopted the GRADE system of developing clinical practice guidelines. Grading of Recommendations, Assessment, Development, and Evaluation working group guidance suggests that strong recommendations based on low or very low (L/VL) confidence may often be inappropriate, and has offered a taxonomy of paradigmatic situations in which strong recommendations based on L/VL confidence estimates may be appropriate.
OBJECTIVE: We sought to characterize strong recommendations of TES based on L/VL confidence evidence.
DATA SOURCES AND EXTRACTION: We identified all strong recommendations based on L/VL confidence evidence published in TES guidelines between 2005 and 2011. We identified those consistent with one of the paradigmatic situations in the taxonomy.
DATA SYNTHESIS: Two hundred six of 357 (58%) of the recommendations of TES were strong; of these, 121 (59%) were based on L/VL confidence evidence. Of these 121, 35 (29%) were consistent with one of the paradigmatic situations. The most common situation (13, 11%) was of a strong recommendation against the intervention because of low confidence evidence for benefit and high confidence evidence for harm. The remaining 86 (71%) comprised 43 (36%) "best practice" statements for which sensible alternatives do not exist; 5 (4%) in which recommendations were for "additional research"; 5 (4%) in which greater confidence in the estimates was warranted; and 33 (27%) for which we could not find a compelling explanation for the incongruence.
CONCLUSIONS: Guideline panels should beware of formulating strong recommendations when confidence in estimates is low. Our taxonomy when such recommendations are appropriate may be helpful.},
	author = {Brito, Juan P and Domecq, Juan P and Murad, Mohammed H and Guyatt, Gordon H and Montori, Victor M},
	date-added = {2023-09-12 14:52:09 -0500},
	date-modified = {2023-09-12 14:52:09 -0500},
	doi = {10.1210/jc.2013-1814},
	journal = {J Clin Endocrinol Metab},
	journal-full = {The Journal of clinical endocrinology and metabolism},
	mesh = {Endocrine System Diseases; Humans; Practice Guidelines as Topic; Societies, Medical},
	month = {Aug},
	number = {8},
	pages = {3246-52},
	pmid = {23783104},
	pst = {ppublish},
	title = {The Endocrine Society guidelines: when the confidence cart goes before the evidence horse},
	volume = {98},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1210/jc.2013-1814}}

@article{Jacobs2014a,
	abstract = {BACKGROUND: Consensus statements and clinical practice guidelines are widely available for enhancing the care of cancer patients. Despite subtle differences in their definition and purpose, these terms are often used interchangeably. We systematically assessed the methodological quality of consensus statements and clinical practice guidelines published in three commonly read, geographically diverse, cancer-specific journals. Methods Consensus statements and clinical practice guidelines published between January 2005 and September 2013 in Current Oncology, European Journal of Cancer and Journal of Clinical Oncology were evaluated. Each publication was assessed using the Appraisal of Guidelines for Research and Evaluation II (AGREE II) rigour of development and editorial independence domains. For assessment of transparency of document development, 7 additional items were taken from the Institute of Medicine's standards for practice guidelines and the Journal of Clinical Oncology guidelines for authors of guidance documents.
METHODS: Consensus statements and clinical practice guidelines published between January 2005 and September 2013 in Current Oncology, European Journal of Cancer and Journal of Clinical Oncology were evaluated. Each publication was assessed using the Appraisal of Guidelines for Research and Evaluation II (AGREE II) rigour of development and editorial independence domains. For assessment of transparency of document development, 7 additional items were taken from the Institute of Medicine's standards for practice guidelines and the Journal of Clinical Oncology guidelines for authors of guidance documents.
FINDINGS: Thirty-four consensus statements and 67 clinical practice guidelines were evaluated. The rigour of development score for consensus statements over the three journals was 32% lower than that of clinical practice guidelines. The editorial independence score was 15% lower for consensus statements than clinical practice guidelines. One journal scored consistently lower than the others over both domains. No journals adhered to all the items related to the transparency of document development. One journal's consensus statements endorsed a product made by the sponsoring pharmaceutical company in 64% of cases.
CONCLUSION: Guidance documents are an essential part of oncology care and should be subjected to a rigorous and validated development process. Consensus statements had lower methodological quality than clinical practice guidelines using AGREE II. At a minimum, journals should ensure that that all consensus statements and clinical practice guidelines adhere to AGREE II criteria. Journals should consider explicitly requiring guidelines to declare pharmaceutical company sponsorship and to identify the sponsor's product to enhance transparency.},
	author = {Jacobs, Carmel and Graham, Ian D and Makarski, Julie and Chass{\'e}, Micha{\"e}l and Fergusson, Dean and Hutton, Brian and Clemons, Mark},
	date-added = {2023-09-12 09:52:35 -0500},
	date-modified = {2023-09-12 09:52:35 -0500},
	doi = {10.1371/journal.pone.0110469},
	journal = {PLoS One},
	journal-full = {PloS one},
	mesh = {Analysis of Variance; Humans; Medical Oncology; Patient Care; Practice Guidelines as Topic; Quality Control},
	number = {10},
	pages = {e110469},
	pmc = {PMC4201546},
	pmid = {25329669},
	pst = {epublish},
	title = {Clinical practice guidelines and consensus statements in oncology--an assessment of their methodological quality},
	volume = {9},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0110469}}

@article{Brouwers2016,
	abstract = {AGREE II is a widely used standard for assessing the methodological quality of practice guidelines. This article describes the development of the AGREE Reporting Checklist, which was designed to improve the quality of practice guideline reporting and aligns with AGREE II in its structure and content.},
	author = {Brouwers, Melissa C and Kerkvliet, Kate and Spithoff, Karen and {AGREE Next Steps Consortium}},
	date-added = {2023-09-12 09:46:51 -0500},
	date-modified = {2023-09-12 09:47:12 -0500},
	doi = {10.1136/bmj.i1152},
	journal = {BMJ},
	journal-full = {BMJ (Clinical research ed.)},
	mesh = {Biomedical Research; Checklist; Humans; Practice Guidelines as Topic; Practice Patterns, Physicians'; Research Design},
	month = {Mar},
	pages = {i1152},
	pmc = {PMC5118873},
	pmid = {26957104},
	pst = {epublish},
	title = {The {AGREE} Reporting Checklist: a tool to improve reporting of clinical practice guidelines},
	volume = {352},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1136/bmj.i1152}}

@article{Jue2019,
	abstract = {In 2011, the Institute of Medicine (IOM) (now the National Academy of Medicine) published standards for trustworthy guidelines and recommended that the National Guideline Clearinghouse (NGC) of the Agency for Healthcare Research and Quality clearly indicate the extent to which guidelines adhere to these standards. To accomplish this, the authors developed and tested the NGC Extent of Adherence to Trustworthy Standards (NEATS) instrument. The standards were operationalized as an instrument containing 15 items that cover disclosure of the funding source; disclosure and management of conflicts of interest; multidisciplinary input; incorporation of patient perspectives; rigorous systematic review; recommendations accompanied by rationale, assessment of benefits and harms, clear linkage to the evidence, and assessment of strength of evidence and strength of recommendation; clear articulation of recommendations; external review by diverse stakeholders; and plans for updating. After multiple rounds of feedback from experts on clinical practice guideline development, the external validity and interrater reliability of the instrument were evaluated. For each item, 80% to 100% of survey respondents judged it to be a good measure of the IOM standards. All external stakeholders stated that NEATS was suitable for its intended goal. Interrater reliability for the final NEATS instrument had a weighted Îº of 0.73. The NEATS instrument is a focused tool that provides a concise evaluation of a guideline's adherence to the IOM standards for trustworthy guidelines. It has good external validity among guideline developers and good interrater reliability across trained reviewers.},
	author = {Jue, J Jane and Cunningham, Sarah and Lohr, Kathleen and Shekelle, Paul and Shiffman, Richard and Robbins, Craig and Nix, Mary and Coates, Vivian and Schoelles, Karen},
	date-added = {2023-09-12 09:44:18 -0500},
	date-modified = {2023-09-12 09:44:18 -0500},
	doi = {10.7326/M18-2950},
	journal = {Ann Intern Med},
	journal-full = {Annals of internal medicine},
	month = {Apr},
	number = {7},
	pages = {480-487},
	pmid = {30884527},
	pst = {ppublish},
	title = {Developing and Testing the Agency for Healthcare Research and Quality's National Guideline Clearinghouse Extent of Adherence to Trustworthy Standards (NEATS) Instrument},
	volume = {170},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.7326/M18-2950}}

@article{Stewart2015,
	abstract = {BACKGROUND: The grades of recommendation, assessment, development and evaluation (GRADE) approach is widely implemented in systematic reviews, health technology assessment and guideline development organisations throughout the world. A key advantage to this approach is that it aids transparency regarding judgments on the quality of evidence. However, the intricacies of making judgments about research methodology and evidence make the GRADE system complex and challenging to apply without training.
METHODS: We have developed a semi-automated quality assessment tool (SAQAT) l based on GRADE. This is informed by responses by reviewers to checklist questions regarding characteristics that may lead to unreliability. These responses are then entered into the Bayesian network to ascertain the probabilities of risk of bias, inconsistency, indirectness, imprecision and publication bias conditional on review characteristics. The model then combines these probabilities to provide a probability for each of the GRADE overall quality categories. We tested the model using a range of plausible scenarios that guideline developers or review authors could encounter.
RESULTS: Overall, the model reproduced GRADE judgements for a range of scenarios. Potential advantages over standard assessment are use of explicit and consistent weightings for different review characteristics, forcing consideration of important but sometimes neglected characteristics and principled downgrading where small but important probabilities of downgrading are accrued across domains.
CONCLUSIONS: Bayesian networks have considerable potential for use as tools to assess the validity of research evidence. The key strength of such networks lies in the provision of a statistically coherent method for combining probabilities across a complex framework based on both belief and evidence. In addition to providing tools for less experienced users to implement reliability assessment, the potential for sensitivity analyses and automation may be beneficial for application and the methodological development of reliability tools.},
	author = {Stewart, Gavin B and Higgins, Julian P T and Sch{\"u}nemann, Holger and Meader, Nick},
	date-added = {2023-09-11 16:52:14 -0500},
	date-modified = {2023-09-11 16:52:14 -0500},
	doi = {10.1371/journal.pone.0114497},
	journal = {PLoS One},
	journal-full = {PloS one},
	mesh = {Bayes Theorem; Checklist; Epidemiologic Studies; Evidence-Based Practice; Humans; Meta-Analysis as Topic; Models, Statistical; Randomized Controlled Trials as Topic; Research Design; Technology Assessment, Biomedical},
	number = {3},
	pages = {e0114497},
	pmc = {PMC4383525},
	pmid = {25837450},
	pst = {epublish},
	title = {The use of Bayesian networks to assess the quality of evidence from research synthesis: 1},
	volume = {10},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0114497}}

@article{Llewellyn2015,
	abstract = {BACKGROUND: The grades of recommendation, assessment, development and evaluation (GRADE) approach is widely implemented in systematic reviews, health technology assessment and guideline development organisations throughout the world. We have previously reported on the development of the Semi-Automated Quality Assessment Tool (SAQAT), which enables a semi-automated validity assessment based on GRADE criteria. The main advantage to our approach is the potential to improve inter-rater agreement of GRADE assessments particularly when used by less experienced researchers, because such judgements can be complex and challenging to apply without training. This is the first study examining the inter-rater agreement of the SAQAT.
METHODS: We conducted two studies to compare: a) the inter-rater agreement of two researchers using the SAQAT independently on 28 meta-analyses and b) the inter-rater agreement between a researcher using the SAQAT (who had no experience of using GRADE) and an experienced member of the GRADE working group conducting a standard GRADE assessment on 15 meta-analyses.
RESULTS: There was substantial agreement between independent researchers using the Quality Assessment Tool for all domains (for example, overall GRADE rating: weighted kappa 0.79; 95% CI 0.65 to 0.93). Comparison between the SAQAT and a standard GRADE assessment suggested that inconsistency was parameterised too conservatively by the SAQAT. Therefore the tool was amended. Following amendment we found fair-to-moderate agreement between the standard GRADE assessment and the SAQAT (for example, overall GRADE rating: weighted kappa 0.35; 95% CI 0.09 to 0.87).
CONCLUSIONS: Despite a need for further research, the SAQAT may aid consistent application of GRADE, particularly by less experienced researchers.},
	author = {Llewellyn, Alexis and Whittington, Craig and Stewart, Gavin and Higgins, Julian Pt and Meader, Nick},
	date-added = {2023-09-11 16:51:26 -0500},
	date-modified = {2023-09-11 16:51:26 -0500},
	doi = {10.1371/journal.pone.0123511},
	journal = {PLoS One},
	journal-full = {PloS one},
	mesh = {Bayes Theorem; Humans; Publication Bias; Quality Control; Reproducibility of Results; Research; Technology Assessment, Biomedical},
	number = {12},
	pages = {e0123511},
	pmc = {PMC4696848},
	pmid = {26716874},
	pst = {epublish},
	title = {The Use of {Bayesian} Networks to Assess the Quality of Evidence from Research Synthesis: 2. {I}nter-Rater Reliability and Comparison with Standard {GRADE} Assessment},
	volume = {10},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0123511}}

@article{Jacobs2014,
	author = {Jacobs, Alice K and Anderson, Jeffrey L and Halperin, Jonathan L},
	date-added = {2023-09-11 16:35:16 -0500},
	date-modified = {2023-09-11 16:35:16 -0500},
	doi = {10.1016/j.jacc.2014.06.001},
	journal = {J Am Coll Cardiol},
	journal-full = {Journal of the American College of Cardiology},
	keywords = {ACC/AHA Clinical Practice Guidelines; evidence-based medicine; health care; methodology},
	mesh = {Advisory Committees; American Heart Association; Cardiology; Cardiovascular Diseases; Humans; Practice Guidelines as Topic; United States},
	month = {Sep},
	number = {13},
	pages = {1373-84},
	pmid = {25103073},
	pst = {ppublish},
	title = {The evolution and future of {ACC/AHA} clinical practice guidelines: a 30-year journey: a report of the {American College of Cardiology/American Heart Association Task Force on Practice Guidelines}},
	volume = {64},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1016/j.jacc.2014.06.001}}

@book{Goodman2013,
	abstract = {BACKGROUND: Virtually all current frameworks for the evaluation of the strength of evidence for an intervention's effect focus on the quality of the design linking the intervention to a given outcome. Knowledge of biological mechanism plays no formal role. In none of the evidence grading schemas, new statistical methodologies or other technology assessment guidelines is there a formal language and structure for how knowledge of how an intervention works. OBJECTIVES: The objective was to identify and pilot test a framework for the evaluation of the evidential weight of mechanistic knowledge in evidence-based medicine and technology assessment. METHODS: Six steps were used to develop a framework for the evaluation of the evidential weight of mechanistic knowledge: (1) Focused literature review, (2) Development of draft framework, (3) Workshop with technical experts, (4) Refinement of framework, (5) Development of two case studies, (6) Pilot test of framework on case studies. RESULTS: The final version of the framework for evaluation of mechanistic evidence incorporates an evaluation of the strength of evidence for the: 1. Intervention's target effect in nonhuman models. 2. Clinical impact of target effect in nonhuman models. 3. Predictive power of nonhuman model for an effect in humans: 3t. The predictive power of the target effect model. 3c. The predictive power of the clinical effect model. 4. Intervention's target effect in human disease states. 5. Clinical impact of the target effect in human disease states. A graphic representation is included in the full report. CONCLUSION: This framework has several features combining work from a variety of fields that represent an important step forward in the rigorous assessment of such evidence. 1. It uses a definition of evidence based on inferential effect, not study design. 2. It separates evidence based on mechanistic knowledge from that based on direct evidence linking the intervention to a given clinical outcome. 3. It represents the minimum sufficient set of steps for building an indirect chain of mechanistic evidence. 4. It is adaptable and generalizable to all forms of interventions and health outcomes. It mirrors in the evidential framework the conceptual framework for translational medicine, thus linking the fields of basic science, evidence-based medicine and comparative effectiveness research.},
	author = {Goodman, Steven N and Gerson, Jason},
	bti = {Mechanistic Evidence in Evidence-Based Medicine: A Conceptual Framework},
	cdat = {2013/09/13 06:00},
	cti = {AHRQ Methods for Effective Health Care},
	da = {20130913},
	date = {2013 Jun},
	date-added = {2023-09-11 16:34:12 -0500},
	date-modified = {2023-09-11 16:34:12 -0500},
	edat = {2013/09/13 06:00},
	journal = {AHRQ Methods for Effective Health Care},
	language = {eng},
	mhda = {2013/09/13 06:00},
	month = {Jun},
	pb = {Agency for Healthcare Research and Quality (US)},
	pl = {Rockville (MD)},
	pmid = {24027800},
	pt = {Review; Book},
	publisher = {AHRQ Methods for Effective Health Care, Agency for Healthcare Research and Quality (US), Rockville (MD)},
	status = {Publisher},
	title = {Mechanistic Evidence in Evidence-Based Medicine: A Conceptual Framework},
	year = {2013}}

@article{Schulz2023,
	author = {Schulz, Juliana and Moodie, Erica E M and Shortreed, Susan M},
	date-added = {2023-09-11 16:23:11 -0500},
	date-modified = {2023-09-11 16:23:11 -0500},
	doi = {10.1093/aje/kwad133},
	journal = {Am J Epidemiol},
	journal-full = {American journal of epidemiology},
	mesh = {Humans; Models, Statistical; Computer Simulation; Bias},
	month = {Sep},
	number = {9},
	pages = {1604-1605},
	pmid = {37280737},
	pst = {ppublish},
	title = {NO UNMEASURED CONFOUNDING: KNOWN UNKNOWNS OR{\ldots} NOT?},
	volume = {192},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1093/aje/kwad133}}

@article{Simonsohn2014,
	abstract = {Journals tend to publish only statistically significant evidence, creating a scientific record that markedly overstates the size of effects. We provide a new tool that corrects for this bias without requiring access to nonsignificant results. It capitalizes on the fact that the distribution of significant p values, p-curve, is a function of the true underlying effect. Researchers armed only with sample sizes and test results of the published findings can correct for publication bias. We validate the technique with simulations and by reanalyzing data from the Many-Labs Replication project. We demonstrate that p-curve can arrive at conclusions opposite that of existing tools by reanalyzing the meta-analysis of the "choice overload" literature.},
	author = {Simonsohn, Uri and Nelson, Leif D and Simmons, Joseph P},
	date-added = {2023-09-11 15:34:12 -0500},
	date-modified = {2023-09-11 15:34:12 -0500},
	doi = {10.1177/1745691614553988},
	journal = {Perspect Psychol Sci},
	journal-full = {Perspectives on psychological science : a journal of the Association for Psychological Science},
	keywords = {p-curve; p-hacking; publication bias},
	mesh = {Computer Simulation; Publication Bias; Statistics as Topic},
	month = {Nov},
	number = {6},
	pages = {666-81},
	pmid = {26186117},
	pst = {ppublish},
	title = {p-Curve and Effect Size: Correcting for Publication Bias Using Only Significant Results},
	volume = {9},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1177/1745691614553988}}

@article{Copas2014,
	abstract = {It is often suspected (or known) that outcomes published in medical trials are selectively reported. A systematic review for a particular outcome of interest can only include studies where that outcome was reported and so may omit, for example, a study that has considered several outcome measures but only reports those giving significant results. Using the methodology of the Outcome Reporting Bias (ORB) in Trials study of (Kirkham and others, 2010. The impact of outcome reporting bias in randomised controlled trials on a cohort of systematic reviews. British Medical Journal 340, c365), we suggest a likelihood-based model for estimating the effect of ORB on confidence intervals and p-values in meta-analysis. Correcting for bias has the effect of moving estimated treatment effects toward the null and hence more cautious assessments of significance. The bias can be very substantial, sometimes sufficient to completely overturn previous claims of significance. We re-analyze two contrasting examples, and derive a simple fixed effects approximation that can be used to give an initial estimate of the effect of ORB in practice.},
	author = {Copas, John and Dwan, Kerry and Kirkham, Jamie and Williamson, Paula},
	date-added = {2023-09-11 15:33:06 -0500},
	date-modified = {2023-09-11 15:33:06 -0500},
	doi = {10.1093/biostatistics/kxt046},
	journal = {Biostatistics},
	journal-full = {Biostatistics (Oxford, England)},
	keywords = {Bias correction; Meta-analysis; ORBIT classification; Outcome reporting bias; Selective reporting},
	mesh = {Bias; Clinical Trials as Topic; Data Interpretation, Statistical; Likelihood Functions; Meta-Analysis as Topic; Models, Statistical; Risk Assessment; Treatment Outcome},
	month = {Apr},
	number = {2},
	pages = {370-83},
	pmid = {24215031},
	pst = {ppublish},
	title = {A model-based correction for outcome reporting bias in meta-analysis},
	volume = {15},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1093/biostatistics/kxt046}}

@article{Rucker2011,
	abstract = {Statistical heterogeneity and small-study effects are 2 major issues affecting the validity of meta-analysis. In this article, we introduce the concept of a limit meta-analysis, which leads to shrunken, empirical Bayes estimates of study effects after allowing for small-study effects. This in turn leads to 3 model-based adjusted pooled treatment-effect estimators and associated confidence intervals. We show how visualizing our estimators using the radial plot indicates how they can be calculated using existing software. The concept of limit meta-analysis also gives rise to a new measure of heterogeneity, termed G(2), for heterogeneity that remains after small-study effects are accounted for. In a simulation study with binary data and small-study effects, we compared our proposed estimators with those currently used together with a recent proposal by Moreno and others. Our criteria were bias, mean squared error (MSE), variance, and coverage of 95% confidence intervals. Only the estimators arising from the limit meta-analysis produced approximately unbiased treatment-effect estimates in the presence of small-study effects, while the MSE was acceptably small, provided that the number of studies in the meta-analysis was not less than 10. These limit meta-analysis estimators were also relatively robust against heterogeneity and one of them had a relatively small coverage error.},
	author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James R and Binder, Harald and Schumacher, Martin},
	date-added = {2023-09-11 15:30:28 -0500},
	date-modified = {2023-09-11 15:31:36 -0500},
	doi = {10.1093/biostatistics/kxq046},
	journal = {Biostatistics},
	journal-full = {Biostatistics (Oxford, England)},
	mesh = {Bayes Theorem; Computer Simulation; Humans; Meta-Analysis as Topic; Models, Statistical; Sample Size; Treatment Outcome},
	month = {Jan},
	number = {1},
	pages = {122-42},
	pmid = {20656692},
	pst = {ppublish},
	title = {Treatment-effect estimates adjusted for small-study effects via a limit meta-analysis},
	volume = {12},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1093/biostatistics/kxq046}}

@article{Stanley2014,
	abstract = {Publication selection bias is a serious challenge to the integrity of all empirical sciences. We derive meta-regression approximations to reduce this bias. Our approach employs Taylor polynomial approximations to the conditional mean of a truncated distribution. A quadratic approximation without a linear term, precision-effect estimate with standard error (PEESE), is shown to have the smallest bias and mean squared error in most cases and to outperform conventional meta-analysis estimators, often by a great deal. Monte Carlo simulations also demonstrate how a new hybrid estimator that conditionally combines PEESE and the Egger regression intercept can provide a practical solution to publication selection bias. PEESE is easily expanded to accommodate systematic heterogeneity along with complex and differential publication selection bias that is related to moderator variables. By providing an intuitive reason for these approximations, we can also explain why the Egger regression works so well and when it does not. These meta-regression methods are applied to several policy-relevant areas of research including antidepressant effectiveness, the value of a statistical life, the minimum wage, and nicotine replacement therapy.},
	author = {Stanley, T D and Doucouliagos, Hristos},
	date-added = {2023-09-11 15:27:43 -0500},
	date-modified = {2023-09-11 15:27:43 -0500},
	doi = {10.1002/jrsm.1095},
	journal = {Res Synth Methods},
	journal-full = {Research synthesis methods},
	keywords = {meta-regression; publication selection bias; systematic reviews, truncation},
	mesh = {Clinical Trials as Topic; Computer Simulation; Data Interpretation, Statistical; Evidence-Based Medicine; Meta-Analysis as Topic; Models, Statistical; Predictive Value of Tests; Publication Bias; Regression Analysis},
	month = {Mar},
	number = {1},
	pages = {60-78},
	pmid = {26054026},
	pst = {ppublish},
	title = {Meta-regression approximations to reduce publication selection bias},
	volume = {5},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1002/jrsm.1095}}

@article{Duval2000,
	abstract = {We study recently developed nonparametric methods for estimating the number of missing studies that might exist in a meta-analysis and the effect that these studies might have had on its outcome. These are simple rank-based data augmentation techniques, which formalize the use of funnel plots. We show that they provide effective and relatively powerful tests for evaluating the existence of such publication bias. After adjusting for missing studies, we find that the point estimate of the overall effect size is approximately correct and coverage of the effect size confidence intervals is substantially improved, in many cases recovering the nominal confidence levels entirely. We illustrate the trim and fill method on existing meta-analyses of studies in clinical trials and psychometrics.},
	author = {Duval, S and Tweedie, R},
	date-added = {2023-09-11 15:22:00 -0500},
	date-modified = {2023-09-11 15:22:00 -0500},
	doi = {10.1111/j.0006-341x.2000.00455.x},
	journal = {Biometrics},
	journal-full = {Biometrics},
	mesh = {Analysis of Variance; Bias; Biometry; Clinical Trials as Topic; Humans; Meta-Analysis as Topic; Psychometrics; Publishing; Statistics, Nonparametric},
	month = {Jun},
	number = {2},
	pages = {455-63},
	pmid = {10877304},
	pst = {ppublish},
	title = {Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis},
	volume = {56},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1111/j.0006-341x.2000.00455.x}}

@article{Pustejovsky2019,
	abstract = {Publication bias and other forms of outcome reporting bias are critical threats to the validity of findings from research syntheses. A variety of methods have been proposed for detecting selective outcome reporting in a collection of effect size estimates, including several methods based on assessment of asymmetry of funnel plots, such as the Egger's regression test, the rank correlation test, and the Trim-and-Fill test. Previous research has demonstated that the Egger's regression test is miscalibrated when applied to log-odds ratio effect size estimates, because of artifactual correlation between the effect size estimate and its standard error. This study examines similar problems that occur in meta-analyses of the standardized mean difference, a ubiquitous effect size measure in educational and psychological research. In a simulation study of standardized mean difference effect sizes, we assess the Type I error rates of conventional tests of funnel plot asymmetry, as well as the likelihood ratio test from a three-parameter selection model. Results demonstrate that the conventional tests have inflated Type I error due to the correlation between the effect size estimate and its standard error, while tests based on either a simple modification to the conventional standard error formula or a variance-stabilizing transformation both maintain close-to-nominal Type I error.},
	author = {Pustejovsky, James E and Rodgers, Melissa A},
	date-added = {2023-09-11 15:10:52 -0500},
	date-modified = {2023-09-11 15:10:52 -0500},
	doi = {10.1002/jrsm.1332},
	journal = {Res Synth Methods},
	journal-full = {Research synthesis methods},
	keywords = {meta-analysis; outcome reporting bias; publication bias; standardized mean difference},
	mesh = {Animals; Computer Simulation; Data Interpretation, Statistical; Humans; Likelihood Functions; Models, Statistical; Monte Carlo Method; Odds Ratio; Programming Languages; Publication Bias; Reference Standards; Regression Analysis; Reproducibility of Results; Research Design; Sample Size},
	month = {Mar},
	number = {1},
	pages = {57-71},
	pmid = {30506832},
	pst = {ppublish},
	title = {Testing for funnel plot asymmetry of standardized mean differences},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1002/jrsm.1332}}

@article{Macaskill2001,
	abstract = {Meta-analyses are subject to bias for many of reasons, including publication bias. Asymmetry in a funnel plot of study size against treatment effect is often used to identify such bias. We compare the performance of three simple methods of testing for bias: the rank correlation method; a simple linear regression of the standardized estimate of treatment effect on the precision of the estimate; and a regression of the treatment effect on sample size. The tests are applied to simulated meta-analyses in the presence and absence of publication bias. Both one-sided and two-sided censoring of studies based on statistical significance was used. The results indicate that none of the tests performs consistently well. Test performance varied with the magnitude of the true treatment effect, distribution of study size and whether a one- or two-tailed significance test was employed. Overall, the power of the tests was low when the number of studies per meta-analysis was close to that often observed in practice. Tests that showed the highest power also had type I error rates higher than the nominal level. Based on the empirical type I error rates, a regression of treatment effect on sample size, weighted by the inverse of the variance of the logit of the pooled proportion (using the marginal total) is the preferred method.},
	author = {Macaskill, P and Walter, S D and Irwig, L},
	date-added = {2023-09-11 15:09:52 -0500},
	date-modified = {2023-09-11 15:09:52 -0500},
	doi = {10.1002/sim.698},
	journal = {Stat Med},
	journal-full = {Statistics in medicine},
	mesh = {Computer Simulation; Humans; Linear Models; Meta-Analysis as Topic; Publication Bias; Sample Size; Statistics as Topic},
	month = {Feb},
	number = {4},
	pages = {641-54},
	pmid = {11223905},
	pst = {ppublish},
	title = {A comparison of methods to detect publication bias in meta-analysis},
	volume = {20},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1002/sim.698}}

@article{Peters2006,
	abstract = {CONTEXT: Egger's regression test is often used to help detect publication bias in meta-analyses. However, the performance of this test and the usual funnel plot have been challenged particularly when the summary estimate is the natural log of the odds ratio (lnOR).
OBJECTIVE: To compare the performance of Egger's regression test with a regression test based on sample size (a modification of Macaskill's test) with lnOR as the summary estimate.
DESIGN: Simulation of meta-analyses under a number of scenarios in the presence and absence of publication bias and between-study heterogeneity.
MAIN OUTCOME MEASURES: Type I error rates (the proportion of false-positive results) for each regression test and their power to detect publication bias when it is present (the proportion of true-positive results).
RESULTS: Type I error rates for Egger's regression test are higher than those for the alternative regression test. The alternative regression test has the appropriate type I error rates regardless of the size of the underlying OR, the number of primary studies in the meta-analysis, and the level of between-study heterogeneity. The alternative regression test has comparable power to Egger's regression test to detect publication bias under conditions of low between-study heterogeneity.
CONCLUSION: Because of appropriate type I error rates and reduction in the correlation between the lnOR and its variance, the alternative regression test can be used in place of Egger's regression test when the summary estimates are lnORs.},
	author = {Peters, Jaime L and Sutton, Alex J and Jones, David R and Abrams, Keith R and Rushton, Lesley},
	date-added = {2023-09-11 15:06:39 -0500},
	date-modified = {2023-09-11 15:06:39 -0500},
	doi = {10.1001/jama.295.6.676},
	journal = {JAMA},
	journal-full = {JAMA},
	mesh = {Meta-Analysis as Topic; Publication Bias; Regression Analysis; Research Design},
	month = {Feb},
	number = {6},
	pages = {676-80},
	pmid = {16467236},
	pst = {ppublish},
	title = {Comparison of two methods to detect publication bias in meta-analysis},
	volume = {295},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1001/jama.295.6.676}}

@article{Thompson1999,
	abstract = {Exploring the possible reasons for heterogeneity between studies is an important aspect of conducting a meta-analysis. This paper compares a number of methods which can be used to investigate whether a particular covariate, with a value defined for each study in the meta-analysis, explains any heterogeneity. The main example is from a meta-analysis of randomized trials of serum cholesterol reduction, in which the log-odds ratio for coronary events is related to the average extent of cholesterol reduction achieved in each trial. Different forms of weighted normal errors regression and random effects logistic regression are compared. These analyses quantify the extent to which heterogeneity is explained, as well as the effect of cholesterol reduction on the risk of coronary events. In a second example, the relationship between treatment effect estimates and their precision is examined, in order to assess the evidence for publication bias. We conclude that methods which allow for an additive component of residual heterogeneity should be used. In weighted regression, a restricted maximum likelihood estimator is appropriate, although a number of other estimators are also available. Methods which use the original form of the data explicitly, for example the binomial model for observed proportions rather than assuming normality of the log-odds ratios, are now computationally feasible. Although such methods are preferable in principle, they often give similar results in practice.},
	author = {Thompson, S G and Sharp, S J},
	date-added = {2023-09-11 15:05:40 -0500},
	date-modified = {2023-09-11 15:05:40 -0500},
	doi = {10.1002/(sici)1097-0258(19991030)18:20<2693::aid-sim235>3.0.co;2-v},
	journal = {Stat Med},
	journal-full = {Statistics in medicine},
	mesh = {Bayes Theorem; Cholesterol; Esophageal and Gastric Varices; Fibrosis; Hemorrhage; Humans; Likelihood Functions; Meta-Analysis as Topic; Myocardial Ischemia; Odds Ratio; Randomized Controlled Trials as Topic; Regression Analysis; Sclerotherapy; Statistics, Nonparametric},
	month = {Oct},
	number = {20},
	pages = {2693-708},
	pmid = {10521860},
	pst = {ppublish},
	title = {Explaining heterogeneity in meta-analysis: a comparison of methods},
	volume = {18},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1002/(sici)1097-0258(19991030)18:20%3C2693::aid-sim235%3E3.0.co;2-v}}

@article{Begg1994,
	abstract = {An adjusted rank correlation test is proposed as a technique for identifying publication bias in a meta-analysis, and its operating characteristics are evaluated via simulations. The test statistic is a direct statistical analogue of the popular "funnel-graph." The number of component studies in the meta-analysis, the nature of the selection mechanism, the range of variances of the effect size estimates, and the true underlying effect size are all observed to be influential in determining the power of the test. The test is fairly powerful for large meta-analyses with 75 component studies, but has only moderate power for meta-analyses with 25 component studies. However, in many of the configurations in which there is low power, there is also relatively little bias in the summary effect size estimate. Nonetheless, the test must be interpreted with caution in small meta-analyses. In particular, bias cannot be ruled out if the test is not significant. The proposed technique has potential utility as an exploratory tool for meta-analysts, as a formal procedure to complement the funnel-graph.},
	author = {Begg, C B and Mazumdar, M},
	date-added = {2023-09-11 15:03:01 -0500},
	date-modified = {2023-09-11 15:03:01 -0500},
	journal = {Biometrics},
	journal-full = {Biometrics},
	mesh = {Bias; Case-Control Studies; Chlamydia Infections; Chlorine; Contraceptives, Oral; Female; Humans; Lung Neoplasms; Mathematics; Meta-Analysis as Topic; Models, Statistical; Neoplasms; Odds Ratio; Publishing; Tobacco Smoke Pollution; Water Supply},
	month = {Dec},
	number = {4},
	pages = {1088-101},
	pmid = {7786990},
	pst = {ppublish},
	title = {Operating characteristics of a rank correlation test for publication bias},
	volume = {50},
	year = {1994}}

@article{Sterne2011,
	author = {Sterne, Jonathan A C and Sutton, Alex J and Ioannidis, John P A and Terrin, Norma and Jones, David R and Lau, Joseph and Carpenter, James and R{\"u}cker, Gerta and Harbord, Roger M and Schmid, Christopher H and Tetzlaff, Jennifer and Deeks, Jonathan J and Peters, Jaime and Macaskill, Petra and Schwarzer, Guido and Duval, Sue and Altman, Douglas G and Moher, David and Higgins, Julian P T},
	date-added = {2023-09-11 15:00:49 -0500},
	date-modified = {2023-09-11 15:00:49 -0500},
	doi = {10.1136/bmj.d4002},
	journal = {BMJ},
	journal-full = {BMJ (Clinical research ed.)},
	mesh = {Meta-Analysis as Topic; Randomized Controlled Trials as Topic; Selection Bias; Statistics as Topic},
	month = {Jul},
	pages = {d4002},
	pmid = {21784880},
	pst = {epublish},
	title = {Recommendations for examining and interpreting funnel plot asymmetry in meta-analyses of randomised controlled trials},
	volume = {343},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1136/bmj.d4002}}

@article{Maclure2001,
	abstract = {A risk ratio or difference from a meta-analysis is as many as ten steps away from the unobservable causal risk ratios and differences in target populations. The steps are like lenses, filters, or other fallible components of the epidemiologist's "telescope" for observing populations. Each step is another domain where different biases can be caused. How biases combine across domains in the production of epidemiologic evidence can be quickly explained to nonepidemiologists by using a sequence of causal arrow diagrams with easy notation: (a) agent of interest, (b) background risk factors, (c) correlated causes, (d) diagnosis, (e) exposure measurement, (f) filing of data, (g) grouping of cohorts, (h) harvesting of cases and controls, (i) interpretations of investigators, (j) judgments of journals, and (k) knowledge of meta-analysts. For epidemiologists, this article serves as a review of ideas about confounding, information bias, and selection bias and underscores the need for routinely analyzing the sensitivity of study findings to multiple hypothesized biases.},
	author = {Maclure, M and Schneeweiss, S},
	date-added = {2023-09-11 14:58:08 -0500},
	date-modified = {2023-09-11 14:58:08 -0500},
	doi = {10.1097/00001648-200101000-00019},
	journal = {Epidemiology},
	journal-full = {Epidemiology (Cambridge, Mass.)},
	mesh = {Bias; Causality; Confounding Factors, Epidemiologic; Epidemiologic Methods; Humans; Meta-Analysis as Topic},
	month = {Jan},
	number = {1},
	pages = {114-22},
	pmid = {11138805},
	pst = {ppublish},
	title = {Causation of bias: the episcope},
	volume = {12},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1097/00001648-200101000-00019}}

@article{Ioannidis2016,
	abstract = {POLICY POINTS: Currently, there is massive production of unnecessary, misleading, and conflicted systematic reviews and meta-analyses. Instead of promoting evidence-based medicine and health care, these instruments often serve mostly as easily produced publishable units or marketing tools. Suboptimal systematic reviews and meta-analyses can be harmful given the major prestige and influence these types of studies have acquired. The publication of systematic reviews and meta-analyses should be realigned to remove biases and vested interests and to integrate them better with the primary production of evidence.
CONTEXT: Currently, most systematic reviews and meta-analyses are done retrospectively with fragmented published information. This article aims to explore the growth of published systematic reviews and meta-analyses and to estimate how often they are redundant, misleading, or serving conflicted interests.
METHODS: Data included information from PubMed surveys and from empirical evaluations of meta-analyses.
FINDINGS: Publication of systematic reviews and meta-analyses has increased rapidly. In the period January 1, 1986, to December 4, 2015, PubMed tags 266,782 items as "systematic reviews" and 58,611 as "meta-analyses." Annual publications between 1991 and 2014 increased 2,728% for systematic reviews and 2,635% for meta-analyses versus only 153% for all PubMed-indexed items. Currently, probably more systematic reviews of trials than new randomized trials are published annually. Most topics addressed by meta-analyses of randomized trials have overlapping, redundant meta-analyses; same-topic meta-analyses may exceed 20 sometimes. Some fields produce massive numbers of meta-analyses; for example, 185 meta-analyses of antidepressants for depression were published between 2007 and 2014. These meta-analyses are often produced either by industry employees or by authors with industry ties and results are aligned with sponsor interests. China has rapidly become the most prolific producer of English-language, PubMed-indexed meta-analyses. The most massive presence of Chinese meta-analyses is on genetic associations (63% of global production in 2014), where almost all results are misleading since they combine fragmented information from mostly abandoned era of candidate genes. Furthermore, many contracting companies working on evidence synthesis receive industry contracts to produce meta-analyses, many of which probably remain unpublished. Many other meta-analyses have serious flaws. Of the remaining, most have weak or insufficient evidence to inform decision making. Few systematic reviews and meta-analyses are both non-misleading and useful.
CONCLUSIONS: The production of systematic reviews and meta-analyses has reached epidemic proportions. Possibly, the large majority of produced systematic reviews and meta-analyses are unnecessary, misleading, and/or conflicted.},
	author = {Ioannidis, John P A},
	date-added = {2023-09-11 14:46:36 -0500},
	date-modified = {2023-09-11 14:46:36 -0500},
	doi = {10.1111/1468-0009.12210},
	journal = {Milbank Q},
	journal-full = {The Milbank quarterly},
	keywords = {China; bias; conflicts of interest; evidence-based medicine; industry; meta-analyses; systematic reviews},
	mesh = {Antidepressive Agents; Bias; Deception; Evidence-Based Medicine; Genetics; Meta-Analysis as Topic; Quality Control; Randomized Controlled Trials as Topic; Review Literature as Topic},
	month = {Sep},
	number = {3},
	pages = {485-514},
	pmc = {PMC5020151},
	pmid = {27620683},
	pst = {ppublish},
	title = {The Mass Production of Redundant, Misleading, and Conflicted Systematic Reviews and Meta-analyses},
	volume = {94},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1111/1468-0009.12210}}

@article{Schunemann2019,
	abstract = {OBJECTIVE: To provide guidance on how systematic review authors, guideline developers, and health technology assessment practitioners should approach the use of the risk of bias in nonrandomized studies of interventions (ROBINS-I) tool as a part of GRADE's certainty rating process.
STUDY DESIGN AND SETTING: The study design and setting comprised iterative discussions, testing in systematic reviews, and presentation at GRADE working group meetings with feedback from the GRADE working group.
RESULTS: We describe where to start the initial assessment of a body of evidence with the use of ROBINS-I and where one would anticipate the final rating would end up. The GRADE accounted for issues that mitigate concerns about confounding and selection bias by introducing the upgrading domains: large effects, dose-effect relations, and when plausible residual confounders or other biases increase certainty. They will need to be considered in an assessment of a body of evidence when using ROBINS-I.
CONCLUSIONS: The use of ROBINS-I in GRADE assessments may allow for a better comparison of evidence from randomized controlled trials (RCTs) and nonrandomized studies (NRSs) because they are placed on a common metric for risk of bias. Challenges remain, including appropriate presentation of evidence from RCTs and NRSs for decision-making and how to optimally integrate RCTs and NRSs in an evidence assessment.},
	author = {Sch{\"u}nemann, Holger and Cuello, Carlos and Akl, Elie A and Mustafa, Reem A and Meerpohl, J{\"o}rg J and Thayer, Kris and Morgan, Rebecca L and Gartlehner, Gerald and Kunz, Regina and Katikireddi, S Vittal and Sterne, Jonathan and Higgins, Julian Pt and Guyatt, Gordon and {GRADE Working Group}},
	date-added = {2023-08-11 14:12:38 -0500},
	date-modified = {2023-08-11 14:26:07 -0500},
	doi = {10.1016/j.jclinepi.2018.01.012},
	journal = {J Clin Epidemiol},
	journal-full = {Journal of clinical epidemiology},
	keywords = {Certainty of the evidence; GRADE; Nonrandomized studies; Quality of evidence; ROBINS; Risk of bias},
	mesh = {Bias; Clinical Trials as Topic; Evidence-Based Medicine; Humans; Observational Studies as Topic; Practice Guidelines as Topic; Risk Assessment; Risk Factors; Systematic Reviews as Topic; Uncertainty},
	month = {Jul},
	pages = {105-114},
	pmc = {PMC6692166},
	pmid = {29432858},
	pst = {ppublish},
	title = {GRADE guidelines: 18. How ROBINS-I and other tools to assess risk of bias in nonrandomized studies should be used to rate the certainty of a body of evidence},
	volume = {111},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1016/j.jclinepi.2018.01.012}}

@article{Balshem2011,
	abstract = {This article introduces the approach of GRADE to rating quality of evidence. GRADE specifies four categories-high, moderate, low, and very low-that are applied to a body of evidence, not to individual studies. In the context of a systematic review, quality reflects our confidence that the estimates of the effect are correct. In the context of recommendations, quality reflects our confidence that the effect estimates are adequate to support a particular recommendation. Randomized trials begin as high-quality evidence, observational studies as low quality. "Quality" as used in GRADE means more than risk of bias and so may also be compromised by imprecision, inconsistency, indirectness of study results, and publication bias. In addition, several factors can increase our confidence in an estimate of effect. GRADE provides a systematic approach for considering and reporting each of these factors. GRADE separates the process of assessing quality of evidence from the process of making recommendations. Judgments about the strength of a recommendation depend on more than just the quality of evidence.},
	author = {Balshem, H. and Helfand, M. and Schunemann, H. J. and Oxman, A. D. and Kunz, R. and Brozek, J. and Vist, G. E. and Falck-Ytter, Y. and Meerpohl, J. and Norris, S. and Guyatt, G. H.},
	date-added = {2023-08-11 12:26:59 -0500},
	date-modified = {2023-08-11 12:26:59 -0500},
	doi = {10.1016/j.jclinepi.2010.07.015},
	issn = {1878-5921 (Electronic) 0895-4356 (Linking)},
	journal = {J Clin Epidemiol},
	keywords = {Evidence-Based Medicine/*standards Female Guideline Adherence Humans Male Practice Guidelines as Topic/*standards *Publication Bias Quality Assurance, Health Care/*standards},
	number = {4},
	pages = {401-6},
	title = {GRADE guidelines: 3. Rating the quality of evidence},
	type = {Journal Article},
	volume = {64},
	year = {2011},
	bdsk-url-1 = {http://www.ncbi.nlm.nih.gov/pubmed/21208779},
	bdsk-url-2 = {http://dx.doi.org/10.1016/j.jclinepi.2010.07.015}}

@article{Spiegelhalter2011,
	abstract = {Numerous types of uncertainty arise when using formal models in the analysis of risks. Uncertainty is best seen as a relation, allowing a clear separation of the object, source and 'owner' of the uncertainty, and we argue that all expressions of uncertainty are constructed from judgements based on possibly inadequate assumptions, and are therefore contingent. We consider a five-level structure for assessing and communicating uncertainties, distinguishing three within-model levels--event, parameter and model uncertainty--and two extra-model levels concerning acknowledged and unknown inadequacies in the modelling process, including possible disagreements about the framing of the problem. We consider the forms of expression of uncertainty within the five levels, providing numerous examples of the way in which inadequacies in understanding are handled, and examining criticisms of the attempts taken by the Intergovernmental Panel on Climate Change to separate the likelihood of events from the confidence in the science. Expressing our confidence in the adequacy of the modelling process requires an assessment of the quality of the underlying evidence, and we draw on a scale that is widely used within evidence-based medicine. We conclude that the contingent nature of risk-modelling needs to be explicitly acknowledged in advice given to policy-makers, and that unconditional expressions of uncertainty remain an aspiration.},
	author = {Spiegelhalter, David J and Riesch, Hauke},
	date-added = {2023-08-04 10:27:20 -0500},
	date-modified = {2023-08-04 10:27:20 -0500},
	doi = {10.1098/rsta.2011.0163},
	journal = {Philos Trans A Math Phys Eng Sci},
	journal-full = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
	month = {Dec},
	number = {1956},
	pages = {4730-50},
	pmid = {22042895},
	pst = {ppublish},
	title = {Don't know, can't know: embracing deeper uncertainties when analysing risks},
	volume = {369},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1098/rsta.2011.0163}}

@article{Halperin2016,
	author = {Halperin, Jonathan L and Levine, Glenn N and Al-Khatib, Sana M and Birtcher, Kim K and Bozkurt, Biykem and Brindis, Ralph G and Cigarroa, Joaquin E and Curtis, Lesley H and Fleisher, Lee A and Gentile, Federico and Gidding, Samuel and Hlatky, Mark A and Ikonomidis, John and Joglar, Jos{\'e} and Pressler, Susan J and Wijeysundera, Duminda N},
	date-added = {2023-08-04 10:19:03 -0500},
	date-modified = {2023-08-04 10:19:03 -0500},
	doi = {10.1016/j.jacc.2015.09.001},
	journal = {J Am Coll Cardiol},
	journal-full = {Journal of the American College of Cardiology},
	keywords = {ACC/AHA Clinical Practice Guideline; evidence-based medicine; health care; methodology},
	mesh = {Adult; American Heart Association; Cardiology; Evidence-Based Practice; Humans; Practice Guidelines as Topic; Tachycardia, Supraventricular; United States},
	month = {Apr},
	number = {13},
	pages = {1572-1574},
	pmid = {26409257},
	pst = {ppublish},
	title = {Further Evolution of the ACC/AHA Clinical Practice Guideline Recommendation Classification System: A Report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines},
	volume = {67},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1016/j.jacc.2015.09.001}}

@webpage{GRADE,
	author = {Sch\"{u}nemann, Holger and Bro\`{z}ek, Jan and Guyatt, Gordon and Oxman, Andrew},
	date-added = {2023-08-04 10:11:23 -0500},
	date-modified = {2023-08-04 10:11:46 -0500},
	lastchecked = {08/04/2023},
	title = {{GRADE Handbook}},
	url = {https://gdt.gradepro.org/app/handbook/handbook.html},
	year = {2013},
	bdsk-url-1 = {https://gdt.gradepro.org/app/handbook/handbook.html}}

@manual{R2023,
	address = {Vienna, Austria},
	author = {{R Core Team}},
	organization = {R Foundation for Statistical Computing},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	year = {2023},
	bdsk-url-1 = {https://www.R-project.org/}}

@book{Schwarzer2015,
	author = {Schwarzer, Guido and Carpenter, James R and R{\"u}cker, Gerta},
	date-added = {2023-08-03 10:36:45 -0500},
	date-modified = {2023-09-11 17:39:10 -0500},
	publisher = {Springer},
	title = {Meta-analysis with {R}},
	year = {2015}}

@book{Harrer2021,
	author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert, David},
	date-added = {2023-08-01 15:12:14 -0500},
	date-modified = {2023-08-01 15:12:14 -0500},
	publisher = {Chapman and Hall/CRC},
	title = {Doing meta-analysis with R: A hands-on guide},
	year = {2021}}

@article{Balduzzi2019,
	author = {Balduzzi, Sara and R{\"u}cker, Gerta and Schwarzer, Guido},
	date-added = {2023-08-01 15:10:58 -0500},
	date-modified = {2023-08-01 15:10:58 -0500},
	journal = {BMJ Ment Health},
	number = {4},
	pages = {153--160},
	publisher = {Royal College of Psychiatrists},
	title = {How to perform a meta-analysis with R: a practical tutorial},
	volume = {22},
	year = {2019}}

@article{Rucker2008,
	abstract = {BACKGROUND: The heterogeneity statistic I(2), interpreted as the percentage of variability due to heterogeneity between studies rather than sampling error, depends on precision, that is, the size of the studies included.
METHODS: Based on a real meta-analysis, we simulate artificially 'inflating' the sample size under the random effects model. For a given inflation factor M = 1, 2, 3,... and for each trial i, we create a M-inflated trial by drawing a treatment effect estimate from the random effects model, using s(i)(2)/M as within-trial sampling variance.
RESULTS: As precision increases, while estimates of the heterogeneity variance tau(2) remain unchanged on average, estimates of I(2) increase rapidly to nearly 100{\%}. A similar phenomenon is apparent in a sample of 157 meta-analyses.
CONCLUSION: When deciding whether or not to pool treatment estimates in a meta-analysis, the yard-stick should be the clinical relevance of any heterogeneity present. tau(2), rather than I(2), is the appropriate measure for this purpose.},
	author = {R{\"u}cker, Gerta and Schwarzer, Guido and Carpenter, James R and Schumacher, Martin},
	date-added = {2023-08-01 15:08:41 -0500},
	date-modified = {2023-08-01 15:08:41 -0500},
	doi = {10.1186/1471-2288-8-79},
	journal = {BMC Med Res Methodol},
	journal-full = {BMC medical research methodology},
	mesh = {Data Interpretation, Statistical; Humans; Models, Statistical; Randomized Controlled Trials as Topic; Reproducibility of Results; Research Design; Sample Size; Sensitivity and Specificity},
	pages = {79},
	pmc = {PMC2648991},
	pmid = {19036172},
	pst = {epublish},
	title = {Undue reliance on {I}$^2$ in assessing heterogeneity may mislead},
	volume = {8},
	year = {2008},
	bdsk-url-1 = {http://dx.doi.org/10.1186/1471-2288-8-79}}

@book{Dias2018a,
	author = {Dias, Sofia and Ades, Anthony E and Welton, Nicky J and Jansen, Jeroen P and Sutton, Alexander J},
	date-added = {2023-08-01 15:06:33 -0500},
	date-modified = {2023-08-01 15:06:33 -0500},
	publisher = {John Wiley \& Sons},
	title = {Network meta-analysis for decision-making},
	year = {2018}}

@article{Beliveau2019,
	abstract = {BACKGROUND: Several reviews have noted shortcomings regarding the quality and reporting of network meta-analyses (NMAs). We suspect that this issue may be partially attributable to limitations in current NMA software which do not readily produce all of the output needed to satisfy current guidelines.
RESULTS: To better facilitate the conduct and reporting of NMAs, we have created an R package called "BUGSnet" (Bayesian inference Using Gibbs Sampling to conduct a Network meta-analysis). This R package relies upon Just Another Gibbs Sampler (JAGS) to conduct Bayesian NMA using a generalized linear model. BUGSnet contains a suite of functions that can be used to describe the evidence network, estimate a model and assess the model fit and convergence, assess the presence of heterogeneity and inconsistency, and output the results in a variety of formats including league tables and surface under the cumulative rank curve (SUCRA) plots. We provide a demonstration of the functions contained within BUGSnet by recreating a Bayesian NMA found in the second technical support document composed by the National Institute for Health and Care Excellence Decision Support Unit (NICE-DSU). We have also mapped these functions to checklist items within current reporting and best practice guidelines.
CONCLUSION: BUGSnet is a new R package that can be used to conduct a Bayesian NMA and produce all of the necessary output needed to satisfy current scientific and regulatory standards. We hope that this software will help to improve the conduct and reporting of NMAs.},
	author = {B{\'e}liveau, Audrey and Boyne, Devon J and Slater, Justin and Brenner, Darren and Arora, Paul},
	date-added = {2023-08-01 15:06:19 -0500},
	date-modified = {2023-08-01 15:06:19 -0500},
	doi = {10.1186/s12874-019-0829-2},
	journal = {BMC Med Res Methodol},
	journal-full = {BMC medical research methodology},
	keywords = {Bayesian inference; Clinical efficacy; Health technology assessment; Indirect treatment comparison; Knowledge synthesis; Network meta-analysis; R package; Reporting guidelines; Systematic review},
	month = {10},
	number = {1},
	pages = {196},
	pmc = {PMC6805536},
	pmid = {31640567},
	pst = {epublish},
	title = {BUGSnet: an R package to facilitate the conduct and reporting of Bayesian network Meta-analyses},
	volume = {19},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1186/s12874-019-0829-2}}

@article{Balduzzi2023,
	author = {Balduzzi, Sara and R{\"u}cker, Gerta and Nikolakopoulou, Adriani and Papakonstantinou, Theodoros and Salanti, Georgia and Efthimiou, Orestis and Schwarzer, Guido},
	date-added = {2023-08-01 15:05:52 -0500},
	date-modified = {2023-08-01 15:05:52 -0500},
	journal = {Journal of Statistical Software},
	pages = {1--40},
	title = {netmeta: An R package for network meta-analysis using frequentist methods},
	volume = {106},
	year = {2023}}

@article{Shi2020,
	abstract = {When reporting the results of clinical studies, some researchers may choose the five-number summary (including the sample median, the first and third quartiles, and the minimum and maximum values) rather than the sample mean and standard deviation (SD), particularly for skewed data. For these studies, when included in a meta-analysis, it is often desired to convert the five-number summary back to the sample mean and SD. For this purpose, several methods have been proposed in the recent literature and they are increasingly used nowadays. In this article, we propose to further advance the literature by developing a smoothly weighted estimator for the sample SD that fully utilizes the sample size information. For ease of implementation, we also derive an approximation formula for the optimal weight, as well as a shortcut formula for the sample SD. Numerical results show that our new estimator provides a more accurate estimate for normal data and also performs favorably for non-normal data. Together with the optimal sample mean estimator in Luo et al., our new methods have dramatically improved the existing methods for data transformation, and they are capable to serve as "rules of thumb" in meta-analysis for studies reported with the five-number summary. Finally for practical use, an Excel spreadsheet and an online calculator are also provided for implementing our optimal estimators.},
	author = {Shi, Jiandong and Luo, Dehui and Weng, Hong and Zeng, Xian-Tao and Lin, Lu and Chu, Haitao and Tong, Tiejun},
	date-added = {2023-08-01 15:03:48 -0500},
	date-modified = {2023-08-01 15:03:48 -0500},
	doi = {10.1002/jrsm.1429},
	journal = {Res Synth Methods},
	journal-full = {Research synthesis methods},
	keywords = {SD; five-number summary; interquartile range; range; sample mean; sample size},
	mesh = {Algorithms; Clinical Trials as Topic; Data Interpretation, Statistical; Humans; Meta-Analysis as Topic; Models, Theoretical; Reference Standards; Sample Size; Statistics as Topic},
	month = {Sep},
	number = {5},
	pages = {641-654},
	pmid = {32562361},
	pst = {ppublish},
	title = {Optimally estimating the sample standard deviation from the five-number summary},
	volume = {11},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1002/jrsm.1429}}

@article{Cornell2014,
	abstract = {A primary goal of meta-analysis is to improve the estimation of treatment effects by pooling results of similar studies. This article explains how the most widely used method for pooling heterogeneous studies--the Der Simonian-Laird (DL) estimator--can produce biased estimates with falsely high precision. A classic example is presented to show that use of the DL estimator can lead to erroneous conclusions. Particular problems with the DL estimator are discussed, and several alternative methods for summarizing heterogeneous evidence are presented. The authors support replacing universal use of the DL estimator with analyses based on a critical synthesis that recognizes the uncertainty in the evidence,focuses on describing and explaining the probable sources of variation in the evidence, and uses random-effects estimates that provide more accurate confidence limits than the DL estimator.},
	author = {Cornell, John E and Mulrow, Cynthia D and Localio, Russell and Stack, Catharine B and Meibohm, Anne R and Guallar, Eliseo and Goodman, Steven N},
	date-added = {2023-08-01 15:03:39 -0500},
	date-modified = {2023-08-01 15:03:39 -0500},
	doi = {10.7326/M13-2886},
	journal = {Ann Intern Med},
	journal-full = {Annals of internal medicine},
	mesh = {Confidence Intervals; Data Interpretation, Statistical; Meta-Analysis as Topic; Software},
	month = {Feb},
	number = {4},
	pages = {267-70},
	pmid = {24727843},
	pst = {ppublish},
	title = {Random-effects meta-analysis of inconsistent effects: a time for change},
	volume = {160},
	year = {2014},
	bdsk-url-1 = {http://dx.doi.org/10.7326/M13-2886}}

@article{Viechtbauer2005,
	author = {Wolfgang Viechtbauer},
	date-added = {2023-08-01 15:01:42 -0500},
	date-modified = {2023-08-01 15:01:42 -0500},
	doi = {10.3102/10769986030003261},
	issn = {1935-1054},
	journal = {Journal of Educational and Behavioral Statistics},
	number = {3},
	pages = {261--293},
	title = {Bias and Efficiency of Meta-Analytic Variance Estimators in the Random-Effects Model},
	volume = {30},
	year = {2005},
	bdsk-url-1 = {http://dx.doi.org/10.3102/10769986030003261}}

@article{Thompson2002,
	abstract = {Appropriate methods for meta-regression applied to a set of clinical trials, and the limitations and pitfalls in interpretation, are insufficiently recognized. Here we summarize recent research focusing on these issues, and consider three published examples of meta-regression in the light of this work. One principal methodological issue is that meta-regression should be weighted to take account of both within-trial variances of treatment effects and the residual between-trial heterogeneity (that is, heterogeneity not explained by the covariates in the regression). This corresponds to random effects meta-regression. The associations derived from meta-regressions are observational, and have a weaker interpretation than the causal relationships derived from randomized comparisons. This applies particularly when averages of patient characteristics in each trial are used as covariates in the regression. Data dredging is the main pitfall in reaching reliable conclusions from meta-regression. It can only be avoided by prespecification of covariates that will be investigated as potential sources of heterogeneity. However, in practice this is not always easy to achieve. The examples considered in this paper show the tension between the scientific rationale for using meta-regression and the difficult interpretative problems to which such analyses are prone.},
	author = {Thompson, Simon G and Higgins, Julian P T},
	date-added = {2023-08-01 14:38:28 -0500},
	date-modified = {2023-08-01 14:38:28 -0500},
	doi = {10.1002/sim.1187},
	journal = {Stat Med},
	journal-full = {Statistics in medicine},
	mesh = {Adrenergic beta-Antagonists; Aminoglycosides; Anti-Bacterial Agents; Anti-Inflammatory Agents, Non-Steroidal; Aspirin; Bacterial Infections; Clinical Trials as Topic; Humans; Meta-Analysis as Topic; Myocardial Infarction; Statistics as Topic; Stroke},
	month = {Jun},
	number = {11},
	pages = {1559-73},
	pmid = {12111920},
	pst = {ppublish},
	title = {How should meta-regression analyses be undertaken and interpreted?},
	volume = {21},
	year = {2002},
	bdsk-url-1 = {http://dx.doi.org/10.1002/sim.1187}}

@article{Hedges1998,
	author = {Hedges, Larry V and Vevea, Jack L},
	date-added = {2023-08-01 14:35:06 -0500},
	date-modified = {2023-08-01 14:35:06 -0500},
	doi = {http://dx.doi.org/10.1037/1082-989X.3.4.486},
	journal = {Psychological Methods},
	number = {4},
	pages = {486},
	publisher = {American Psychological Association},
	title = {Fixed-and random-effects models in meta-analysis.},
	volume = {3},
	year = {1998},
	bdsk-url-1 = {http://dx.doi.org/10.1037/1082-989X.3.4.486}}

@article{Box1976,
	author = {Box, George EP},
	date-modified = {2023-08-01 11:48:56 -0500},
	journal = {Journal of the American Statistical Association},
	number = {356},
	pages = {791--799},
	publisher = {Taylor \& Francis},
	title = {Science and statistics},
	volume = {71},
	year = {1976}}

@article{Pallath2023,
	abstract = {Systematic reviews are vital instruments for researchers to understand broad trends in a field and synthesize evidence on the effectiveness of interventions in addressing specific issues. The quality of a systematic review depends critically on having comprehensively surveyed all relevant literature on the review topic. In addition to database searching, handsearching is an important supplementary technique that helps increase the likelihood of identifying all relevant studies in a literature search. Traditional handsearching requires reviewers to manually browse through a curated list of field-specific journals and conference proceedings to find articles relevant to the review topic. This manual process is not only time-consuming, laborious, costly, and error-prone due to human fatigue, but it also lacks replicability due to its cumbersome manual nature. To address these issues, this paper presents a free and open-source Python package and an accompanying web-app, Paperfetcher, to automate the retrieval of article metadata for handsearching. With Paperfetcher's assistance, researchers can retrieve article metadata from designated journals within a specified time frame in just a few clicks. In addition to handsearching, it also incorporates a beta version of citation searching in both forward and backward directions. Paperfetcher has an easy-to-use interface, which allows researchers to download the metadata of retrieved studies as a list of DOIs or as an RIS file to facilitate seamless import into systematic review screening software. To the best of our knowledge, Paperfetcher is the first tool to automate handsearching with high usability and a multi-disciplinary focus.},
	author = {Pallath, Akash and Zhang, Qiyang},
	date-added = {2023-07-31 09:51:23 -0500},
	date-modified = {2023-07-31 09:51:23 -0500},
	doi = {10.1002/jrsm.1604},
	journal = {Res Synth Methods},
	journal-full = {Research synthesis methods},
	keywords = {citation searching; handsearching; information retrieval; literature search; supplementary techniques; systematic reviews},
	mesh = {Humans; Information Storage and Retrieval; Systematic Reviews as Topic; Software; Databases, Factual; Research Personnel},
	month = {Mar},
	number = {2},
	pages = {323-335},
	pmid = {36260090},
	pst = {ppublish},
	title = {Paperfetcher: A tool to automate handsearching and citation searching for systematic reviews},
	volume = {14},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1002/jrsm.1604}}

@article{Haddaway2022,
	abstract = {Systematic searching aims to find all possibly relevant research from multiple sources, the basis for an unbiased and comprehensive evidence base. Along with bibliographic databases, systematic reviewers use a variety of additional methods to minimise procedural bias. Citation chasing exploits connections between research articles to identify relevant records for a review by making use of explicit mentions of one article within another. Citation chasing is a popular supplementary search method because it helps to build on the work of primary research and review authors. It does so by identifying potentially relevant studies that might otherwise not be retrieved by other search methods; for example, because they did not use the review authors' search terms in the specified combinations in their titles, abstracts, or keywords. Here, we briefly provide an overview of citation chasing as a method for systematic reviews. Furthermore, given the challenges and high resource requirements associated with citation chasing, the limited application of citation chasing in otherwise rigorous systematic reviews, and the potential benefit of identifying terminologically disconnected but semantically linked research studies, we have developed and describe a free and open source tool that allows for rapid forward and backward citation chasing. We introduce citationchaser, an R package and Shiny app for conducting forward and backward citation chasing from a starting set of articles. We describe the sources of data, the backend code functionality, and the user interface provided in the Shiny app.},
	author = {Haddaway, Neal R and Grainger, Matthew J and Gray, Charles T},
	date-added = {2023-07-26 11:42:03 -0500},
	date-modified = {2023-07-26 11:42:03 -0500},
	doi = {10.1002/jrsm.1563},
	journal = {Res Synth Methods},
	journal-full = {Research synthesis methods},
	keywords = {bibliographic checking; evidence synthesis tools; information retrieval; pearl growing; software development; systematic review tool; systematic searching},
	mesh = {Databases, Bibliographic; Information Storage and Retrieval; Research Design; Systematic Reviews as Topic},
	month = {Jul},
	number = {4},
	pages = {533-545},
	pmid = {35472127},
	pst = {ppublish},
	title = {Citationchaser: A tool for transparent and efficient forward and backward citation chasing in systematic searching},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1002/jrsm.1563}}

@article{Xu2022,
	abstract = {Rapid reviews have been widely employed to support timely decision-making, and limiting the search date is the most popular approach in published rapid reviews. We assessed the accuracy and workload of search date limits on the meta-analytical results to determine the best rapid strategy. The meta-analyses data were collected from the Cochrane Database of Systematic Reviews (CDSR). We emulated the rapid reviews by limiting the search date of the original CDSR to the recent 40, 35, 30, 25, 20, 15, 10, 7, 5, and 3âyears, and their results were compared to the full meta-analyses. A random sample of 10% was drawn to repeat the literature search by the same timeframe limits to measure the relative workload reduction (RWR). The relationship between accuracy and RWR was established. We identified 21,363 meta-analyses of binary outcomes and 7683 meta-analyses of continuous outcomes from 2693 CDSRs. Our results suggested that under a maximum tolerance of 5% and 10% on the bias of magnitude, a limit on the recent 20âyears can achieve good accuracy and at the same time save the most workload. Under the tolerance of 15% and 20% on the bias, a limit on the recent 10âyears and 15âyears could be considered. Limiting the search date is a valid rapid method to produce credible evidence for timely decisions. When conducting rapid reviews, researchers should consider both the accuracy and workload to make an appropriate decision.},
	author = {Xu, Chang and Ju, Ke and Lin, Lifeng and Jia, Pengli and Kwong, Joey S W and Syed, Asma and Furuya-Kanamori, Luis},
	date-added = {2023-07-24 17:39:02 -0500},
	date-modified = {2023-07-24 17:39:02 -0500},
	doi = {10.1002/jrsm.1525},
	journal = {Res Synth Methods},
	journal-full = {Research synthesis methods},
	keywords = {accuracy; limit on search date; rapid approach; rapid review; workload},
	mesh = {Diagnostic Tests, Routine; Epidemiologic Studies; Publications; Research Design; Systematic Reviews as Topic},
	month = {Jan},
	number = {1},
	pages = {68-76},
	pmid = {34523791},
	pst = {ppublish},
	title = {Rapid evidence synthesis approach for limits on the search date: How rapid could it be?},
	volume = {13},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1002/jrsm.1525}}

@article{Egger1997,
	abstract = {BACKGROUND: Some randomised controlled trials (RCTs) done in German-speaking Europe are published in international English-language journals and others in national German-language journals. We assessed whether authors are more likely to report trials with statistically significant results in English than in German.
METHODS: We studied pairs of RCT reports, matched for first author and time of publication, with one report published in German and the other in English. Pairs were identified from reports round in a manual search of five leading German-language journals and from reports published by the same authors in English found on Medline. Quality of methods and reporting were assessed with two different scales by two investigators who were unaware of authors' identities, affiliations, and other characteristics of trial reports. Main study endpoints were selected by two investigators who were unaware of trial results. Our main outcome was the number of pairs of studies in which the levels of significance (shown by p values) were discordant.
FINDINGS: 62 eligible pairs of reports were identified but 19 (31%) were excluded because they were duplicate publications. A further three pairs (5%) were excluded because no p values were given. The remaining 40 pairs were analysed. Design characteristics and quality features were similar for reports in both languages. Only 35% of German-language articles, compared with 62% of English-language articles, reported significant (p < 0.05) differences in the main endpoint between study and control groups (p = 0.002 by McNemar's test). Logistic regression showed that the only characteristic that predicted publication in an English-language journal was a significant result. The odds ratio for publication of trials with significant results in English was 3.75 (95% CI 1.25-11.3).
INTERPRETATION: Authors were more likely to publish RCTs in an English-language journal if the results were statistically significant. English language bias may, therefore, be introduced in reviews and meta-analyses if they include only trials reported in English. The effort of the Cochrane Collaboration to identify as many controlled trials as possible, through the manual search of many medical journals published in different languages will help to reduce such bias.},
	author = {Egger, M and Zellweger-Z{\"a}hner, T and Schneider, M and Junker, C and Lengeler, C and Antes, G},
	date-added = {2023-07-24 17:06:35 -0500},
	date-modified = {2023-09-12 09:48:12 -0500},
	doi = {10.1016/S0140-6736(97)02419-7},
	journal = {Lancet},
	journal-full = {Lancet (London, England)},
	mesh = {Bias; Communication Barriers; Germany; Humans; Language; Periodicals as Topic; Publishing; Randomized Controlled Trials as Topic; United States},
	month = {Aug},
	number = {9074},
	pages = {326-9},
	pmid = {9251637},
	pst = {ppublish},
	title = {Language bias in randomised controlled trials published in English and German},
	volume = {350},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1016/S0140-6736(97)02419-7}}

@article{Juni2002,
	abstract = {BACKGROUND: Excluding clinical trials reported in languages other than English from meta-analyses may introduce bias and reduce the precision of combined estimates of treatment effects. We examined the influence of trials published in languages other than English on combined estimates and conclusions of published meta-analyses.
METHODS: We searched journals and the Cochrane Database of Systematic Reviews for meta-analyses of at least five trials with binary outcomes that were based on comprehensive literature searches without language restrictions. We compared estimates of treatment effects from trials published in languages other than English to those from trials published in English, and assessed the impact of restricting meta-analyses to trials published in English.
RESULTS: We identified 303 meta-analyses: 159 (52.4%) employed comprehensive literature searches of which 50 included 485 English and 115 non-English language trials. Non-English language trials included fewer participants (median 88 versus 116, P = 0.006) and were more likely to produce significant results at P < 0.05 (41.7% versus 31.3%, P = 0.033). The methodological quality of non-English language trials tended to be lower than that of trials published in English. Estimates of treatment effects were on average 16% (95% CI : 3-26%) more beneficial in non-English-language trials than in English-language trials. In 29 (58.0%) meta-analyses the change in effect estimates after exclusion of non-English language trials was less than 5%. In the remaining meta-analyses, 5 (10.0%) showed more benefit and 16 (32.0%) less benefit after exclusion of non-English language trials.
CONCLUSIONS: This retrospective analysis suggests that excluding trials published in languages other than English has generally little effect on summary treatment effect estimates. The importance of non-English language trials is, however, difficult to predict for individual systematic reviews. Comprehensive literature searches followed by a careful assessment of trial quality are required to assess the contribution of all relevant trials, independent of language of publication.},
	author = {J{\"u}ni, Peter and Holenstein, Franziska and Sterne, Jonathan and Bartlett, Christopher and Egger, Matthias},
	date-added = {2023-07-24 17:05:54 -0500},
	date-modified = {2023-07-24 17:05:54 -0500},
	doi = {10.1093/ije/31.1.115},
	journal = {Int J Epidemiol},
	journal-full = {International journal of epidemiology},
	mesh = {Evidence-Based Medicine; Humans; Language; Meta-Analysis as Topic; Publication Bias; Randomized Controlled Trials as Topic; Retrospective Studies},
	month = {Feb},
	number = {1},
	pages = {115-23},
	pmid = {11914306},
	pst = {ppublish},
	title = {Direction and impact of language bias in meta-analyses of controlled trials: empirical study},
	volume = {31},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1093/ije/31.1.115}}

@article{Mao2020,
	author = {Mao, Chen and Li, Mengfeng},
	date-added = {2023-07-24 17:04:05 -0500},
	date-modified = {2023-07-24 17:04:05 -0500},
	doi = {10.1001/jamanetworkopen.2020.6370},
	journal = {JAMA Netw Open},
	journal-full = {JAMA network open},
	mesh = {Asian People; Bias; Humans; Language; Publication Bias; Randomized Controlled Trials as Topic},
	month = {May},
	number = {5},
	pages = {e206370},
	pmid = {32463464},
	pst = {epublish},
	title = {Language Bias Among Chinese-Sponsored Randomized Clinical Trials in Systematic Reviews and Meta-analyses-Can Anything Be Done?},
	volume = {3},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1001/jamanetworkopen.2020.6370}}

@article{Jia2020,
	abstract = {IMPORTANCE: Language and indexing biases may exist among Chinese-sponsored randomized clinical trials (CS-RCTs). Such biases may threaten the validity of systematic reviews.
OBJECTIVE: To evaluate the existence of language and indexing biases among CS-RCTs on drug interventions.
DESIGN, SETTING, AND PARTICIPANTS: In this retrospective cohort study, eligible CS-RCTs were retrieved from trial registries, and bibliographic databases were searched to determine their publication status. Eligible CS-RCTs were for drug interventions conducted from January 1, 2008, to December 31, 2014. The search and analysis were conducted from March 1 to August 31, 2019. Primary trial registries were recognized by the World Health Organization and the Drug Clinical Trial Registry Platform sponsored by the China Food and Drug Administration.
EXPOSURES: Individual CS-RCTs with positive vs negative results (positive vs negative CS-RCTs).
MAIN OUTCOMES AND MEASURES: For assessing language bias, the main outcome was the language of the journal in which CS-RCTs were published (English vs Chinese). For indexing bias, the main outcome was the language of the bibliographic database where the CS-RCTs were indexed (English vs Chinese).
RESULTS: The search identified 891 eligible CS-RCTs. Four hundred seventy CS-RCTs were published by August 31, 2019, of which 368 (78.3%) were published in English. Among CS-RCTs registered in the Chinese Clinical Trial Registry (ChiCTR), positive CS-RCTs were 3.92 (95% CI, 2.20-7.00) times more likely to be published in English than negative CS-RCTs; among CS-RCTs in English-language registries, positive CS-RCTs were 3.22 (95% CI, 1.34-7.78) times more likely to be published in English than negative CS-RCTs. These findings suggest the existence of language bias. Among CS-RCTs registered in ChiCTR, positive CS-RCTs were 2.89 (95% CI, 1.55-5.40) times more likely to be indexed in English bibliographic databases than negative CS-RCTs; among CS-RCTs in English-language registries, positive CS-RCTs were 2.19 (95% CI, 0.82-5.82) times more likely to be indexed in English bibliographic databases than negative CS-RCTs. These findings support the existence of indexing bias.
CONCLUSIONS AND RELEVANCE: This study suggests the existence of language and indexing biases among registered CS-RCTs on drug interventions. These biases may distort evidence synthesis toward more positive results of drug interventions.},
	author = {Jia, Yuanxi and Huang, Doudou and Wen, Jiajun and Wang, Yehua and Rosman, Lori and Chen, Qingkun and Robinson, Karen A and Gagnier, Joel J and Ehrhardt, Stephan and Celentano, David D},
	date-added = {2023-07-24 17:03:40 -0500},
	date-modified = {2023-07-24 17:03:40 -0500},
	doi = {10.1001/jamanetworkopen.2020.5894},
	journal = {JAMA Netw Open},
	journal-full = {JAMA network open},
	mesh = {Bias; China; Databases, Bibliographic; Drug Therapy; Humans; Language; Negative Results; Periodicals as Topic; Publication Bias; Randomized Controlled Trials as Topic; Registries; Retrospective Studies},
	month = {May},
	number = {5},
	pages = {e205894},
	pmc = {PMC7256669},
	pmid = {32463469},
	pst = {epublish},
	title = {Assessment of Language and Indexing Biases Among Chinese-Sponsored Randomized Clinical Trials},
	volume = {3},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1001/jamanetworkopen.2020.5894}}

@article{Efird2021,
	abstract = {Researchers often report a measure to several decimal places more than what is sensible or realistic. Rounding involves replacing a number with a value of lesser accuracy while minimizing the practical loss of validity. This practice is generally acceptable to simplify data presentation and to facilitate the communication and comparison of research results. Rounding also may reduce spurious accuracy when the extraneous digits are not justified by the exactness of the recording instrument or data collection procedure. However, substituting a more explicit or simpler representation for an original measure may not be practicable or acceptable if an adequate degree of accuracy is not retained. The error introduced by rounding exact numbers may result in misleading conclusions and the interpretation of study findings. For example, rounding the upper confidence interval for a relative effect estimate of 0.996 to 2 decimal places may obscure the statistical significance of the result. When presenting the findings of a study, authors need to be careful that they do not report numbers that contain too few significant digits. Equally important, they should avoid providing more significant figures than are warranted to convey the underlying meaning of the result.},
	author = {Efird, Jimmy T},
	date-added = {2022-11-15 11:18:49 -0600},
	date-modified = {2022-11-15 11:18:49 -0600},
	doi = {10.1177/1176935120985132},
	journal = {Cancer Inform},
	journal-full = {Cancer informatics},
	keywords = {Accuracy; numeric representation error; precision; relative effect estimates; rounding error},
	pages = {1176935120985132},
	pmc = {PMC7791303},
	pmid = {33456306},
	pst = {epublish},
	title = {Goldilocks Rounding: Achieving Balance Between Accuracy and Parsimony in the Reporting of Relative Effect Estimates},
	volume = {20},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1177/1176935120985132}}

@article{Cole2015,
	author = {Cole, T J},
	date-added = {2022-11-15 11:18:33 -0600},
	date-modified = {2022-11-15 11:18:33 -0600},
	doi = {10.1136/archdischild-2014-307149},
	journal = {Arch Dis Child},
	journal-full = {Archives of disease in childhood},
	keywords = {Measurement; Statistics},
	mesh = {Animals; Data Interpretation, Statistical; Humans; Periodicals as Topic; Publishing; Statistics as Topic},
	month = {Jul},
	number = {7},
	pages = {608-9},
	pmc = {PMC4483789},
	pmid = {25877157},
	pst = {ppublish},
	title = {Too many digits: the presentation of numerical data},
	volume = {100},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1136/archdischild-2014-307149}}

@article{Meltzer2011,
	abstract = {Value of information (VOI) techniques can provide estimates of the expected benefits from clinical research studies that can inform decisions about the design and priority of those studies. Most VOI studies use decision-analytic models to characterize the uncertainty of the effects of interventions on health outcomes, but the complexity of constructing such models can pose barriers to some practical applications of VOI. However, because some clinical studies can directly characterize uncertainty in health outcomes, it may sometimes be possible to perform VOI analysis with only minimal modeling. This article 1) develops a framework to define and classify minimal modeling approaches to VOI, 2) reviews existing VOI studies that apply minimal modeling approaches, and 3) illustrates and discusses the application of the minimal modeling to 2 new clinical applications to which the approach appears well suited because clinical trials with comprehensive outcomes provide preliminary estimates of the uncertainty in outcomes. The authors conclude that minimal modeling approaches to VOI can be readily applied in some instances to estimate the expected benefits of clinical research.},
	author = {Meltzer, David O and Hoomans, Ties and Chung, Jeanette W and Basu, Anirban},
	date = {2011 Nov-Dec},
	date-added = {2022-05-19 09:46:15 -0500},
	date-modified = {2022-05-19 09:52:55 -0500},
	doi = {10.1177/0272989X11412975},
	journal = {Med Decis Making},
	journal-full = {Medical decision making : an international journal of the Society for Medical Decision Making},
	mesh = {Health Services Research; Models, Theoretical},
	number = {6},
	pages = {E1-E22},
	pmid = {21712493},
	pst = {ppublish},
	title = {Minimal modeling approaches to value of information analysis for health research},
	volume = {31},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1177/0272989X11412975}}

@article{Polanin2019,
	address = {American Institutes for ResearchWashington, DC Chicago Illinois Gainesville Florida Bethesda Maryland USA},
	an = {PMC6771536},
	author = {Polanin, Joshua R. and Pigott, Terri D. and Espelage, Dorothy L. and Grotpeter, Jennifer K.},
	date = {2019/09//},
	date-added = {2022-05-18 16:12:26 -0500},
	date-modified = {2022-05-18 16:12:26 -0500},
	db = {PubMed Central},
	id = {9},
	isbn = {1759-2879},
	j2 = {Research Synthesis Methods},
	journal = {Research Synthesis Methods},
	la = {eng},
	number = {3},
	pages = {330--342},
	st = {Best practice guidelines for abstract screening largeâevidence systematic reviews and metaâanalyses},
	title = {Best practice guidelines for abstract screening largeâevidence systematic reviews and metaâanalyses},
	url = {http://europepmc.org/abstract/PMC/PMC6771536},
	volume = {10},
	year = {2019},
	bdsk-url-1 = {http://europepmc.org/abstract/PMC/PMC6771536}}

@article{Booth2012,
	abstract = {BACKGROUND: Following publication of the PRISMA statement, the UK Centre for Reviews and Dissemination (CRD) at the University of York in England began to develop an international prospective register of systematic reviews with health-related outcomes. The objectives were to reduce unplanned duplication of reviews and provide transparency in the review process, with the aim of minimizing reporting bias. METHODS: An international advisory group was formed and a consultation undertaken to establish the key items necessary for inclusion in the register and to gather views on various aspects of functionality. This article describes the development of the register, now called PROSPERO, and the process of registration. RESULTS: PROSPERO offers free registration and free public access to a unique prospective register of systematic reviews across all areas of health from all around the world. The dedicated web-based interface is electronically searchable and available to all prospective registrants. At the moment, inclusion in PROSPERO is restricted to systematic reviews of the effects of interventions and strategies to prevent, diagnose, treat, and monitor health conditions, for which there is a health-related outcome.Ideally, registration should take place before the researchers have started formal screening against inclusion criteria but reviews are eligible as long as they have not progressed beyond the point of completing data extraction.The required dataset captures the key attributes of review design as well as the administrative details necessary for registration.Submitted registration forms are checked against the scope for inclusion in PROSPERO and for clarity of content before being made publicly available on the register, rejected, or returned to the applicant for clarification.The public records include an audit trail of major changes to planned methods, details of when the review has been completed, and links to resulting publications when provided by the authors. CONCLUSIONS: There has been international support and an enthusiastic response to the principle of prospective registration of protocols for systematic reviews and to the development of PROSPERO.In October 2011, PROSPERO contained 200 records of systematic reviews being undertaken in 26 countries around the world on a diverse range of interventions.},
	address = {Centre for Reviews and Dissemination, University of York, Alcuin B Block, Heslington, York, UK, YO10 5DD. alison.booth{\char64}york.ac.uk},
	an = {22587842},
	annote = {Booth, Alison Clarke, Mike Dooley, Gordon Ghersi, Davina Moher, David Petticrew, Mark Stewart, Lesley eng G0901530/Medical Research Council/United Kingdom Research Support, Non-U.S. Gov't England Syst Rev. 2012 Feb 9;1:2. doi: 10.1186/2046-4053-1-2.},
	author = {Booth, A. and Clarke, M. and Dooley, G. and Ghersi, D. and Moher, D. and Petticrew, M. and Stewart, L.},
	c2 = {PMC3348673},
	date = {Feb 9},
	date-added = {2022-04-24 10:34:45 -0500},
	date-modified = {2022-04-24 10:34:45 -0500},
	doi = {10.1186/2046-4053-1-2},
	edition = {2012/05/17},
	id = {193},
	isbn = {2046-4053 (Electronic) 2046-4053 (Linking)},
	journal = {Syst Rev},
	keywords = {Evidence-Based Practice Information Dissemination *Internationality Internet Registries/*standards *Review Literature as Topic},
	pages = {2},
	st = {The nuts and bolts of PROSPERO: an international prospective register of systematic reviews},
	title = {The nuts and bolts of PROSPERO: an international prospective register of systematic reviews},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/22587842},
	volume = {1},
	year = {2012},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/22587842},
	bdsk-url-2 = {https://doi.org/10.1186/2046-4053-1-2}}

@webpage{PCORI2019,
	author = {{PCORI}},
	date-added = {2022-04-24 10:11:50 -0500},
	date-modified = {2023-07-24 13:54:37 -0500},
	institution = {{PCORI}},
	title = {Patient-Centered Outcomes Research Institute Methodology Standards 11: Standards for Systematic Reviews.},
	url = {https://www.pcori.org/research/about-our-research/research-methodology/pcori-methodology-standards#Systematic%20Reviews},
	year = {2019},
	bdsk-url-1 = {https://www.pcori.org/research/about-our-research/research-methodology/pcori-methodology-standards#Systematic%20Reviews}}

@article{House1990,
	author = {{Health Subcommittee Hearing}},
	date-added = {2022-04-23 10:57:10 -0500},
	date-modified = {2023-07-24 09:01:33 -0500},
	id = {4},
	st = {Hearing Before the Subcommittee on Health of the Committee on Ways and Means House of Representatives One Hundred First Congress Second Session April 23, 1990 Serial 101-95},
	title = {Hearing Before the Subcommittee on Health of the Committee on Ways and Means House of Representatives One Hundred First Congress Second Session April 23, 1990 Serial 101-95},
	year = {1990}}

@book{Eden2011,
	address = {Washington, D.C.},
	annote = {LDR 02212cam 22004454a 4500 001 16755165 005 20110906140836.0 008 110425s2011 dcua b 000 0 eng 906 {\$}a7{\$}bcbc{\$}corignew{\$}d1{\$}eecip{\$}f20{\$}gy-gencatlg 925 0 {\$}aacquire{\$}b2 shelf copies{\$}xpolicy default 955 {\$}bxh11 2011-05-26{\$}dxh11 2011-05-26 to Dewey{\$}wrd13 2011-05-27{\$}axe05 2011-08-04 1 copy rec'd., to CIP ver.{\$}fxj04 2011-08-12 Z-CipVer{\$}txg20 2011-09-06 copy 2 added 010 {\$}a 2011017455 016 7 {\$}a101559242{\$}2DNLM 020 {\$}a9780309164252 (pbk.) 020 {\$}a0309164257 (pbk.) 020 {\$}a9780309164269 (pdf) 020 {\$}a0309164265 (pdf) 020 {\$}a9780309210539 020 {\$}a0309210534 035 {\$}a(OCoLC)ocn710018987 040 {\$}aDNLM/DLC{\$}cDLC{\$}dNLM{\$}dYDXCP{\$}dUAB{\$}dUPM{\$}dDLC 042 {\$}apcc 043 {\$}an-us--- 050 00 {\$}aRA399.A3{\$}bI565 2011 060 00 {\$}a2011 H-715 060 10 {\$}aW 84.3 082 00 {\$}a610.28/9{\$}223 110 2 {\$}aInstitute of Medicine (U.S.).{\$}bCommittee on Standards for Systematic Reviews of Comparative Effectiveness Research. 245 10 {\$}aFinding what works in health care :{\$}bstandards for systematic reviews /{\$}cCommittee on Standards for Systematic Reviews of Comparative Effectiveness Research, Board on Health Care Services, Institute of Medicine of the National Academies ; Jill Eden ... {$[$}et al.{$]$}, editors. 260 {\$}aWashington, D.C. :{\$}bNational Academies Press,{\$}cc2011. 300 {\$}axxii, 317 p. :{\$}bill. ;{\$}c23 cm. 504 {\$}aIncludes bibliographical references. 505 0 {\$}aStandards for initiating a systematic review -- Standards for finding and assessing individual studies -- Standards for synthesizing the body of evidence -- Standards for reporting systematic reviews -- Improving the quality of systematic reviews. 530 {\$}aAlso available in Open Book format via the National Academies Press home page. 650 0 {\$}aMedical care{\$}xStandards{\$}zUnited States. 650 0 {\$}aMedical care{\$}zUnited States{\$}xQuality control. 650 12 {\$}aComparative Effectiveness Research{\$}xstandards{\$}zUnited States. 650 22 {\$}aOutcome and Process Assessment (Health Care){\$}zUnited States. 700 1 {\$}aEden, Jill. 985 {\$}aNLMCIP{\$}d2011-04-29},
	author = {Eden, Jill},
	date-added = {2022-04-23 10:57:10 -0500},
	date-modified = {2022-04-23 10:58:51 -0500},
	id = {96},
	isbn = {9780309164252 (pbk.)},
	publisher = {National Academies Press},
	st = {Finding what works in health care: standards for systematic reviews},
	title = {Finding what works in health care: standards for systematic reviews},
	year = {2011}}

@article{Roizen1993,
	author = {Roizen, Michael F. and Berger, David I. and Gabel, Ronald A. and Gerson, John and Mark, Jonathan B. and Parks, Robert I. and Paulus, David A. and Smith, John S. and Woolf, Steven H.},
	date-added = {2022-04-23 10:57:10 -0500},
	date-modified = {2022-04-23 10:58:51 -0500},
	id = {118},
	journal = {Anesthesiology},
	month = {Feb},
	number = {2},
	pages = {380-94},
	st = {Practice guidelines for pulmonary artery catheterization. A report by the American Society of Anesthesiologists Task Force on Pulmonary Artery Catheterization},
	title = {Practice guidelines for pulmonary artery catheterization. A report by the American Society of Anesthesiologists Task Force on Pulmonary Artery Catheterization},
	volume = {78},
	year = {1993}}

@article{Caplan1993,
	author = {Caplan, Robert A. and Benumof, Jonathan L. and Berry, Frederic A. and Blitt, Casey D. and Bode, Robert H. and Cheney, Frederick W. and Connis, Richard T. and Guidry, Orin F. and Ovassapian, Andranik},
	date-added = {2022-04-23 10:57:10 -0500},
	date-modified = {2022-04-23 10:58:51 -0500},
	id = {123},
	journal = {Anesthesiology},
	month = {Mar},
	number = {3},
	pages = {597-602},
	st = {Practice guidelines for management of the difficult airway. A report by the American Society of Anesthesiologists Task Force on Management of the Difficult Airway},
	title = {Practice guidelines for management of the difficult airway. A report by the American Society of Anesthesiologists Task Force on Management of the Difficult Airway},
	volume = {78},
	year = {1993}}

@book{Woolf1991,
	address = {Rockville, MD},
	annote = {"March 1991."; "A protocol for expert panels convened by the Office of the Forum for Quality and Effectiveness in Health Care"--Cover.; Includes bibliographical references (p. {\$}{$[$}{\$}44{\$}{$]$}{\$}-{\$}{$[$}{\$}45{\$}{$]$}{\$}).},
	author = {Woolf, Steven H.},
	date-added = {2022-04-23 10:57:10 -0500},
	date-modified = {2022-04-23 10:58:51 -0500},
	id = {124},
	keywords = {Clinical medicine / Standards / United States / Handbooks manuals etc. Quality of Health Care / United States. Clinical Medicine / standards / United States.},
	publisher = {U.S. Dept. of Health and Human Services, Public Health Service, Agency for Health Care Policy and Research ; Springfield, VA: Available from the National Technical Information Service},
	st = {Manual for clinical practice guideline development},
	title = {Manual for clinical practice guideline development},
	year = {1991}}

@book{Graham2011,
	address = {Washington, DC},
	annote = {LDR 02157cam 22003614a 4500 001 16753338 005 20140612153037.0 008 110425s2011 dcua b 000 0 eng 906 {\$}a7{\$}bcbc{\$}corignew{\$}d1{\$}eecip{\$}f20{\$}gy-gencatlg 925 0 {\$}aacquire{\$}b1 shelf copy{\$}xpolicy default 925 1 {\$}aacquire{\$}b2 shelf copies{\$}xpolicy default 955 {\$}axh00 2011-05-27{\$}dxh07 2011-05-27 to Dewey{\$}wrd13 2011-05-27{\$}axe14 2011-07-28 2 copies rec'd., to CIP ver.{\$}fre11 2011-08-12 to BCCD{\$}tre11 2011-08-22 copy 2 added 010 {\$}a 2011017454 020 {\$}a9780309164221 (pbk.) 020 {\$}a9780309164238 (pdf) 035 {\$}a(DNLM)101559241 040 {\$}aDNLM/DLC{\$}cDLC{\$}dDLC 043 {\$}an-us--- 050 00 {\$}aR728{\$}b.I47 2011 060 10 {\$}aW 84.4 AA1 082 00 {\$}a610.68{\$}223 110 2 {\$}aInstitute of Medicine (U.S.).{\$}bCommittee on Standards for Developing Trustworthy Clinical Practice Guidelines. 245 10 {\$}aClinical practice guidelines we can trust /{\$}cCommittee on Standards for Developing Trustworthy Clinical Practice Guidelines, Board on Health Care Services, Institute of Medicine of the National Academies ; Robin Graham ... {$[$}et al.{$]$}, editors. 260 {\$}aWashington, DC :{\$}bNational Academies Press,{\$}cc2011. 300 {\$}axxxiv, 266 p. :{\$}bill. ;{\$}c23 cm. 504 {\$}aIncludes bibliographical references. 505 0 {\$}aBackground and key stakeholders in guidelines development and use -- Trustworthy clinical practice guidelines : challenges and potential -- Current best practices and proposed standards for development of trustworthy CPGS : part I, getting started -- Current best practices and standards for development of trustworthy CPGS : part II, traversing the process -- Promoting adoption of clinical practice guidelines -- Development, identification, and evaluation of trustworthy clinical practice guidelines. 650 0 {\$}aMedicine{\$}xPractice{\$}xStandards{\$}zUnited States. 650 0 {\$}aTotal quality management{\$}zUnited States. 650 12 {\$}aPractice Guidelines as Topic{\$}xstandards{\$}zUnited States. 650 22 {\$}aTotal Quality Management{\$}xstandards{\$}zUnited States. 700 1 {\$}aGraham, Robin,{\$}d1952- 985 {\$}aNLMCIP{\$}d2011-04-28},
	author = {Graham, Robin},
	date-added = {2022-04-23 10:57:10 -0500},
	date-modified = {2022-04-23 10:58:51 -0500},
	id = {152},
	isbn = {9780309164221 (pbk.)},
	publisher = {National Academies Press},
	st = {Clinical practice guidelines we can trust},
	title = {Clinical practice guidelines we can trust},
	year = {2011}}

@article{Guyatt2011,
	author = {Guyatt, G. H. and Oxman, A. D. and Kunz, R. and Atkins, D. and Brozek, J. and Vist, G. and Alderson, P. and Glasziou, P. and Falck-Ytter, Y. and Schunemann, H. J.},
	date = {2011-04},
	date-modified = {2022-10-04 10:47:14 -0500},
	doi = {10.1016/j.jclinepi.2010.09.012},
	journal = {J Clin Epidemiol},
	month = {04},
	note = {Edition: 2011/01/05},
	number = {4},
	pages = {395--400},
	title = {GRADE guidelines: 2. Framing the question and deciding on important outcomes},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/21194891},
	volume = {64},
	year = {2011},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/21194891},
	bdsk-url-2 = {https://doi.org/10.1016/j.jclinepi.2010.09.012}}

@book{Spiegelhalter2004,
	author = {Spiegelhalter, David J and Abrams, Keith R and Myles, Jonathan P},
	date = {2004},
	date-modified = {2023-09-11 17:30:20 -0500},
	publisher = {Wiley},
	title = {Bayesian approaches to clinical trials and health care evaluation},
	year = {2004}}

@article{Sterne2019,
	author = {Sterne, J. A. C. and Savovic, J. and Page, M. J. and Elbers, R. G. and Blencowe, N. S. and Boutron, I. and Cates, C. J. and Cheng, H. Y. and Corbett, M. S. and Eldridge, S. M. and Emberson, J. R. and Hernan, M. A. and Hopewell, S. and Hrobjartsson, A. and Junqueira, D. R. and Juni, P. and Kirkham, J. J. and Lasserson, T. and Li, T. and McAleenan, A. and Reeves, B. C. and Shepperd, S. and Shrier, I. and Stewart, L. A. and Tilling, K. and White, I. R. and Whiting, P. F. and Higgins, J. P. T.},
	date = {2019-08-28},
	date-modified = {2022-10-04 05:30:39 -0500},
	doi = {10.1136/bmj.l4898},
	journal = {BMJ},
	month = {08},
	note = {Edition: 2019/08/30},
	pages = {l4898},
	title = {RoB 2: a revised tool for assessing risk of bias in randomised trials},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/31462531},
	volume = {366},
	year = {2019},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/31462531},
	bdsk-url-2 = {https://doi.org/10.1136/bmj.l4898}}

@article{Sterne2016,
	author = {Sterne, Jonathan Ac and {Hern{\'a}n}, Miguel A. and Reeves, Barnaby C. and {Savovi{\'{c}}}, Jelena and Berkman, Nancy D. and Viswanathan, Meera and Henry, David and Altman, Douglas G. and Ansari, Mohammed T. and Boutron, Isabelle and Carpenter, James R. and Chan, An-Wen and Churchill, Rachel and Deeks, Jonathan J. and {Hr{\'o}bjartsson}, {Asbj{\o}rn} and Kirkham, Jamie and {J{\"u}ni}, Peter and Loke, Yoon K. and Pigott, Theresa D. and Ramsay, Craig R. and Regidor, Deborah and Rothstein, Hannah R. and Sandhu, Lakhbir and Santaguida, Pasqualina L. and {Sch{\"u}nemann}, Holger J. and Shea, Beverly and Shrier, Ian and Tugwell, Peter and Turner, Lucy and Valentine, Jeffrey C. and Waddington, Hugh and Waters, Elizabeth and Wells, George A. and Whiting, Penny F. and Higgins, Julian Pt},
	date = {2016},
	date-modified = {2022-10-04 05:30:30 -0500},
	doi = {10.1136/bmj.i4919},
	journal = {BMJ},
	pages = {i4919},
	title = {ROBINS-I: a tool for assessing risk of bias in non-randomised studies of interventions},
	volume = {355},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1136/bmj.i4919}}

@article{Whiting2011,
	author = {Whiting, P. F. and Rutjes, A. W. and Westwood, M. E. and Mallett, S. and Deeks, J. J. and Reitsma, J. B. and Leeflang, M. M. and Sterne, J. A. and Bossuyt, P. M. and Group, Quadas-},
	date = {2011-10-18},
	date-modified = {2022-10-04 05:30:46 -0500},
	doi = {10.7326/0003-4819-155-8-201110180-00009},
	journal = {Ann Intern Med},
	month = {10},
	note = {Edition: 2011/10/19},
	number = {8},
	pages = {529--36},
	title = {QUADAS-2: a revised tool for the quality assessment of diagnostic accuracy studies},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/22007046},
	volume = {155},
	year = {2011},
	bdsk-url-1 = {https://www.ncbi.nlm.nih.gov/pubmed/22007046},
	bdsk-url-2 = {https://doi.org/10.7326/0003-4819-155-8-201110180-00009}}
